<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <title>Test Report</title>
    <link href="assets/style.css" rel="stylesheet" type="text/css"/></head>
  <body onLoad="init()">
    <script>/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this file,
 * You can obtain one at http://mozilla.org/MPL/2.0/. */


function toArray(iter) {
    if (iter === null) {
        return null;
    }
    return Array.prototype.slice.call(iter);
}

function find(selector, elem) {
    if (!elem) {
        elem = document;
    }
    return elem.querySelector(selector);
}

function find_all(selector, elem) {
    if (!elem) {
        elem = document;
    }
    return toArray(elem.querySelectorAll(selector));
}

function sort_column(elem) {
    toggle_sort_states(elem);
    var colIndex = toArray(elem.parentNode.childNodes).indexOf(elem);
    var key;
    if (elem.classList.contains('numeric')) {
        key = key_num;
    } else if (elem.classList.contains('result')) {
        key = key_result;
    } else {
        key = key_alpha;
    }
    sort_table(elem, key(colIndex));
}

function show_all_extras() {
    find_all('.col-result').forEach(show_extras);
}

function hide_all_extras() {
    find_all('.col-result').forEach(hide_extras);
}

function show_extras(colresult_elem) {
    var extras = colresult_elem.parentNode.nextElementSibling;
    var expandcollapse = colresult_elem.firstElementChild;
    extras.classList.remove("collapsed");
    expandcollapse.classList.remove("expander");
    expandcollapse.classList.add("collapser");
}

function hide_extras(colresult_elem) {
    var extras = colresult_elem.parentNode.nextElementSibling;
    var expandcollapse = colresult_elem.firstElementChild;
    extras.classList.add("collapsed");
    expandcollapse.classList.remove("collapser");
    expandcollapse.classList.add("expander");
}

function show_filters() {
    var filter_items = document.getElementsByClassName('filter');
    for (var i = 0; i < filter_items.length; i++)
        filter_items[i].hidden = false;
}

function add_collapse() {
    // Add links for show/hide all
    var resulttable = find('table#results-table');
    var showhideall = document.createElement("p");
    showhideall.innerHTML = '<a href="javascript:show_all_extras()">Show all details</a> / ' +
                            '<a href="javascript:hide_all_extras()">Hide all details</a>';
    resulttable.parentElement.insertBefore(showhideall, resulttable);

    // Add show/hide link to each result
    find_all('.col-result').forEach(function(elem) {
        var collapsed = get_query_parameter('collapsed') || 'Passed';
        var extras = elem.parentNode.nextElementSibling;
        var expandcollapse = document.createElement("span");
        if (extras.classList.contains("collapsed")) {
            expandcollapse.classList.add("expander")
        } else if (collapsed.includes(elem.innerHTML)) {
            extras.classList.add("collapsed");
            expandcollapse.classList.add("expander");
        } else {
            expandcollapse.classList.add("collapser");
        }
        elem.appendChild(expandcollapse);

        elem.addEventListener("click", function(event) {
            if (event.currentTarget.parentNode.nextElementSibling.classList.contains("collapsed")) {
                show_extras(event.currentTarget);
            } else {
                hide_extras(event.currentTarget);
            }
        });
    })
}

function get_query_parameter(name) {
    var match = RegExp('[?&]' + name + '=([^&]*)').exec(window.location.search);
    return match && decodeURIComponent(match[1].replace(/\+/g, ' '));
}

function init () {
    reset_sort_headers();

    add_collapse();

    show_filters();

    sort_column(find('.initial-sort'));

    find_all('.sortable').forEach(function(elem) {
        elem.addEventListener("click",
                              function(event) {
                                  sort_column(elem);
                              }, false)
    });

};

function sort_table(clicked, key_func) {
    var rows = find_all('.results-table-row');
    var reversed = !clicked.classList.contains('asc');
    var sorted_rows = sort(rows, key_func, reversed);
    /* Whole table is removed here because browsers acts much slower
     * when appending existing elements.
     */
    var thead = document.getElementById("results-table-head");
    document.getElementById('results-table').remove();
    var parent = document.createElement("table");
    parent.id = "results-table";
    parent.appendChild(thead);
    sorted_rows.forEach(function(elem) {
        parent.appendChild(elem);
    });
    document.getElementsByTagName("BODY")[0].appendChild(parent);
}

function sort(items, key_func, reversed) {
    var sort_array = items.map(function(item, i) {
        return [key_func(item), i];
    });

    sort_array.sort(function(a, b) {
        var key_a = a[0];
        var key_b = b[0];

        if (key_a == key_b) return 0;

        if (reversed) {
            return (key_a < key_b ? 1 : -1);
        } else {
            return (key_a > key_b ? 1 : -1);
        }
    });

    return sort_array.map(function(item) {
        var index = item[1];
        return items[index];
    });
}

function key_alpha(col_index) {
    return function(elem) {
        return elem.childNodes[1].childNodes[col_index].firstChild.data.toLowerCase();
    };
}

function key_num(col_index) {
    return function(elem) {
        return parseFloat(elem.childNodes[1].childNodes[col_index].firstChild.data);
    };
}

function key_result(col_index) {
    return function(elem) {
        var strings = ['Error', 'Failed', 'Rerun', 'XFailed', 'XPassed',
                       'Skipped', 'Passed'];
        return strings.indexOf(elem.childNodes[1].childNodes[col_index].firstChild.data);
    };
}

function reset_sort_headers() {
    find_all('.sort-icon').forEach(function(elem) {
        elem.parentNode.removeChild(elem);
    });
    find_all('.sortable').forEach(function(elem) {
        var icon = document.createElement("div");
        icon.className = "sort-icon";
        icon.textContent = "vvv";
        elem.insertBefore(icon, elem.firstChild);
        elem.classList.remove("desc", "active");
        elem.classList.add("asc", "inactive");
    });
}

function toggle_sort_states(elem) {
    //if active, toggle between asc and desc
    if (elem.classList.contains('active')) {
        elem.classList.toggle('asc');
        elem.classList.toggle('desc');
    }

    //if inactive, reset all other functions and add ascending active
    if (elem.classList.contains('inactive')) {
        reset_sort_headers();
        elem.classList.remove('inactive');
        elem.classList.add('active');
    }
}

function is_all_rows_hidden(value) {
  return value.hidden == false;
}

function filter_table(elem) {
    var outcome_att = "data-test-result";
    var outcome = elem.getAttribute(outcome_att);
    class_outcome = outcome + " results-table-row";
    var outcome_rows = document.getElementsByClassName(class_outcome);

    for(var i = 0; i < outcome_rows.length; i++){
        outcome_rows[i].hidden = !elem.checked;
    }

    var rows = find_all('.results-table-row').filter(is_all_rows_hidden);
    var all_rows_hidden = rows.length == 0 ? true : false;
    var not_found_message = document.getElementById("not-found-message");
    not_found_message.hidden = !all_rows_hidden;
}
</script>
    <h1>test_transform.html</h1>
    <p>Report generated on 27-Jul-2020 at 12:37:53 by <a href="https://pypi.python.org/pypi/pytest-html">pytest-html</a> v2.1.1</p>
    <h2>Environment</h2>
    <table id="environment">
      <tr>
        <td>Packages</td>
        <td>{"pluggy": "0.13.1", "py": "1.9.0", "pytest": "5.4.3"}</td></tr>
      <tr>
        <td>Platform</td>
        <td>Linux-4.9.140-tegra-aarch64-with-Ubuntu-18.04-bionic</td></tr>
      <tr>
        <td>Plugins</td>
        <td>{"benchmark": "3.2.3", "html": "2.1.1", "metadata": "1.10.0"}</td></tr>
      <tr>
        <td>Python</td>
        <td>3.6.9</td></tr></table>
    <h2>Summary</h2>
    <p>77 tests ran in 290.90 seconds. </p>
    <p class="filter" hidden="true">(Un)check the boxes to filter the results.</p><input checked="true" class="filter" data-test-result="passed" hidden="true" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="passed">36 passed</span>, <input checked="true" class="filter" data-test-result="skipped" hidden="true" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="skipped">2 skipped</span>, <input checked="true" class="filter" data-test-result="failed" hidden="true" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="failed">41 failed</span>, <input checked="true" class="filter" data-test-result="error" disabled="true" hidden="true" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="error">0 errors</span>, <input checked="true" class="filter" data-test-result="xfailed" disabled="true" hidden="true" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="xfailed">0 expected failures</span>, <input checked="true" class="filter" data-test-result="xpassed" disabled="true" hidden="true" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="xpassed">0 unexpected passes</span>
    <h2>Results</h2>
    <table id="results-table">
      <thead id="results-table-head">
        <tr>
          <th class="sortable result initial-sort" col="result">Result</th>
          <th class="sortable" col="name">Test</th>
          <th class="sortable numeric" col="duration">Duration</th>
          <th>Links</th></tr>
        <tr hidden="true" id="not-found-message">
          <th colspan="4">No results found. Try to check the filters</th></tr></thead>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_sub[c-cuda:0:0]</td>
          <td class="col-duration">0.03</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7faee9cda0&gt;, method = &#x27;c&#x27;<br/><br/>    def test_sub(thread, method):<br/>        if method == &quot;cuda_asm&quot; and thread.api.get_id() != cluda.cuda_id():<br/>            pytest.skip()<br/>&gt;       check_func(thread, ntt.sub(method=method), ref_sub, &#x27;ff_number&#x27;, [&#x27;ff_number&#x27;, &#x27;ff_number&#x27;])<br/><br/>test/test_transform/test_arithmetic.py:142: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_transform/test_arithmetic.py:101: in check_func<br/>    test = get_func_kernel(thread, func_module, output_type, input_types)<br/>test/test_transform/test_arithmetic.py:90: in get_func_kernel<br/>    repetitions=repetitions))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:537: in compile<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:655: in __init__<br/>    self.source, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:504: in _create_program<br/>    src, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:227: in _compile<br/>    return SourceModule(src, no_extern_c=True, options=options, keep=keep)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;pycuda.compiler.SourceModule object at 0x7fae549400&gt;<br/>source = &#x27;\n\n\n    #define CUDA\n    // taken from pycuda._cluda\n    #define LOCAL_BARRIER __syncthreads()\n\n    #define WIT...\n                res\n                , a2_load\n                );\n        }\n\n        dest[i] = res;\n    }\n    &#x27;<br/>nvcc = &#x27;nvcc&#x27;, options = [], keep = False, no_extern_c = True, arch = None, code = None, cache_dir = None, include_dirs = []<br/><br/>    def __init__(self, source, nvcc=&quot;nvcc&quot;, options=None, keep=False,<br/>            no_extern_c=False, arch=None, code=None, cache_dir=None,<br/>            include_dirs=[]):<br/>        self._check_arch(arch)<br/>    <br/>        cubin = compile(source, nvcc, options, keep, no_extern_c,<br/>                arch, code, cache_dir, include_dirs)<br/>    <br/>        from pycuda.driver import module_from_buffer<br/>&gt;       self.module = module_from_buffer(cubin)<br/><span class="error">E       pycuda._driver.LogicError: cuModuleLoadDataEx failed: invalid device context -</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/compiler.py:294: LogicError<br/> -------------------------------Captured log call-------------------------------- <br/>[31m[1mERROR   [0m root:api.py:507 Failed to compile:
1:
2:
3:
4:    #define CUDA
5:    // taken from pycuda._cluda
6:    #define LOCAL_BARRIER __syncthreads()
7:
8:    #define WITHIN_KERNEL __device__
9:    #define KERNEL extern &quot;C&quot; __global__
10:    #define GLOBAL_MEM /* empty */
11:    #define GLOBAL_MEM_ARG /* empty */
12:    #define LOCAL_MEM __shared__
13:    #define LOCAL_MEM_DYNAMIC extern __shared__
14:    #define LOCAL_MEM_ARG /* empty */
15:    #define CONSTANT_MEM __constant__
16:    #define CONSTANT_MEM_ARG /* empty */
17:    #define INLINE __forceinline__
18:    #define SIZE_T int
19:    #define VSIZE_T int
20:
21:    // used to align fields in structures
22:    #define ALIGN(bytes) __align__(bytes)
23:
24:    
25:
26:    WITHIN_KERNEL SIZE_T get_local_id(unsigned int dim)
27:    {
28:        if(dim == 0) return threadIdx.x;
29:        if(dim == 1) return threadIdx.y;
30:        if(dim == 2) return threadIdx.z;
31:        return 0;
32:    }
33:
34:    WITHIN_KERNEL SIZE_T get_group_id(unsigned int dim)
35:    {
36:        if(dim == 0) return blockIdx.x;
37:        if(dim == 1) return blockIdx.y;
38:        if(dim == 2) return blockIdx.z;
39:        return 0;
40:    }
41:
42:    WITHIN_KERNEL SIZE_T get_local_size(unsigned int dim)
43:    {
44:        if(dim == 0) return blockDim.x;
45:        if(dim == 1) return blockDim.y;
46:        if(dim == 2) return blockDim.z;
47:        return 1;
48:    }
49:
50:    WITHIN_KERNEL SIZE_T get_num_groups(unsigned int dim)
51:    {
52:        if(dim == 0) return gridDim.x;
53:        if(dim == 1) return gridDim.y;
54:        if(dim == 2) return gridDim.z;
55:        return 1;
56:    }
57:
58:    WITHIN_KERNEL SIZE_T get_global_size(unsigned int dim)
59:    {
60:        return get_num_groups(dim) * get_local_size(dim);
61:    }
62:
63:    WITHIN_KERNEL SIZE_T get_global_id(unsigned int dim)
64:    {
65:        return get_local_id(dim) + get_group_id(dim) * get_local_size(dim);
66:    }
67:
68:
69:
70:
71:    #define COMPLEX_CTR(T) make_##T
72:
73:    WITHIN_KERNEL float2 operator+(float2 a, float2 b)
74:    {
75:        return COMPLEX_CTR(float2)(a.x + b.x, a.y + b.y);
76:    }
77:    WITHIN_KERNEL float2 operator-(float2 a, float2 b)
78:    {
79:        return COMPLEX_CTR(float2)(a.x - b.x, a.y - b.y);
80:    }
81:    WITHIN_KERNEL float2 operator+(float2 a) { return a; }
82:    WITHIN_KERNEL float2 operator-(float2 a) { return COMPLEX_CTR(float2)(-a.x, -a.y); }
83:    WITHIN_KERNEL double2 operator+(double2 a, double2 b)
84:    {
85:        return COMPLEX_CTR(double2)(a.x + b.x, a.y + b.y);
86:    }
87:    WITHIN_KERNEL double2 operator-(double2 a, double2 b)
88:    {
89:        return COMPLEX_CTR(double2)(a.x - b.x, a.y - b.y);
90:    }
91:    WITHIN_KERNEL double2 operator+(double2 a) { return a; }
92:    WITHIN_KERNEL double2 operator-(double2 a) { return COMPLEX_CTR(double2)(-a.x, -a.y); }
93:
94:
95:typedef struct __module0_
96:{
97:    unsigned long val;
98:} _module0_;
99:
100:
101:WITHIN_KERNEL INLINE _module0_ _module0_pack(unsigned long x)
102:{
103:    _module0_ res = {x};
104:    return res;
105:}
106:
107:WITHIN_KERNEL INLINE unsigned long _module0_unpack(_module0_ x)
108:{
109:    return x.val;
110:}
111:
112:#define _module0_zero (_module0_pack(0));
113:
114:
115:#define _module0_UNPACK(hi, lo, src) {lo = (src); hi = (src) &gt;&gt; 32;}
116:
117:#ifdef CUDA
118:#define _module0_PACK(hi, lo) (((unsigned long)(hi) &lt;&lt; 32) | (lo))
119:#else
120:#define _module0_PACK(hi, lo) upsample(hi, lo)
121:#endif
122:
123:
124:#define _module0_SUB_CC(hi, lo, x1, x2) { bool cc = (x1) &lt; (x2); lo = (x1) - (x2); if (cc) { hi -= 1; } }
125:#define _module0_ADD_CC(hi, lo, x1, x2, hi_prev) { lo = (x1) + (x2); bool cc = lo &lt; (x2); hi = hi_prev; if (cc) { hi += 1; } }
126:
127:
128:
129:
130:/** Subtraction in FF(P): val_ = a + b mod P. */
131:WITHIN_KERNEL INLINE _module0_ _module1_(_module0_ a, _module0_ b)
132:{
133:
134:    /*
135:    Algorithm:
136:    We calculate `s = x - y`
137:    Now there are three variants:
138:    - no underflow (x &gt;= y): all good, `result = s`.
139:    - underflow (detected if `s &gt; x`), so essentially `s = x - y + N`.
140:      This means we need to calculate `s - N + P = s - (2^32 - 1)`
141:    */
142:
143:    _module0_ res = {a.val - b.val};
144:    unsigned int x = -(res.val &gt; a.val);
145:    res.val -= x;
146:    return res;
147:
148:}
149:
150:
151:
152:    
153:    KERNEL void test(
154:        GLOBAL_MEM _module0_ *dest
155:        , GLOBAL_MEM _module0_ *a1
156:        , GLOBAL_MEM _module0_ *a2
157:        )
158:    {
159:        const SIZE_T i = get_global_id(0);
160:        _module0_ a1_load = a1[i];
161:        _module0_ a2_load = a2[i];
162:
163:        // To stop the compiler from optimizing away the code,
164:        // we need to use the return value somehow.
165:        // We&#x27;re using it as the first argument to the next invocation.
166:        // Assuming here that the first argument of the tested function is either
167:        // a finite field element, or something convertable to it (e.g. uint64).
168:        _module0_ res =
169:            a1_load;
170:
171:        for (int i = 0; i &lt; 1; i++)
172:        {
173:            res = _module1_(
174:                res
175:                , a2_load
176:                );
177:        }
178:
179:        dest[i] = res;
180:    }
181:<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_sub[cuda_asm-cuda:0:0]</td>
          <td class="col-duration">0.03</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7faee9cda0&gt;, method = &#x27;cuda_asm&#x27;<br/><br/>    def test_sub(thread, method):<br/>        if method == &quot;cuda_asm&quot; and thread.api.get_id() != cluda.cuda_id():<br/>            pytest.skip()<br/>&gt;       check_func(thread, ntt.sub(method=method), ref_sub, &#x27;ff_number&#x27;, [&#x27;ff_number&#x27;, &#x27;ff_number&#x27;])<br/><br/>test/test_transform/test_arithmetic.py:142: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_transform/test_arithmetic.py:101: in check_func<br/>    test = get_func_kernel(thread, func_module, output_type, input_types)<br/>test/test_transform/test_arithmetic.py:90: in get_func_kernel<br/>    repetitions=repetitions))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:537: in compile<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:655: in __init__<br/>    self.source, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:504: in _create_program<br/>    src, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:227: in _compile<br/>    return SourceModule(src, no_extern_c=True, options=options, keep=keep)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;pycuda.compiler.SourceModule object at 0x7fae510198&gt;<br/>source = &#x27;\n\n\n    #define CUDA\n    // taken from pycuda._cluda\n    #define LOCAL_BARRIER __syncthreads()\n\n    #define WIT...\n                res\n                , a2_load\n                );\n        }\n\n        dest[i] = res;\n    }\n    &#x27;<br/>nvcc = &#x27;nvcc&#x27;, options = [], keep = False, no_extern_c = True, arch = None, code = None, cache_dir = None, include_dirs = []<br/><br/>    def __init__(self, source, nvcc=&quot;nvcc&quot;, options=None, keep=False,<br/>            no_extern_c=False, arch=None, code=None, cache_dir=None,<br/>            include_dirs=[]):<br/>        self._check_arch(arch)<br/>    <br/>        cubin = compile(source, nvcc, options, keep, no_extern_c,<br/>                arch, code, cache_dir, include_dirs)<br/>    <br/>        from pycuda.driver import module_from_buffer<br/>&gt;       self.module = module_from_buffer(cubin)<br/><span class="error">E       pycuda._driver.LogicError: cuModuleLoadDataEx failed: invalid device context -</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/compiler.py:294: LogicError<br/> -------------------------------Captured log call-------------------------------- <br/>[31m[1mERROR   [0m root:api.py:507 Failed to compile:
1:
2:
3:
4:    #define CUDA
5:    // taken from pycuda._cluda
6:    #define LOCAL_BARRIER __syncthreads()
7:
8:    #define WITHIN_KERNEL __device__
9:    #define KERNEL extern &quot;C&quot; __global__
10:    #define GLOBAL_MEM /* empty */
11:    #define GLOBAL_MEM_ARG /* empty */
12:    #define LOCAL_MEM __shared__
13:    #define LOCAL_MEM_DYNAMIC extern __shared__
14:    #define LOCAL_MEM_ARG /* empty */
15:    #define CONSTANT_MEM __constant__
16:    #define CONSTANT_MEM_ARG /* empty */
17:    #define INLINE __forceinline__
18:    #define SIZE_T int
19:    #define VSIZE_T int
20:
21:    // used to align fields in structures
22:    #define ALIGN(bytes) __align__(bytes)
23:
24:    
25:
26:    WITHIN_KERNEL SIZE_T get_local_id(unsigned int dim)
27:    {
28:        if(dim == 0) return threadIdx.x;
29:        if(dim == 1) return threadIdx.y;
30:        if(dim == 2) return threadIdx.z;
31:        return 0;
32:    }
33:
34:    WITHIN_KERNEL SIZE_T get_group_id(unsigned int dim)
35:    {
36:        if(dim == 0) return blockIdx.x;
37:        if(dim == 1) return blockIdx.y;
38:        if(dim == 2) return blockIdx.z;
39:        return 0;
40:    }
41:
42:    WITHIN_KERNEL SIZE_T get_local_size(unsigned int dim)
43:    {
44:        if(dim == 0) return blockDim.x;
45:        if(dim == 1) return blockDim.y;
46:        if(dim == 2) return blockDim.z;
47:        return 1;
48:    }
49:
50:    WITHIN_KERNEL SIZE_T get_num_groups(unsigned int dim)
51:    {
52:        if(dim == 0) return gridDim.x;
53:        if(dim == 1) return gridDim.y;
54:        if(dim == 2) return gridDim.z;
55:        return 1;
56:    }
57:
58:    WITHIN_KERNEL SIZE_T get_global_size(unsigned int dim)
59:    {
60:        return get_num_groups(dim) * get_local_size(dim);
61:    }
62:
63:    WITHIN_KERNEL SIZE_T get_global_id(unsigned int dim)
64:    {
65:        return get_local_id(dim) + get_group_id(dim) * get_local_size(dim);
66:    }
67:
68:
69:
70:
71:    #define COMPLEX_CTR(T) make_##T
72:
73:    WITHIN_KERNEL float2 operator+(float2 a, float2 b)
74:    {
75:        return COMPLEX_CTR(float2)(a.x + b.x, a.y + b.y);
76:    }
77:    WITHIN_KERNEL float2 operator-(float2 a, float2 b)
78:    {
79:        return COMPLEX_CTR(float2)(a.x - b.x, a.y - b.y);
80:    }
81:    WITHIN_KERNEL float2 operator+(float2 a) { return a; }
82:    WITHIN_KERNEL float2 operator-(float2 a) { return COMPLEX_CTR(float2)(-a.x, -a.y); }
83:    WITHIN_KERNEL double2 operator+(double2 a, double2 b)
84:    {
85:        return COMPLEX_CTR(double2)(a.x + b.x, a.y + b.y);
86:    }
87:    WITHIN_KERNEL double2 operator-(double2 a, double2 b)
88:    {
89:        return COMPLEX_CTR(double2)(a.x - b.x, a.y - b.y);
90:    }
91:    WITHIN_KERNEL double2 operator+(double2 a) { return a; }
92:    WITHIN_KERNEL double2 operator-(double2 a) { return COMPLEX_CTR(double2)(-a.x, -a.y); }
93:
94:
95:typedef struct __module0_
96:{
97:    unsigned long val;
98:} _module0_;
99:
100:
101:WITHIN_KERNEL INLINE _module0_ _module0_pack(unsigned long x)
102:{
103:    _module0_ res = {x};
104:    return res;
105:}
106:
107:WITHIN_KERNEL INLINE unsigned long _module0_unpack(_module0_ x)
108:{
109:    return x.val;
110:}
111:
112:#define _module0_zero (_module0_pack(0));
113:
114:
115:#define _module0_UNPACK(hi, lo, src) {lo = (src); hi = (src) &gt;&gt; 32;}
116:
117:#ifdef CUDA
118:#define _module0_PACK(hi, lo) (((unsigned long)(hi) &lt;&lt; 32) | (lo))
119:#else
120:#define _module0_PACK(hi, lo) upsample(hi, lo)
121:#endif
122:
123:
124:#define _module0_SUB_CC(hi, lo, x1, x2) { bool cc = (x1) &lt; (x2); lo = (x1) - (x2); if (cc) { hi -= 1; } }
125:#define _module0_ADD_CC(hi, lo, x1, x2, hi_prev) { lo = (x1) + (x2); bool cc = lo &lt; (x2); hi = hi_prev; if (cc) { hi += 1; } }
126:
127:
128:
129:
130:/** Subtraction in FF(P): val_ = a + b mod P. */
131:WITHIN_KERNEL INLINE _module0_ _module1_(_module0_ a, _module0_ b)
132:{
133:
134:    _module0_ res = {0};
135:    asm(&quot;{\n\t&quot;
136:        &quot;.reg .u32          m;\n\t&quot;
137:        &quot;.reg .u64          t;\n\t&quot;
138:        // this = a - b;
139:        &quot;sub.u64            %0, %1, %2;\n\t&quot;
140:        // this -= (uint32_t)(-(this &gt; a));
141:        &quot;set.gt.u32.u64     m, %0, %1;\n\t&quot;
142:        &quot;mov.b64            t, {m, 0};\n\t&quot;
143:        &quot;sub.u64            %0, %0, t;\n\t&quot;
144:        &quot;}&quot;
145:        : &quot;+l&quot;(res.val)
146:        : &quot;l&quot;(a.val), &quot;l&quot;(b.val));
147:    return res;
148:
149:}
150:
151:
152:
153:    
154:    KERNEL void test(
155:        GLOBAL_MEM _module0_ *dest
156:        , GLOBAL_MEM _module0_ *a1
157:        , GLOBAL_MEM _module0_ *a2
158:        )
159:    {
160:        const SIZE_T i = get_global_id(0);
161:        _module0_ a1_load = a1[i];
162:        _module0_ a2_load = a2[i];
163:
164:        // To stop the compiler from optimizing away the code,
165:        // we need to use the return value somehow.
166:        // We&#x27;re using it as the first argument to the next invocation.
167:        // Assuming here that the first argument of the tested function is either
168:        // a finite field element, or something convertable to it (e.g. uint64).
169:        _module0_ res =
170:            a1_load;
171:
172:        for (int i = 0; i &lt; 1; i++)
173:        {
174:            res = _module1_(
175:                res
176:                , a2_load
177:                );
178:        }
179:
180:        dest[i] = res;
181:    }
182:<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_sub[c_from_asm-cuda:0:0]</td>
          <td class="col-duration">0.03</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7faee9cda0&gt;, method = &#x27;c_from_asm&#x27;<br/><br/>    def test_sub(thread, method):<br/>        if method == &quot;cuda_asm&quot; and thread.api.get_id() != cluda.cuda_id():<br/>            pytest.skip()<br/>&gt;       check_func(thread, ntt.sub(method=method), ref_sub, &#x27;ff_number&#x27;, [&#x27;ff_number&#x27;, &#x27;ff_number&#x27;])<br/><br/>test/test_transform/test_arithmetic.py:142: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_transform/test_arithmetic.py:101: in check_func<br/>    test = get_func_kernel(thread, func_module, output_type, input_types)<br/>test/test_transform/test_arithmetic.py:90: in get_func_kernel<br/>    repetitions=repetitions))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:537: in compile<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:655: in __init__<br/>    self.source, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:504: in _create_program<br/>    src, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:227: in _compile<br/>    return SourceModule(src, no_extern_c=True, options=options, keep=keep)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;pycuda.compiler.SourceModule object at 0x7fae4dad30&gt;<br/>source = &#x27;\n\n\n    #define CUDA\n    // taken from pycuda._cluda\n    #define LOCAL_BARRIER __syncthreads()\n\n    #define WIT...\n                res\n                , a2_load\n                );\n        }\n\n        dest[i] = res;\n    }\n    &#x27;<br/>nvcc = &#x27;nvcc&#x27;, options = [], keep = False, no_extern_c = True, arch = None, code = None, cache_dir = None, include_dirs = []<br/><br/>    def __init__(self, source, nvcc=&quot;nvcc&quot;, options=None, keep=False,<br/>            no_extern_c=False, arch=None, code=None, cache_dir=None,<br/>            include_dirs=[]):<br/>        self._check_arch(arch)<br/>    <br/>        cubin = compile(source, nvcc, options, keep, no_extern_c,<br/>                arch, code, cache_dir, include_dirs)<br/>    <br/>        from pycuda.driver import module_from_buffer<br/>&gt;       self.module = module_from_buffer(cubin)<br/><span class="error">E       pycuda._driver.LogicError: cuModuleLoadDataEx failed: invalid device context -</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/compiler.py:294: LogicError<br/> -------------------------------Captured log call-------------------------------- <br/>[31m[1mERROR   [0m root:api.py:507 Failed to compile:
1:
2:
3:
4:    #define CUDA
5:    // taken from pycuda._cluda
6:    #define LOCAL_BARRIER __syncthreads()
7:
8:    #define WITHIN_KERNEL __device__
9:    #define KERNEL extern &quot;C&quot; __global__
10:    #define GLOBAL_MEM /* empty */
11:    #define GLOBAL_MEM_ARG /* empty */
12:    #define LOCAL_MEM __shared__
13:    #define LOCAL_MEM_DYNAMIC extern __shared__
14:    #define LOCAL_MEM_ARG /* empty */
15:    #define CONSTANT_MEM __constant__
16:    #define CONSTANT_MEM_ARG /* empty */
17:    #define INLINE __forceinline__
18:    #define SIZE_T int
19:    #define VSIZE_T int
20:
21:    // used to align fields in structures
22:    #define ALIGN(bytes) __align__(bytes)
23:
24:    
25:
26:    WITHIN_KERNEL SIZE_T get_local_id(unsigned int dim)
27:    {
28:        if(dim == 0) return threadIdx.x;
29:        if(dim == 1) return threadIdx.y;
30:        if(dim == 2) return threadIdx.z;
31:        return 0;
32:    }
33:
34:    WITHIN_KERNEL SIZE_T get_group_id(unsigned int dim)
35:    {
36:        if(dim == 0) return blockIdx.x;
37:        if(dim == 1) return blockIdx.y;
38:        if(dim == 2) return blockIdx.z;
39:        return 0;
40:    }
41:
42:    WITHIN_KERNEL SIZE_T get_local_size(unsigned int dim)
43:    {
44:        if(dim == 0) return blockDim.x;
45:        if(dim == 1) return blockDim.y;
46:        if(dim == 2) return blockDim.z;
47:        return 1;
48:    }
49:
50:    WITHIN_KERNEL SIZE_T get_num_groups(unsigned int dim)
51:    {
52:        if(dim == 0) return gridDim.x;
53:        if(dim == 1) return gridDim.y;
54:        if(dim == 2) return gridDim.z;
55:        return 1;
56:    }
57:
58:    WITHIN_KERNEL SIZE_T get_global_size(unsigned int dim)
59:    {
60:        return get_num_groups(dim) * get_local_size(dim);
61:    }
62:
63:    WITHIN_KERNEL SIZE_T get_global_id(unsigned int dim)
64:    {
65:        return get_local_id(dim) + get_group_id(dim) * get_local_size(dim);
66:    }
67:
68:
69:
70:
71:    #define COMPLEX_CTR(T) make_##T
72:
73:    WITHIN_KERNEL float2 operator+(float2 a, float2 b)
74:    {
75:        return COMPLEX_CTR(float2)(a.x + b.x, a.y + b.y);
76:    }
77:    WITHIN_KERNEL float2 operator-(float2 a, float2 b)
78:    {
79:        return COMPLEX_CTR(float2)(a.x - b.x, a.y - b.y);
80:    }
81:    WITHIN_KERNEL float2 operator+(float2 a) { return a; }
82:    WITHIN_KERNEL float2 operator-(float2 a) { return COMPLEX_CTR(float2)(-a.x, -a.y); }
83:    WITHIN_KERNEL double2 operator+(double2 a, double2 b)
84:    {
85:        return COMPLEX_CTR(double2)(a.x + b.x, a.y + b.y);
86:    }
87:    WITHIN_KERNEL double2 operator-(double2 a, double2 b)
88:    {
89:        return COMPLEX_CTR(double2)(a.x - b.x, a.y - b.y);
90:    }
91:    WITHIN_KERNEL double2 operator+(double2 a) { return a; }
92:    WITHIN_KERNEL double2 operator-(double2 a) { return COMPLEX_CTR(double2)(-a.x, -a.y); }
93:
94:
95:typedef struct __module0_
96:{
97:    unsigned long val;
98:} _module0_;
99:
100:
101:WITHIN_KERNEL INLINE _module0_ _module0_pack(unsigned long x)
102:{
103:    _module0_ res = {x};
104:    return res;
105:}
106:
107:WITHIN_KERNEL INLINE unsigned long _module0_unpack(_module0_ x)
108:{
109:    return x.val;
110:}
111:
112:#define _module0_zero (_module0_pack(0));
113:
114:
115:#define _module0_UNPACK(hi, lo, src) {lo = (src); hi = (src) &gt;&gt; 32;}
116:
117:#ifdef CUDA
118:#define _module0_PACK(hi, lo) (((unsigned long)(hi) &lt;&lt; 32) | (lo))
119:#else
120:#define _module0_PACK(hi, lo) upsample(hi, lo)
121:#endif
122:
123:
124:#define _module0_SUB_CC(hi, lo, x1, x2) { bool cc = (x1) &lt; (x2); lo = (x1) - (x2); if (cc) { hi -= 1; } }
125:#define _module0_ADD_CC(hi, lo, x1, x2, hi_prev) { lo = (x1) + (x2); bool cc = lo &lt; (x2); hi = hi_prev; if (cc) { hi += 1; } }
126:
127:
128:
129:
130:/** Subtraction in FF(P): val_ = a + b mod P. */
131:WITHIN_KERNEL INLINE _module0_ _module1_(_module0_ a, _module0_ b)
132:{
133:
134:    /*
135:    Algorithm:
136:    We calculate `s = x - y`
137:    Now there are three variants:
138:    - no underflow (x &gt;= y): all good, `result = s`.
139:    - underflow (detected if `s &gt; x`), so essentially `s = x - y + N`.
140:      This means we need to calculate `s - N + P = s - (2^32 - 1)`
141:    */
142:
143:    _module0_ res = {a.val - b.val};
144:    unsigned int x = -(res.val &gt; a.val);
145:    res.val -= x;
146:    return res;
147:
148:}
149:
150:
151:
152:    
153:    KERNEL void test(
154:        GLOBAL_MEM _module0_ *dest
155:        , GLOBAL_MEM _module0_ *a1
156:        , GLOBAL_MEM _module0_ *a2
157:        )
158:    {
159:        const SIZE_T i = get_global_id(0);
160:        _module0_ a1_load = a1[i];
161:        _module0_ a2_load = a2[i];
162:
163:        // To stop the compiler from optimizing away the code,
164:        // we need to use the return value somehow.
165:        // We&#x27;re using it as the first argument to the next invocation.
166:        // Assuming here that the first argument of the tested function is either
167:        // a finite field element, or something convertable to it (e.g. uint64).
168:        _module0_ res =
169:            a1_load;
170:
171:        for (int i = 0; i &lt; 1; i++)
172:        {
173:            res = _module1_(
174:                res
175:                , a2_load
176:                );
177:        }
178:
179:        dest[i] = res;
180:    }
181:<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_mul[c-cuda:0:0]</td>
          <td class="col-duration">0.04</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7fae5eb240&gt;, method = &#x27;c&#x27;<br/><br/>    def test_mul(thread, method):<br/>        if method == &quot;cuda_asm&quot; and thread.api.get_id() != cluda.cuda_id():<br/>            pytest.skip()<br/>        check_func(<br/>            thread, ntt.mul(method=method), ref_mul, &#x27;ff_number&#x27;, [&#x27;ff_number&#x27;, &#x27;ff_number&#x27;],<br/>            test_values=[<br/>&gt;               (ntt_cpu.GaloisNumber.modulus - 1, 2**33) # regression test for an error in method=c<br/>                ]<br/>            )<br/><br/>test/test_transform/test_arithmetic.py:172: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_transform/test_arithmetic.py:101: in check_func<br/>    test = get_func_kernel(thread, func_module, output_type, input_types)<br/>test/test_transform/test_arithmetic.py:90: in get_func_kernel<br/>    repetitions=repetitions))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:537: in compile<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:655: in __init__<br/>    self.source, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:504: in _create_program<br/>    src, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:227: in _compile<br/>    return SourceModule(src, no_extern_c=True, options=options, keep=keep)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;pycuda.compiler.SourceModule object at 0x7faee7fe80&gt;<br/>source = &#x27;\n\n\n    #define CUDA\n    // taken from pycuda._cluda\n    #define LOCAL_BARRIER __syncthreads()\n\n    #define WIT...\n                res\n                , a2_load\n                );\n        }\n\n        dest[i] = res;\n    }\n    &#x27;<br/>nvcc = &#x27;nvcc&#x27;, options = [], keep = False, no_extern_c = True, arch = None, code = None, cache_dir = None, include_dirs = []<br/><br/>    def __init__(self, source, nvcc=&quot;nvcc&quot;, options=None, keep=False,<br/>            no_extern_c=False, arch=None, code=None, cache_dir=None,<br/>            include_dirs=[]):<br/>        self._check_arch(arch)<br/>    <br/>        cubin = compile(source, nvcc, options, keep, no_extern_c,<br/>                arch, code, cache_dir, include_dirs)<br/>    <br/>        from pycuda.driver import module_from_buffer<br/>&gt;       self.module = module_from_buffer(cubin)<br/><span class="error">E       pycuda._driver.LogicError: cuModuleLoadDataEx failed: invalid device context -</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/compiler.py:294: LogicError<br/> -------------------------------Captured log call-------------------------------- <br/>[31m[1mERROR   [0m root:api.py:507 Failed to compile:
1:
2:
3:
4:    #define CUDA
5:    // taken from pycuda._cluda
6:    #define LOCAL_BARRIER __syncthreads()
7:
8:    #define WITHIN_KERNEL __device__
9:    #define KERNEL extern &quot;C&quot; __global__
10:    #define GLOBAL_MEM /* empty */
11:    #define GLOBAL_MEM_ARG /* empty */
12:    #define LOCAL_MEM __shared__
13:    #define LOCAL_MEM_DYNAMIC extern __shared__
14:    #define LOCAL_MEM_ARG /* empty */
15:    #define CONSTANT_MEM __constant__
16:    #define CONSTANT_MEM_ARG /* empty */
17:    #define INLINE __forceinline__
18:    #define SIZE_T int
19:    #define VSIZE_T int
20:
21:    // used to align fields in structures
22:    #define ALIGN(bytes) __align__(bytes)
23:
24:    
25:
26:    WITHIN_KERNEL SIZE_T get_local_id(unsigned int dim)
27:    {
28:        if(dim == 0) return threadIdx.x;
29:        if(dim == 1) return threadIdx.y;
30:        if(dim == 2) return threadIdx.z;
31:        return 0;
32:    }
33:
34:    WITHIN_KERNEL SIZE_T get_group_id(unsigned int dim)
35:    {
36:        if(dim == 0) return blockIdx.x;
37:        if(dim == 1) return blockIdx.y;
38:        if(dim == 2) return blockIdx.z;
39:        return 0;
40:    }
41:
42:    WITHIN_KERNEL SIZE_T get_local_size(unsigned int dim)
43:    {
44:        if(dim == 0) return blockDim.x;
45:        if(dim == 1) return blockDim.y;
46:        if(dim == 2) return blockDim.z;
47:        return 1;
48:    }
49:
50:    WITHIN_KERNEL SIZE_T get_num_groups(unsigned int dim)
51:    {
52:        if(dim == 0) return gridDim.x;
53:        if(dim == 1) return gridDim.y;
54:        if(dim == 2) return gridDim.z;
55:        return 1;
56:    }
57:
58:    WITHIN_KERNEL SIZE_T get_global_size(unsigned int dim)
59:    {
60:        return get_num_groups(dim) * get_local_size(dim);
61:    }
62:
63:    WITHIN_KERNEL SIZE_T get_global_id(unsigned int dim)
64:    {
65:        return get_local_id(dim) + get_group_id(dim) * get_local_size(dim);
66:    }
67:
68:
69:
70:
71:    #define COMPLEX_CTR(T) make_##T
72:
73:    WITHIN_KERNEL float2 operator+(float2 a, float2 b)
74:    {
75:        return COMPLEX_CTR(float2)(a.x + b.x, a.y + b.y);
76:    }
77:    WITHIN_KERNEL float2 operator-(float2 a, float2 b)
78:    {
79:        return COMPLEX_CTR(float2)(a.x - b.x, a.y - b.y);
80:    }
81:    WITHIN_KERNEL float2 operator+(float2 a) { return a; }
82:    WITHIN_KERNEL float2 operator-(float2 a) { return COMPLEX_CTR(float2)(-a.x, -a.y); }
83:    WITHIN_KERNEL double2 operator+(double2 a, double2 b)
84:    {
85:        return COMPLEX_CTR(double2)(a.x + b.x, a.y + b.y);
86:    }
87:    WITHIN_KERNEL double2 operator-(double2 a, double2 b)
88:    {
89:        return COMPLEX_CTR(double2)(a.x - b.x, a.y - b.y);
90:    }
91:    WITHIN_KERNEL double2 operator+(double2 a) { return a; }
92:    WITHIN_KERNEL double2 operator-(double2 a) { return COMPLEX_CTR(double2)(-a.x, -a.y); }
93:
94:
95:typedef struct __module0_
96:{
97:    unsigned long val;
98:} _module0_;
99:
100:
101:WITHIN_KERNEL INLINE _module0_ _module0_pack(unsigned long x)
102:{
103:    _module0_ res = {x};
104:    return res;
105:}
106:
107:WITHIN_KERNEL INLINE unsigned long _module0_unpack(_module0_ x)
108:{
109:    return x.val;
110:}
111:
112:#define _module0_zero (_module0_pack(0));
113:
114:
115:#define _module0_UNPACK(hi, lo, src) {lo = (src); hi = (src) &gt;&gt; 32;}
116:
117:#ifdef CUDA
118:#define _module0_PACK(hi, lo) (((unsigned long)(hi) &lt;&lt; 32) | (lo))
119:#else
120:#define _module0_PACK(hi, lo) upsample(hi, lo)
121:#endif
122:
123:
124:#define _module0_SUB_CC(hi, lo, x1, x2) { bool cc = (x1) &lt; (x2); lo = (x1) - (x2); if (cc) { hi -= 1; } }
125:#define _module0_ADD_CC(hi, lo, x1, x2, hi_prev) { lo = (x1) + (x2); bool cc = lo &lt; (x2); hi = hi_prev; if (cc) { hi += 1; } }
126:
127:
128:
129:
130:// Addition in FF(P): val_ = a + b mod P.
131:WITHIN_KERNEL INLINE _module0_ _module2_(_module0_ a, _module0_ b)
132:{
133:
134:    /*
135:    Algorithm:
136:    We calculate `s = x + y`
137:    Now there are three variants:
138:    - `s &lt; P` and no integer overflow: all good, `result = s`.
139:    - `s &gt; P` and no integer overflow: `result = s - P = s + (2^32 - 1)`
140:    - integer overflow, so essentially `s = x + y - N`.
141:      This means that we need to calculate `result = s + N - P = s + (2^32 - 1)`.
142:    Note that the last two variants result in the same modifier being applied.
143:    */
144:    _module0_ res = {a.val + b.val};
145:    res.val += ((res.val &lt; b.val) || res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
146:    return res;
147:
148:}
149:
150:
151:
152:WITHIN_KERNEL INLINE _module0_ _module1_(_module0_ a, _module0_ b)
153:{
154:    /*
155:    Algorithm:
156:    Let M = 2^32
157:    Then `(a * b) = m0 + m1 * M + m2 * M^2 + m3 * M^3`
158:    and `(a * b) mod P = (m0 - m2 - m3 + (m1 + m2) * M) mod P`.
159:    Now `m0 - m2 - m3` can range from `-2M` to `M`, so if it&#x27;s negative,
160:    we need to carry 1 or 2 into the sum `m1 + m2`, and process the overflow correctly.
161:    */
162:
163:
164:        // 128-bit = 64-bit * 64-bit
165:        unsigned int r0, r1, m0, m1, m2, m3;
166:
167:        #ifdef CUDA
168:        unsigned long m3_m2 = __umul64hi(a.val, b.val);
169:        #else
170:        unsigned long m3_m2 = mul_hi(a.val, b.val);
171:        #endif
172:        unsigned long m1_m0 = a.val * b.val;
173:
174:        _module0_UNPACK(m3, m2, m3_m2);
175:        _module0_UNPACK(m1, m0, m1_m0);
176:
177:        bool c1, c2;
178:
179:        r1 = m1;
180:        r0 = -m3;
181:        c1 = m1 &gt; 0;
182:        c2 = m3 &gt; 0;
183:        if (c2) r1--;
184:        if (!c1 &amp;&amp; c2) {r0++; if (m3 &gt; 1) r1--; }
185:        _module0_ x1 = { _module0_PACK(r1, r0) }; // m1 * M - m3
186:
187:        r1 = m2;
188:        r0 = -m2;
189:        c1 = m2 &gt; 0;
190:        if (c1) r1--;
191:        // at this point we have (r1, r0) = (m2 * M - m2) mod P
192:
193:        r0 += m0;
194:        c1 = r0 &lt; m0;
195:        if (c1) { r1++; }
196:        c2 = (r0 &gt; 0 &amp;&amp; r1 == 0xffffffff);
197:        if (r0 &gt; 0 &amp;&amp; r1 == 0xffffffff) { r1 = 0; r0--; }
198:
199:        _module0_ x2 = { _module0_PACK(r1, r0) }; // m2 * M - m2 + m0
200:
201:        return _module2_(x1, x2);
202:
203:
204:}
205:
206:
207:
208:    
209:    KERNEL void test(
210:        GLOBAL_MEM _module0_ *dest
211:        , GLOBAL_MEM _module0_ *a1
212:        , GLOBAL_MEM _module0_ *a2
213:        )
214:    {
215:        const SIZE_T i = get_global_id(0);
216:        _module0_ a1_load = a1[i];
217:        _module0_ a2_load = a2[i];
218:
219:        // To stop the compiler from optimizing away the code,
220:        // we need to use the return value somehow.
221:        // We&#x27;re using it as the first argument to the next invocation.
222:        // Assuming here that the first argument of the tested function is either
223:        // a finite field element, or something convertable to it (e.g. uint64).
224:        _module0_ res =
225:            a1_load;
226:
227:        for (int i = 0; i &lt; 1; i++)
228:        {
229:            res = _module1_(
230:                res
231:                , a2_load
232:                );
233:        }
234:
235:        dest[i] = res;
236:    }
237:<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_mul[cuda_asm-cuda:0:0]</td>
          <td class="col-duration">0.03</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7fae5eb240&gt;, method = &#x27;cuda_asm&#x27;<br/><br/>    def test_mul(thread, method):<br/>        if method == &quot;cuda_asm&quot; and thread.api.get_id() != cluda.cuda_id():<br/>            pytest.skip()<br/>        check_func(<br/>            thread, ntt.mul(method=method), ref_mul, &#x27;ff_number&#x27;, [&#x27;ff_number&#x27;, &#x27;ff_number&#x27;],<br/>            test_values=[<br/>&gt;               (ntt_cpu.GaloisNumber.modulus - 1, 2**33) # regression test for an error in method=c<br/>                ]<br/>            )<br/><br/>test/test_transform/test_arithmetic.py:172: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_transform/test_arithmetic.py:101: in check_func<br/>    test = get_func_kernel(thread, func_module, output_type, input_types)<br/>test/test_transform/test_arithmetic.py:90: in get_func_kernel<br/>    repetitions=repetitions))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:537: in compile<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:655: in __init__<br/>    self.source, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:504: in _create_program<br/>    src, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:227: in _compile<br/>    return SourceModule(src, no_extern_c=True, options=options, keep=keep)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;pycuda.compiler.SourceModule object at 0x7fb454d048&gt;<br/>source = &#x27;\n\n\n    #define CUDA\n    // taken from pycuda._cluda\n    #define LOCAL_BARRIER __syncthreads()\n\n    #define WIT...\n                res\n                , a2_load\n                );\n        }\n\n        dest[i] = res;\n    }\n    &#x27;<br/>nvcc = &#x27;nvcc&#x27;, options = [], keep = False, no_extern_c = True, arch = None, code = None, cache_dir = None, include_dirs = []<br/><br/>    def __init__(self, source, nvcc=&quot;nvcc&quot;, options=None, keep=False,<br/>            no_extern_c=False, arch=None, code=None, cache_dir=None,<br/>            include_dirs=[]):<br/>        self._check_arch(arch)<br/>    <br/>        cubin = compile(source, nvcc, options, keep, no_extern_c,<br/>                arch, code, cache_dir, include_dirs)<br/>    <br/>        from pycuda.driver import module_from_buffer<br/>&gt;       self.module = module_from_buffer(cubin)<br/><span class="error">E       pycuda._driver.LogicError: cuModuleLoadDataEx failed: invalid device context -</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/compiler.py:294: LogicError<br/> -------------------------------Captured log call-------------------------------- <br/>[31m[1mERROR   [0m root:api.py:507 Failed to compile:
1:
2:
3:
4:    #define CUDA
5:    // taken from pycuda._cluda
6:    #define LOCAL_BARRIER __syncthreads()
7:
8:    #define WITHIN_KERNEL __device__
9:    #define KERNEL extern &quot;C&quot; __global__
10:    #define GLOBAL_MEM /* empty */
11:    #define GLOBAL_MEM_ARG /* empty */
12:    #define LOCAL_MEM __shared__
13:    #define LOCAL_MEM_DYNAMIC extern __shared__
14:    #define LOCAL_MEM_ARG /* empty */
15:    #define CONSTANT_MEM __constant__
16:    #define CONSTANT_MEM_ARG /* empty */
17:    #define INLINE __forceinline__
18:    #define SIZE_T int
19:    #define VSIZE_T int
20:
21:    // used to align fields in structures
22:    #define ALIGN(bytes) __align__(bytes)
23:
24:    
25:
26:    WITHIN_KERNEL SIZE_T get_local_id(unsigned int dim)
27:    {
28:        if(dim == 0) return threadIdx.x;
29:        if(dim == 1) return threadIdx.y;
30:        if(dim == 2) return threadIdx.z;
31:        return 0;
32:    }
33:
34:    WITHIN_KERNEL SIZE_T get_group_id(unsigned int dim)
35:    {
36:        if(dim == 0) return blockIdx.x;
37:        if(dim == 1) return blockIdx.y;
38:        if(dim == 2) return blockIdx.z;
39:        return 0;
40:    }
41:
42:    WITHIN_KERNEL SIZE_T get_local_size(unsigned int dim)
43:    {
44:        if(dim == 0) return blockDim.x;
45:        if(dim == 1) return blockDim.y;
46:        if(dim == 2) return blockDim.z;
47:        return 1;
48:    }
49:
50:    WITHIN_KERNEL SIZE_T get_num_groups(unsigned int dim)
51:    {
52:        if(dim == 0) return gridDim.x;
53:        if(dim == 1) return gridDim.y;
54:        if(dim == 2) return gridDim.z;
55:        return 1;
56:    }
57:
58:    WITHIN_KERNEL SIZE_T get_global_size(unsigned int dim)
59:    {
60:        return get_num_groups(dim) * get_local_size(dim);
61:    }
62:
63:    WITHIN_KERNEL SIZE_T get_global_id(unsigned int dim)
64:    {
65:        return get_local_id(dim) + get_group_id(dim) * get_local_size(dim);
66:    }
67:
68:
69:
70:
71:    #define COMPLEX_CTR(T) make_##T
72:
73:    WITHIN_KERNEL float2 operator+(float2 a, float2 b)
74:    {
75:        return COMPLEX_CTR(float2)(a.x + b.x, a.y + b.y);
76:    }
77:    WITHIN_KERNEL float2 operator-(float2 a, float2 b)
78:    {
79:        return COMPLEX_CTR(float2)(a.x - b.x, a.y - b.y);
80:    }
81:    WITHIN_KERNEL float2 operator+(float2 a) { return a; }
82:    WITHIN_KERNEL float2 operator-(float2 a) { return COMPLEX_CTR(float2)(-a.x, -a.y); }
83:    WITHIN_KERNEL double2 operator+(double2 a, double2 b)
84:    {
85:        return COMPLEX_CTR(double2)(a.x + b.x, a.y + b.y);
86:    }
87:    WITHIN_KERNEL double2 operator-(double2 a, double2 b)
88:    {
89:        return COMPLEX_CTR(double2)(a.x - b.x, a.y - b.y);
90:    }
91:    WITHIN_KERNEL double2 operator+(double2 a) { return a; }
92:    WITHIN_KERNEL double2 operator-(double2 a) { return COMPLEX_CTR(double2)(-a.x, -a.y); }
93:
94:
95:typedef struct __module0_
96:{
97:    unsigned long val;
98:} _module0_;
99:
100:
101:WITHIN_KERNEL INLINE _module0_ _module0_pack(unsigned long x)
102:{
103:    _module0_ res = {x};
104:    return res;
105:}
106:
107:WITHIN_KERNEL INLINE unsigned long _module0_unpack(_module0_ x)
108:{
109:    return x.val;
110:}
111:
112:#define _module0_zero (_module0_pack(0));
113:
114:
115:#define _module0_UNPACK(hi, lo, src) {lo = (src); hi = (src) &gt;&gt; 32;}
116:
117:#ifdef CUDA
118:#define _module0_PACK(hi, lo) (((unsigned long)(hi) &lt;&lt; 32) | (lo))
119:#else
120:#define _module0_PACK(hi, lo) upsample(hi, lo)
121:#endif
122:
123:
124:#define _module0_SUB_CC(hi, lo, x1, x2) { bool cc = (x1) &lt; (x2); lo = (x1) - (x2); if (cc) { hi -= 1; } }
125:#define _module0_ADD_CC(hi, lo, x1, x2, hi_prev) { lo = (x1) + (x2); bool cc = lo &lt; (x2); hi = hi_prev; if (cc) { hi += 1; } }
126:
127:
128:
129:
130:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
131:WITHIN_KERNEL INLINE _module0_ _module2_(unsigned long a)
132:{
133:
134:    // uses the fact that 2 * P &gt; max(UInt64)
135:    // and that a::UInt64 - P == a + 2^32 - 1
136:
137:    _module0_ res = {a};
138:    res.val += (res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
139:    return res;
140:
141:}
142:
143:
144:
145:WITHIN_KERNEL INLINE _module0_ _module1_(_module0_ a, _module0_ b)
146:{
147:    /*
148:    Algorithm:
149:    Let M = 2^32
150:    Then `(a * b) = m0 + m1 * M + m2 * M^2 + m3 * M^3`
151:    and `(a * b) mod P = (m0 - m2 - m3 + (m1 + m2) * M) mod P`.
152:    Now `m0 - m2 - m3` can range from `-2M` to `M`, so if it&#x27;s negative,
153:    we need to carry 1 or 2 into the sum `m1 + m2`, and process the overflow correctly.
154:    */
155:
156:
157:    unsigned long res = 0;
158:    asm(&quot;{\n\t&quot;
159:        &quot;.reg .u32          r0, r1;\n\t&quot;
160:        &quot;.reg .u32          m0, m1, m2, m3;\n\t&quot;
161:        &quot;.reg .u64          t;\n\t&quot;
162:        &quot;.reg .pred         p, q;\n\t&quot;
163:        // 128-bit = 64-bit * 64-bit
164:        &quot;mul.lo.u64         t, %1, %2;\n\t&quot;
165:        &quot;mov.b64            {m0, m1}, t;\n\t&quot;
166:        &quot;mul.hi.u64         t, %1, %2;\n\t&quot;
167:        &quot;mov.b64            {m2, m3}, t;\n\t&quot;
168:        // 128-bit mod P with add / sub
169:        &quot;add.u32            r1, m1, m2;\n\t&quot;
170:        &quot;sub.cc.u32         r0, m0, m2;\n\t&quot;
171:        &quot;subc.u32           r1, r1, 0;\n\t&quot;
172:        &quot;sub.cc.u32         r0, r0, m3;\n\t&quot;
173:        &quot;subc.u32           r1, r1, 0;\n\t&quot;
174:        &quot;mov.b64            %0, {r0, r1};\n\t&quot;
175:        // fix result
176:        &quot;setp.eq.u32        p|q, m2, 0;\n\t&quot;
177:        &quot;mov.b64            t, {m0, m1};\n\t&quot;
178:        // ret -= (uint32_t)(-(ret &gt; mul[0] &amp;&amp; m[2] == 0));
179:        &quot;set.gt.and.u32.u64 m3, %0, t, p;\n\t&quot;
180:        &quot;sub.cc.u32         r0, r0, m3;\n\t&quot;
181:        &quot;subc.u32           r1, r1, 0;\n\t&quot;
182:        &quot;mov.b64            %0, {r0, r1};\n\t&quot;
183:        // ret += (uint32_t)(-(ret &lt; mul[0] &amp;&amp; m[2] != 0));
184:        &quot;set.lt.and.u32.u64 m3, %0, t, q;\n\t&quot;
185:        &quot;add.cc.u32         r0, r0, m3;\n\t&quot;
186:        &quot;addc.u32           r1, r1, 0;\n\t&quot;
187:        &quot;mov.b64            %0, {r0, r1};\n\t&quot;
188:        &quot;}&quot;
189:        : &quot;+l&quot;(res)
190:        : &quot;l&quot;(a.val), &quot;l&quot;(b.val));
191:
192:    return _module2_(res);
193:
194:
195:}
196:
197:
198:
199:    
200:    KERNEL void test(
201:        GLOBAL_MEM _module0_ *dest
202:        , GLOBAL_MEM _module0_ *a1
203:        , GLOBAL_MEM _module0_ *a2
204:        )
205:    {
206:        const SIZE_T i = get_global_id(0);
207:        _module0_ a1_load = a1[i];
208:        _module0_ a2_load = a2[i];
209:
210:        // To stop the compiler from optimizing away the code,
211:        // we need to use the return value somehow.
212:        // We&#x27;re using it as the first argument to the next invocation.
213:        // Assuming here that the first argument of the tested function is either
214:        // a finite field element, or something convertable to it (e.g. uint64).
215:        _module0_ res =
216:            a1_load;
217:
218:        for (int i = 0; i &lt; 1; i++)
219:        {
220:            res = _module1_(
221:                res
222:                , a2_load
223:                );
224:        }
225:
226:        dest[i] = res;
227:    }
228:<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_mul[c_from_asm-cuda:0:0]</td>
          <td class="col-duration">0.03</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7fae5eb240&gt;, method = &#x27;c_from_asm&#x27;<br/><br/>    def test_mul(thread, method):<br/>        if method == &quot;cuda_asm&quot; and thread.api.get_id() != cluda.cuda_id():<br/>            pytest.skip()<br/>        check_func(<br/>            thread, ntt.mul(method=method), ref_mul, &#x27;ff_number&#x27;, [&#x27;ff_number&#x27;, &#x27;ff_number&#x27;],<br/>            test_values=[<br/>&gt;               (ntt_cpu.GaloisNumber.modulus - 1, 2**33) # regression test for an error in method=c<br/>                ]<br/>            )<br/><br/>test/test_transform/test_arithmetic.py:172: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_transform/test_arithmetic.py:101: in check_func<br/>    test = get_func_kernel(thread, func_module, output_type, input_types)<br/>test/test_transform/test_arithmetic.py:90: in get_func_kernel<br/>    repetitions=repetitions))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:537: in compile<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:655: in __init__<br/>    self.source, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:504: in _create_program<br/>    src, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:227: in _compile<br/>    return SourceModule(src, no_extern_c=True, options=options, keep=keep)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;pycuda.compiler.SourceModule object at 0x7fae5eb6d8&gt;<br/>source = &#x27;\n\n\n    #define CUDA\n    // taken from pycuda._cluda\n    #define LOCAL_BARRIER __syncthreads()\n\n    #define WIT...\n                res\n                , a2_load\n                );\n        }\n\n        dest[i] = res;\n    }\n    &#x27;<br/>nvcc = &#x27;nvcc&#x27;, options = [], keep = False, no_extern_c = True, arch = None, code = None, cache_dir = None, include_dirs = []<br/><br/>    def __init__(self, source, nvcc=&quot;nvcc&quot;, options=None, keep=False,<br/>            no_extern_c=False, arch=None, code=None, cache_dir=None,<br/>            include_dirs=[]):<br/>        self._check_arch(arch)<br/>    <br/>        cubin = compile(source, nvcc, options, keep, no_extern_c,<br/>                arch, code, cache_dir, include_dirs)<br/>    <br/>        from pycuda.driver import module_from_buffer<br/>&gt;       self.module = module_from_buffer(cubin)<br/><span class="error">E       pycuda._driver.LogicError: cuModuleLoadDataEx failed: invalid device context -</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/compiler.py:294: LogicError<br/> -------------------------------Captured log call-------------------------------- <br/>[31m[1mERROR   [0m root:api.py:507 Failed to compile:
1:
2:
3:
4:    #define CUDA
5:    // taken from pycuda._cluda
6:    #define LOCAL_BARRIER __syncthreads()
7:
8:    #define WITHIN_KERNEL __device__
9:    #define KERNEL extern &quot;C&quot; __global__
10:    #define GLOBAL_MEM /* empty */
11:    #define GLOBAL_MEM_ARG /* empty */
12:    #define LOCAL_MEM __shared__
13:    #define LOCAL_MEM_DYNAMIC extern __shared__
14:    #define LOCAL_MEM_ARG /* empty */
15:    #define CONSTANT_MEM __constant__
16:    #define CONSTANT_MEM_ARG /* empty */
17:    #define INLINE __forceinline__
18:    #define SIZE_T int
19:    #define VSIZE_T int
20:
21:    // used to align fields in structures
22:    #define ALIGN(bytes) __align__(bytes)
23:
24:    
25:
26:    WITHIN_KERNEL SIZE_T get_local_id(unsigned int dim)
27:    {
28:        if(dim == 0) return threadIdx.x;
29:        if(dim == 1) return threadIdx.y;
30:        if(dim == 2) return threadIdx.z;
31:        return 0;
32:    }
33:
34:    WITHIN_KERNEL SIZE_T get_group_id(unsigned int dim)
35:    {
36:        if(dim == 0) return blockIdx.x;
37:        if(dim == 1) return blockIdx.y;
38:        if(dim == 2) return blockIdx.z;
39:        return 0;
40:    }
41:
42:    WITHIN_KERNEL SIZE_T get_local_size(unsigned int dim)
43:    {
44:        if(dim == 0) return blockDim.x;
45:        if(dim == 1) return blockDim.y;
46:        if(dim == 2) return blockDim.z;
47:        return 1;
48:    }
49:
50:    WITHIN_KERNEL SIZE_T get_num_groups(unsigned int dim)
51:    {
52:        if(dim == 0) return gridDim.x;
53:        if(dim == 1) return gridDim.y;
54:        if(dim == 2) return gridDim.z;
55:        return 1;
56:    }
57:
58:    WITHIN_KERNEL SIZE_T get_global_size(unsigned int dim)
59:    {
60:        return get_num_groups(dim) * get_local_size(dim);
61:    }
62:
63:    WITHIN_KERNEL SIZE_T get_global_id(unsigned int dim)
64:    {
65:        return get_local_id(dim) + get_group_id(dim) * get_local_size(dim);
66:    }
67:
68:
69:
70:
71:    #define COMPLEX_CTR(T) make_##T
72:
73:    WITHIN_KERNEL float2 operator+(float2 a, float2 b)
74:    {
75:        return COMPLEX_CTR(float2)(a.x + b.x, a.y + b.y);
76:    }
77:    WITHIN_KERNEL float2 operator-(float2 a, float2 b)
78:    {
79:        return COMPLEX_CTR(float2)(a.x - b.x, a.y - b.y);
80:    }
81:    WITHIN_KERNEL float2 operator+(float2 a) { return a; }
82:    WITHIN_KERNEL float2 operator-(float2 a) { return COMPLEX_CTR(float2)(-a.x, -a.y); }
83:    WITHIN_KERNEL double2 operator+(double2 a, double2 b)
84:    {
85:        return COMPLEX_CTR(double2)(a.x + b.x, a.y + b.y);
86:    }
87:    WITHIN_KERNEL double2 operator-(double2 a, double2 b)
88:    {
89:        return COMPLEX_CTR(double2)(a.x - b.x, a.y - b.y);
90:    }
91:    WITHIN_KERNEL double2 operator+(double2 a) { return a; }
92:    WITHIN_KERNEL double2 operator-(double2 a) { return COMPLEX_CTR(double2)(-a.x, -a.y); }
93:
94:
95:typedef struct __module0_
96:{
97:    unsigned long val;
98:} _module0_;
99:
100:
101:WITHIN_KERNEL INLINE _module0_ _module0_pack(unsigned long x)
102:{
103:    _module0_ res = {x};
104:    return res;
105:}
106:
107:WITHIN_KERNEL INLINE unsigned long _module0_unpack(_module0_ x)
108:{
109:    return x.val;
110:}
111:
112:#define _module0_zero (_module0_pack(0));
113:
114:
115:#define _module0_UNPACK(hi, lo, src) {lo = (src); hi = (src) &gt;&gt; 32;}
116:
117:#ifdef CUDA
118:#define _module0_PACK(hi, lo) (((unsigned long)(hi) &lt;&lt; 32) | (lo))
119:#else
120:#define _module0_PACK(hi, lo) upsample(hi, lo)
121:#endif
122:
123:
124:#define _module0_SUB_CC(hi, lo, x1, x2) { bool cc = (x1) &lt; (x2); lo = (x1) - (x2); if (cc) { hi -= 1; } }
125:#define _module0_ADD_CC(hi, lo, x1, x2, hi_prev) { lo = (x1) + (x2); bool cc = lo &lt; (x2); hi = hi_prev; if (cc) { hi += 1; } }
126:
127:
128:
129:
130:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
131:WITHIN_KERNEL INLINE _module0_ _module2_(unsigned long a)
132:{
133:
134:    // uses the fact that 2 * P &gt; max(UInt64)
135:    // and that a::UInt64 - P == a + 2^32 - 1
136:
137:    _module0_ res = {a};
138:    res.val += (res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
139:    return res;
140:
141:}
142:
143:
144:
145:WITHIN_KERNEL INLINE _module0_ _module1_(_module0_ a, _module0_ b)
146:{
147:    /*
148:    Algorithm:
149:    Let M = 2^32
150:    Then `(a * b) = m0 + m1 * M + m2 * M^2 + m3 * M^3`
151:    and `(a * b) mod P = (m0 - m2 - m3 + (m1 + m2) * M) mod P`.
152:    Now `m0 - m2 - m3` can range from `-2M` to `M`, so if it&#x27;s negative,
153:    we need to carry 1 or 2 into the sum `m1 + m2`, and process the overflow correctly.
154:    */
155:
156:
157:        // 128-bit = 64-bit * 64-bit
158:        unsigned long res, t;
159:        unsigned int r0, r1, m0, m1, m2, m3;
160:
161:        #ifdef CUDA
162:        unsigned long m3_m2 = __umul64hi(a.val, b.val);
163:        #else
164:        unsigned long m3_m2 = mul_hi(a.val, b.val);
165:        #endif
166:        unsigned long m1_m0 = a.val * b.val;
167:
168:        _module0_UNPACK(m3, m2, m3_m2);
169:        _module0_UNPACK(m1, m0, m1_m0);
170:
171:        // 128-bit mod P with add / sub
172:        r1 = m1 + m2;
173:        _module0_SUB_CC(r1, r0, m0, m2);
174:        _module0_SUB_CC(r1, r0, r0, m3);
175:        res = _module0_PACK(r1, r0);
176:
177:        // fix result
178:        bool p = m2 == 0;
179:        bool q = !p;
180:
181:        t = _module0_PACK(m1, m0);
182:
183:        // ret -= (uint32_t)(-(ret &gt; mul[0] &amp;&amp; m[2] == 0));
184:        m3 = (res &gt; t &amp;&amp; p) ? 0xffffffff : 0;
185:        _module0_SUB_CC(r1, r0, r0, m3);
186:        res = _module0_PACK(r1, r0);
187:
188:        // ret += (uint32_t)(-(ret &lt; mul[0] &amp;&amp; m[2] != 0));
189:        m3 = (res &lt; t &amp;&amp; q) ? 0xffffffff : 0;
190:        _module0_ADD_CC(r1, r0, r0, m3, r1);
191:        res = _module0_PACK(r1, r0);
192:
193:        return _module2_(res);
194:
195:}
196:
197:
198:
199:    
200:    KERNEL void test(
201:        GLOBAL_MEM _module0_ *dest
202:        , GLOBAL_MEM _module0_ *a1
203:        , GLOBAL_MEM _module0_ *a2
204:        )
205:    {
206:        const SIZE_T i = get_global_id(0);
207:        _module0_ a1_load = a1[i];
208:        _module0_ a2_load = a2[i];
209:
210:        // To stop the compiler from optimizing away the code,
211:        // we need to use the return value somehow.
212:        // We&#x27;re using it as the first argument to the next invocation.
213:        // Assuming here that the first argument of the tested function is either
214:        // a finite field element, or something convertable to it (e.g. uint64).
215:        _module0_ res =
216:            a1_load;
217:
218:        for (int i = 0; i &lt; 1; i++)
219:        {
220:            res = _module1_(
221:                res
222:                , a2_load
223:                );
224:        }
225:
226:        dest[i] = res;
227:    }
228:<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_prepare_for_mul[cuda:0:0]</td>
          <td class="col-duration">0.03</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7fadb38748&gt;<br/><br/>    def test_prepare_for_mul(thread):<br/>        check_func(<br/>            thread, ntt.prepare_for_mul(), ref_prepare_for_mul,<br/>&gt;           &#x27;ff_number&#x27;, [&#x27;ff_number&#x27;])<br/><br/>test/test_transform/test_arithmetic.py:201: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_transform/test_arithmetic.py:101: in check_func<br/>    test = get_func_kernel(thread, func_module, output_type, input_types)<br/>test/test_transform/test_arithmetic.py:90: in get_func_kernel<br/>    repetitions=repetitions))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:537: in compile<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:655: in __init__<br/>    self.source, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:504: in _create_program<br/>    src, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:227: in _compile<br/>    return SourceModule(src, no_extern_c=True, options=options, keep=keep)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;pycuda.compiler.SourceModule object at 0x7fae6492b0&gt;<br/>source = &#x27;\n\n\n    #define CUDA\n    // taken from pycuda._cluda\n    #define LOCAL_BARRIER __syncthreads()\n\n    #define WIT...           res = _module1_(\n                res\n                );\n        }\n\n        dest[i] = res;\n    }\n    &#x27;<br/>nvcc = &#x27;nvcc&#x27;, options = [], keep = False, no_extern_c = True, arch = None, code = None, cache_dir = None, include_dirs = []<br/><br/>    def __init__(self, source, nvcc=&quot;nvcc&quot;, options=None, keep=False,<br/>            no_extern_c=False, arch=None, code=None, cache_dir=None,<br/>            include_dirs=[]):<br/>        self._check_arch(arch)<br/>    <br/>        cubin = compile(source, nvcc, options, keep, no_extern_c,<br/>                arch, code, cache_dir, include_dirs)<br/>    <br/>        from pycuda.driver import module_from_buffer<br/>&gt;       self.module = module_from_buffer(cubin)<br/><span class="error">E       pycuda._driver.LogicError: cuModuleLoadDataEx failed: invalid device context -</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/compiler.py:294: LogicError<br/> -------------------------------Captured log call-------------------------------- <br/>[31m[1mERROR   [0m root:api.py:507 Failed to compile:
1:
2:
3:
4:    #define CUDA
5:    // taken from pycuda._cluda
6:    #define LOCAL_BARRIER __syncthreads()
7:
8:    #define WITHIN_KERNEL __device__
9:    #define KERNEL extern &quot;C&quot; __global__
10:    #define GLOBAL_MEM /* empty */
11:    #define GLOBAL_MEM_ARG /* empty */
12:    #define LOCAL_MEM __shared__
13:    #define LOCAL_MEM_DYNAMIC extern __shared__
14:    #define LOCAL_MEM_ARG /* empty */
15:    #define CONSTANT_MEM __constant__
16:    #define CONSTANT_MEM_ARG /* empty */
17:    #define INLINE __forceinline__
18:    #define SIZE_T int
19:    #define VSIZE_T int
20:
21:    // used to align fields in structures
22:    #define ALIGN(bytes) __align__(bytes)
23:
24:    
25:
26:    WITHIN_KERNEL SIZE_T get_local_id(unsigned int dim)
27:    {
28:        if(dim == 0) return threadIdx.x;
29:        if(dim == 1) return threadIdx.y;
30:        if(dim == 2) return threadIdx.z;
31:        return 0;
32:    }
33:
34:    WITHIN_KERNEL SIZE_T get_group_id(unsigned int dim)
35:    {
36:        if(dim == 0) return blockIdx.x;
37:        if(dim == 1) return blockIdx.y;
38:        if(dim == 2) return blockIdx.z;
39:        return 0;
40:    }
41:
42:    WITHIN_KERNEL SIZE_T get_local_size(unsigned int dim)
43:    {
44:        if(dim == 0) return blockDim.x;
45:        if(dim == 1) return blockDim.y;
46:        if(dim == 2) return blockDim.z;
47:        return 1;
48:    }
49:
50:    WITHIN_KERNEL SIZE_T get_num_groups(unsigned int dim)
51:    {
52:        if(dim == 0) return gridDim.x;
53:        if(dim == 1) return gridDim.y;
54:        if(dim == 2) return gridDim.z;
55:        return 1;
56:    }
57:
58:    WITHIN_KERNEL SIZE_T get_global_size(unsigned int dim)
59:    {
60:        return get_num_groups(dim) * get_local_size(dim);
61:    }
62:
63:    WITHIN_KERNEL SIZE_T get_global_id(unsigned int dim)
64:    {
65:        return get_local_id(dim) + get_group_id(dim) * get_local_size(dim);
66:    }
67:
68:
69:
70:
71:    #define COMPLEX_CTR(T) make_##T
72:
73:    WITHIN_KERNEL float2 operator+(float2 a, float2 b)
74:    {
75:        return COMPLEX_CTR(float2)(a.x + b.x, a.y + b.y);
76:    }
77:    WITHIN_KERNEL float2 operator-(float2 a, float2 b)
78:    {
79:        return COMPLEX_CTR(float2)(a.x - b.x, a.y - b.y);
80:    }
81:    WITHIN_KERNEL float2 operator+(float2 a) { return a; }
82:    WITHIN_KERNEL float2 operator-(float2 a) { return COMPLEX_CTR(float2)(-a.x, -a.y); }
83:    WITHIN_KERNEL double2 operator+(double2 a, double2 b)
84:    {
85:        return COMPLEX_CTR(double2)(a.x + b.x, a.y + b.y);
86:    }
87:    WITHIN_KERNEL double2 operator-(double2 a, double2 b)
88:    {
89:        return COMPLEX_CTR(double2)(a.x - b.x, a.y - b.y);
90:    }
91:    WITHIN_KERNEL double2 operator+(double2 a) { return a; }
92:    WITHIN_KERNEL double2 operator-(double2 a) { return COMPLEX_CTR(double2)(-a.x, -a.y); }
93:
94:
95:typedef struct __module0_
96:{
97:    unsigned long val;
98:} _module0_;
99:
100:
101:WITHIN_KERNEL INLINE _module0_ _module0_pack(unsigned long x)
102:{
103:    _module0_ res = {x};
104:    return res;
105:}
106:
107:WITHIN_KERNEL INLINE unsigned long _module0_unpack(_module0_ x)
108:{
109:    return x.val;
110:}
111:
112:#define _module0_zero (_module0_pack(0));
113:
114:
115:#define _module0_UNPACK(hi, lo, src) {lo = (src); hi = (src) &gt;&gt; 32;}
116:
117:#ifdef CUDA
118:#define _module0_PACK(hi, lo) (((unsigned long)(hi) &lt;&lt; 32) | (lo))
119:#else
120:#define _module0_PACK(hi, lo) upsample(hi, lo)
121:#endif
122:
123:
124:#define _module0_SUB_CC(hi, lo, x1, x2) { bool cc = (x1) &lt; (x2); lo = (x1) - (x2); if (cc) { hi -= 1; } }
125:#define _module0_ADD_CC(hi, lo, x1, x2, hi_prev) { lo = (x1) + (x2); bool cc = lo &lt; (x2); hi = hi_prev; if (cc) { hi += 1; } }
126:
127:
128:
129:
130:/** Subtraction in FF(P): val_ = a + b mod P. */
131:WITHIN_KERNEL INLINE _module0_ _module2_(_module0_ a, _module0_ b)
132:{
133:
134:    /*
135:    Algorithm:
136:    We calculate `s = x - y`
137:    Now there are three variants:
138:    - no underflow (x &gt;= y): all good, `result = s`.
139:    - underflow (detected if `s &gt; x`), so essentially `s = x - y + N`.
140:      This means we need to calculate `s - N + P = s - (2^32 - 1)`
141:    */
142:
143:    _module0_ res = {a.val - b.val};
144:    unsigned int x = -(res.val &gt; a.val);
145:    res.val -= x;
146:    return res;
147:
148:}
149:
150:
151:
152:WITHIN_KERNEL INLINE _module0_ _module1_(_module0_ a)
153:{
154:    /*
155:    Convert the given number to Montgomery representation with the fixed modulus (M=2**64-2**32+1)
156:    and fixed word size (R=2**64), for later use with `mul_prepared()`.
157:    The result is `a * R mod M`.
158:    */
159:
160:    unsigned long lo = a.val &amp; 0xffffffff;
161:    unsigned long hi = a.val &gt;&gt; 32;
162:    unsigned long y = (lo &lt;&lt; 32) - lo;
163:    _module0_ y_ff = { y };
164:    _module0_ hi_ff = { hi };
165:    return _module2_(y_ff, hi_ff);
166:}
167:
168:
169:
170:    
171:    KERNEL void test(
172:        GLOBAL_MEM _module0_ *dest
173:        , GLOBAL_MEM _module0_ *a1
174:        )
175:    {
176:        const SIZE_T i = get_global_id(0);
177:        _module0_ a1_load = a1[i];
178:
179:        // To stop the compiler from optimizing away the code,
180:        // we need to use the return value somehow.
181:        // We&#x27;re using it as the first argument to the next invocation.
182:        // Assuming here that the first argument of the tested function is either
183:        // a finite field element, or something convertable to it (e.g. uint64).
184:        _module0_ res =
185:            a1_load;
186:
187:        for (int i = 0; i &lt; 1; i++)
188:        {
189:            res = _module1_(
190:                res
191:                );
192:        }
193:
194:        dest[i] = res;
195:    }
196:<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_inv_pow2[cuda:0:0]</td>
          <td class="col-duration">0.03</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7fadb5bf98&gt;<br/><br/>    def test_inv_pow2(thread):<br/>        exp_dtype = numpy.uint32<br/>        check_func(<br/>            thread, ntt.inv_pow2(exp_dtype), ref_inv_pow2, &#x27;ff_number&#x27;, [exp_dtype],<br/>&gt;           ranges=[(1, 33)], test_values=[(1,), (32,)])<br/><br/>test/test_transform/test_arithmetic.py:229: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_transform/test_arithmetic.py:101: in check_func<br/>    test = get_func_kernel(thread, func_module, output_type, input_types)<br/>test/test_transform/test_arithmetic.py:90: in get_func_kernel<br/>    repetitions=repetitions))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:537: in compile<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:655: in __init__<br/>    self.source, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:504: in _create_program<br/>    src, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:227: in _compile<br/>    return SourceModule(src, no_extern_c=True, options=options, keep=keep)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;pycuda.compiler.SourceModule object at 0x7fae5aed68&gt;<br/>source = &#x27;\n\n\n    #define CUDA\n    // taken from pycuda._cluda\n    #define LOCAL_BARRIER __syncthreads()\n\n    #define WIT...       res = _module1_(\n                res.val\n                );\n        }\n\n        dest[i] = res;\n    }\n    &#x27;<br/>nvcc = &#x27;nvcc&#x27;, options = [], keep = False, no_extern_c = True, arch = None, code = None, cache_dir = None, include_dirs = []<br/><br/>    def __init__(self, source, nvcc=&quot;nvcc&quot;, options=None, keep=False,<br/>            no_extern_c=False, arch=None, code=None, cache_dir=None,<br/>            include_dirs=[]):<br/>        self._check_arch(arch)<br/>    <br/>        cubin = compile(source, nvcc, options, keep, no_extern_c,<br/>                arch, code, cache_dir, include_dirs)<br/>    <br/>        from pycuda.driver import module_from_buffer<br/>&gt;       self.module = module_from_buffer(cubin)<br/><span class="error">E       pycuda._driver.LogicError: cuModuleLoadDataEx failed: invalid device context -</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/compiler.py:294: LogicError<br/> -------------------------------Captured log call-------------------------------- <br/>[31m[1mERROR   [0m root:api.py:507 Failed to compile:
1:
2:
3:
4:    #define CUDA
5:    // taken from pycuda._cluda
6:    #define LOCAL_BARRIER __syncthreads()
7:
8:    #define WITHIN_KERNEL __device__
9:    #define KERNEL extern &quot;C&quot; __global__
10:    #define GLOBAL_MEM /* empty */
11:    #define GLOBAL_MEM_ARG /* empty */
12:    #define LOCAL_MEM __shared__
13:    #define LOCAL_MEM_DYNAMIC extern __shared__
14:    #define LOCAL_MEM_ARG /* empty */
15:    #define CONSTANT_MEM __constant__
16:    #define CONSTANT_MEM_ARG /* empty */
17:    #define INLINE __forceinline__
18:    #define SIZE_T int
19:    #define VSIZE_T int
20:
21:    // used to align fields in structures
22:    #define ALIGN(bytes) __align__(bytes)
23:
24:    
25:
26:    WITHIN_KERNEL SIZE_T get_local_id(unsigned int dim)
27:    {
28:        if(dim == 0) return threadIdx.x;
29:        if(dim == 1) return threadIdx.y;
30:        if(dim == 2) return threadIdx.z;
31:        return 0;
32:    }
33:
34:    WITHIN_KERNEL SIZE_T get_group_id(unsigned int dim)
35:    {
36:        if(dim == 0) return blockIdx.x;
37:        if(dim == 1) return blockIdx.y;
38:        if(dim == 2) return blockIdx.z;
39:        return 0;
40:    }
41:
42:    WITHIN_KERNEL SIZE_T get_local_size(unsigned int dim)
43:    {
44:        if(dim == 0) return blockDim.x;
45:        if(dim == 1) return blockDim.y;
46:        if(dim == 2) return blockDim.z;
47:        return 1;
48:    }
49:
50:    WITHIN_KERNEL SIZE_T get_num_groups(unsigned int dim)
51:    {
52:        if(dim == 0) return gridDim.x;
53:        if(dim == 1) return gridDim.y;
54:        if(dim == 2) return gridDim.z;
55:        return 1;
56:    }
57:
58:    WITHIN_KERNEL SIZE_T get_global_size(unsigned int dim)
59:    {
60:        return get_num_groups(dim) * get_local_size(dim);
61:    }
62:
63:    WITHIN_KERNEL SIZE_T get_global_id(unsigned int dim)
64:    {
65:        return get_local_id(dim) + get_group_id(dim) * get_local_size(dim);
66:    }
67:
68:
69:
70:
71:    #define COMPLEX_CTR(T) make_##T
72:
73:    WITHIN_KERNEL float2 operator+(float2 a, float2 b)
74:    {
75:        return COMPLEX_CTR(float2)(a.x + b.x, a.y + b.y);
76:    }
77:    WITHIN_KERNEL float2 operator-(float2 a, float2 b)
78:    {
79:        return COMPLEX_CTR(float2)(a.x - b.x, a.y - b.y);
80:    }
81:    WITHIN_KERNEL float2 operator+(float2 a) { return a; }
82:    WITHIN_KERNEL float2 operator-(float2 a) { return COMPLEX_CTR(float2)(-a.x, -a.y); }
83:    WITHIN_KERNEL double2 operator+(double2 a, double2 b)
84:    {
85:        return COMPLEX_CTR(double2)(a.x + b.x, a.y + b.y);
86:    }
87:    WITHIN_KERNEL double2 operator-(double2 a, double2 b)
88:    {
89:        return COMPLEX_CTR(double2)(a.x - b.x, a.y - b.y);
90:    }
91:    WITHIN_KERNEL double2 operator+(double2 a) { return a; }
92:    WITHIN_KERNEL double2 operator-(double2 a) { return COMPLEX_CTR(double2)(-a.x, -a.y); }
93:
94:
95:typedef struct __module0_
96:{
97:    unsigned long val;
98:} _module0_;
99:
100:
101:WITHIN_KERNEL INLINE _module0_ _module0_pack(unsigned long x)
102:{
103:    _module0_ res = {x};
104:    return res;
105:}
106:
107:WITHIN_KERNEL INLINE unsigned long _module0_unpack(_module0_ x)
108:{
109:    return x.val;
110:}
111:
112:#define _module0_zero (_module0_pack(0));
113:
114:
115:#define _module0_UNPACK(hi, lo, src) {lo = (src); hi = (src) &gt;&gt; 32;}
116:
117:#ifdef CUDA
118:#define _module0_PACK(hi, lo) (((unsigned long)(hi) &lt;&lt; 32) | (lo))
119:#else
120:#define _module0_PACK(hi, lo) upsample(hi, lo)
121:#endif
122:
123:
124:#define _module0_SUB_CC(hi, lo, x1, x2) { bool cc = (x1) &lt; (x2); lo = (x1) - (x2); if (cc) { hi -= 1; } }
125:#define _module0_ADD_CC(hi, lo, x1, x2, hi_prev) { lo = (x1) + (x2); bool cc = lo &lt; (x2); hi = hi_prev; if (cc) { hi += 1; } }
126:
127:
128:
129:
130:
131:/**
132:* Return the inverse of 2^log_n in FF(P): 2^{-log_n} mod P.
133:* @param log_n An integer in (0, 32]
134:*/
135:WITHIN_KERNEL INLINE _module0_ _module1_(unsigned int log_n)
136:{
137:    unsigned int r0 = (1 &lt;&lt; (32 - (unsigned int)log_n)) + 1;
138:    unsigned int r1 = -r0;
139:    unsigned long r = ((unsigned long)r1 &lt;&lt; 32) | r0;
140:    _module0_ res = {r};
141:    return res;
142:}
143:
144:
145:
146:    
147:    KERNEL void test(
148:        GLOBAL_MEM _module0_ *dest
149:        , GLOBAL_MEM unsigned int *a1
150:        )
151:    {
152:        const SIZE_T i = get_global_id(0);
153:        unsigned int a1_load = a1[i];
154:
155:        // To stop the compiler from optimizing away the code,
156:        // we need to use the return value somehow.
157:        // We&#x27;re using it as the first argument to the next invocation.
158:        // Assuming here that the first argument of the tested function is either
159:        // a finite field element, or something convertable to it (e.g. uint64).
160:        _module0_ res =
161:            { a1_load };
162:
163:        for (int i = 0; i &lt; 1; i++)
164:        {
165:            res = _module1_(
166:                res.val
167:                );
168:        }
169:
170:        dest[i] = res;
171:    }
172:<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_lsh[cuda_asm-cuda:0:0-192]</td>
          <td class="col-duration">0.03</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7fadb5b550&gt;, exp_range = 192, method = &#x27;cuda_asm&#x27;<br/><br/>    @pytest.mark.parametrize(&quot;exp_range&quot;, [32, 64, 96, 128, 160, 192])<br/>    def test_lsh(thread, exp_range, method):<br/>        if method == &quot;cuda_asm&quot; and thread.api.get_id() != cluda.cuda_id():<br/>            pytest.skip()<br/>        exp_dtype = numpy.uint32<br/>        check_func(<br/>            thread, ntt.lsh(exp_range, exp_dtype, method=method),<br/>            ref_lsh, &#x27;ff_number&#x27;, [&#x27;ff_number&#x27;, exp_dtype],<br/>            ranges=[None, (exp_range - 32, exp_range)],<br/>            test_values=[<br/>&gt;               (11509900421665959066, exp_range - 1)<br/>            ])<br/><br/>test/test_transform/test_arithmetic.py:248: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_transform/test_arithmetic.py:101: in check_func<br/>    test = get_func_kernel(thread, func_module, output_type, input_types)<br/>test/test_transform/test_arithmetic.py:90: in get_func_kernel<br/>    repetitions=repetitions))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:537: in compile<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:655: in __init__<br/>    self.source, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:504: in _create_program<br/>    src, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:227: in _compile<br/>    return SourceModule(src, no_extern_c=True, options=options, keep=keep)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;pycuda.compiler.SourceModule object at 0x7fae510710&gt;<br/>source = &#x27;\n\n\n    #define CUDA\n    // taken from pycuda._cluda\n    #define LOCAL_BARRIER __syncthreads()\n\n    #define WIT...\n                res\n                , a2_load\n                );\n        }\n\n        dest[i] = res;\n    }\n    &#x27;<br/>nvcc = &#x27;nvcc&#x27;, options = [], keep = False, no_extern_c = True, arch = None, code = None, cache_dir = None, include_dirs = []<br/><br/>    def __init__(self, source, nvcc=&quot;nvcc&quot;, options=None, keep=False,<br/>            no_extern_c=False, arch=None, code=None, cache_dir=None,<br/>            include_dirs=[]):<br/>        self._check_arch(arch)<br/>    <br/>        cubin = compile(source, nvcc, options, keep, no_extern_c,<br/>                arch, code, cache_dir, include_dirs)<br/>    <br/>        from pycuda.driver import module_from_buffer<br/>&gt;       self.module = module_from_buffer(cubin)<br/><span class="error">E       pycuda._driver.LogicError: cuModuleLoadDataEx failed: invalid device context -</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/compiler.py:294: LogicError<br/> -------------------------------Captured log call-------------------------------- <br/>[31m[1mERROR   [0m root:api.py:507 Failed to compile:
1:
2:
3:
4:    #define CUDA
5:    // taken from pycuda._cluda
6:    #define LOCAL_BARRIER __syncthreads()
7:
8:    #define WITHIN_KERNEL __device__
9:    #define KERNEL extern &quot;C&quot; __global__
10:    #define GLOBAL_MEM /* empty */
11:    #define GLOBAL_MEM_ARG /* empty */
12:    #define LOCAL_MEM __shared__
13:    #define LOCAL_MEM_DYNAMIC extern __shared__
14:    #define LOCAL_MEM_ARG /* empty */
15:    #define CONSTANT_MEM __constant__
16:    #define CONSTANT_MEM_ARG /* empty */
17:    #define INLINE __forceinline__
18:    #define SIZE_T int
19:    #define VSIZE_T int
20:
21:    // used to align fields in structures
22:    #define ALIGN(bytes) __align__(bytes)
23:
24:    
25:
26:    WITHIN_KERNEL SIZE_T get_local_id(unsigned int dim)
27:    {
28:        if(dim == 0) return threadIdx.x;
29:        if(dim == 1) return threadIdx.y;
30:        if(dim == 2) return threadIdx.z;
31:        return 0;
32:    }
33:
34:    WITHIN_KERNEL SIZE_T get_group_id(unsigned int dim)
35:    {
36:        if(dim == 0) return blockIdx.x;
37:        if(dim == 1) return blockIdx.y;
38:        if(dim == 2) return blockIdx.z;
39:        return 0;
40:    }
41:
42:    WITHIN_KERNEL SIZE_T get_local_size(unsigned int dim)
43:    {
44:        if(dim == 0) return blockDim.x;
45:        if(dim == 1) return blockDim.y;
46:        if(dim == 2) return blockDim.z;
47:        return 1;
48:    }
49:
50:    WITHIN_KERNEL SIZE_T get_num_groups(unsigned int dim)
51:    {
52:        if(dim == 0) return gridDim.x;
53:        if(dim == 1) return gridDim.y;
54:        if(dim == 2) return gridDim.z;
55:        return 1;
56:    }
57:
58:    WITHIN_KERNEL SIZE_T get_global_size(unsigned int dim)
59:    {
60:        return get_num_groups(dim) * get_local_size(dim);
61:    }
62:
63:    WITHIN_KERNEL SIZE_T get_global_id(unsigned int dim)
64:    {
65:        return get_local_id(dim) + get_group_id(dim) * get_local_size(dim);
66:    }
67:
68:
69:
70:
71:    #define COMPLEX_CTR(T) make_##T
72:
73:    WITHIN_KERNEL float2 operator+(float2 a, float2 b)
74:    {
75:        return COMPLEX_CTR(float2)(a.x + b.x, a.y + b.y);
76:    }
77:    WITHIN_KERNEL float2 operator-(float2 a, float2 b)
78:    {
79:        return COMPLEX_CTR(float2)(a.x - b.x, a.y - b.y);
80:    }
81:    WITHIN_KERNEL float2 operator+(float2 a) { return a; }
82:    WITHIN_KERNEL float2 operator-(float2 a) { return COMPLEX_CTR(float2)(-a.x, -a.y); }
83:    WITHIN_KERNEL double2 operator+(double2 a, double2 b)
84:    {
85:        return COMPLEX_CTR(double2)(a.x + b.x, a.y + b.y);
86:    }
87:    WITHIN_KERNEL double2 operator-(double2 a, double2 b)
88:    {
89:        return COMPLEX_CTR(double2)(a.x - b.x, a.y - b.y);
90:    }
91:    WITHIN_KERNEL double2 operator+(double2 a) { return a; }
92:    WITHIN_KERNEL double2 operator-(double2 a) { return COMPLEX_CTR(double2)(-a.x, -a.y); }
93:
94:
95:typedef struct __module0_
96:{
97:    unsigned long val;
98:} _module0_;
99:
100:
101:WITHIN_KERNEL INLINE _module0_ _module0_pack(unsigned long x)
102:{
103:    _module0_ res = {x};
104:    return res;
105:}
106:
107:WITHIN_KERNEL INLINE unsigned long _module0_unpack(_module0_ x)
108:{
109:    return x.val;
110:}
111:
112:#define _module0_zero (_module0_pack(0));
113:
114:
115:#define _module0_UNPACK(hi, lo, src) {lo = (src); hi = (src) &gt;&gt; 32;}
116:
117:#ifdef CUDA
118:#define _module0_PACK(hi, lo) (((unsigned long)(hi) &lt;&lt; 32) | (lo))
119:#else
120:#define _module0_PACK(hi, lo) upsample(hi, lo)
121:#endif
122:
123:
124:#define _module0_SUB_CC(hi, lo, x1, x2) { bool cc = (x1) &lt; (x2); lo = (x1) - (x2); if (cc) { hi -= 1; } }
125:#define _module0_ADD_CC(hi, lo, x1, x2, hi_prev) { lo = (x1) + (x2); bool cc = lo &lt; (x2); hi = hi_prev; if (cc) { hi += 1; } }
126:
127:
128:
129:
130:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
131:WITHIN_KERNEL INLINE _module0_ _module2_(unsigned long a)
132:{
133:
134:    // uses the fact that 2 * P &gt; max(UInt64)
135:    // and that a::UInt64 - P == a + 2^32 - 1
136:
137:    _module0_ res = {a};
138:    res.val += (res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
139:    return res;
140:
141:}
142:
143:
144:
145:
146:/**
147:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
148:* @param[in] l An integer in [0, 32)
149:*/
150:WITHIN_KERNEL INLINE _module0_ _module1_(_module0_ x, unsigned int l)
151:{
152:    /*
153:    Algorithm:
154:
155:    We can decompose the shift as
156:
157:        res = x * 2^l = x * M^k * 2^j,
158:
159:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
160:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
161:
162:    After the multiplication by 2^j, the result contains 3 32-bit parts:
163:
164:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
165:
166:    Thus
167:
168:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
169:
170:    Taking the modulus P = M^2 - M + 1, we get
171:
172:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
173:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
174:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
175:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
176:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
177:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
178:
179:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
180:
181:    The processing for the things inside the parentheses is simpler:
182:
183:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
184:    - (x + y) = PACK(s &lt; y, s), where s = x + y
185:      (that is, check for overflow and add 1 in the high half)
186:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
187:      (that is, check for overflow and add (M-1) in the low half)
188:    */
189:
190:
191:
192:
193:        asm(&quot;{\n\t&quot;
194:            &quot;.reg .u32      r0, r1;\n\t&quot;
195:            &quot;.reg .u32      t0, t1, t2;\n\t&quot;
196:            &quot;.reg .u32      n;\n\t&quot;
197:            &quot;.reg .u64      s;\n\t&quot;
198:            // t[2] = (uint32_t)(x &lt;&lt; (l-160));
199:            // t[1] = (uint32_t)(x &gt;&gt; (224-l));
200:            // t[0] = (uint32_t)(x &gt;&gt; (192-l));
201:            &quot;mov.b64        {r0, r1}, %0;\n\t&quot;
202:            &quot;sub.u32        n, %1, 160;\n\t&quot;
203:            &quot;shl.b32        t2, r0, n;\n\t&quot;
204:            &quot;sub.u32        n, 32, n;\n\t&quot;
205:            &quot;shr.b64        s, %0, n;\n\t&quot;
206:            &quot;mov.b64        {t0, t1}, s;\n\t&quot;
207:            // mod P
208:            &quot;add.cc.u32     r0, t0, t2;\n\t&quot;
209:            &quot;addc.u32       r1, t1, 0;\n\t&quot;
210:            &quot;sub.u32        r1, r1, t2;\n\t&quot;
211:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
212:            // ret += (uint32_t)(-(ret &gt; ((uint64_t *)t)[0]));
213:            &quot;mov.b64        s, {t0, t1};\n\t&quot;
214:            &quot;set.gt.u32.u64 t2, %0, s;\n\t&quot;
215:            &quot;sub.cc.u32     r0, r0, t2;\n\t&quot;
216:            &quot;subc.u32       r1, r1, 0;\n\t&quot;
217:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
218:            &quot;}&quot;
219:            : &quot;+l&quot;(x.val)
220:            : &quot;r&quot;(l));
221:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
222:        return _module2_(x.val);
223:
224:
225:
226:}
227:
228:
229:
230:    
231:    KERNEL void test(
232:        GLOBAL_MEM _module0_ *dest
233:        , GLOBAL_MEM _module0_ *a1
234:        , GLOBAL_MEM unsigned int *a2
235:        )
236:    {
237:        const SIZE_T i = get_global_id(0);
238:        _module0_ a1_load = a1[i];
239:        unsigned int a2_load = a2[i];
240:
241:        // To stop the compiler from optimizing away the code,
242:        // we need to use the return value somehow.
243:        // We&#x27;re using it as the first argument to the next invocation.
244:        // Assuming here that the first argument of the tested function is either
245:        // a finite field element, or something convertable to it (e.g. uint64).
246:        _module0_ res =
247:            a1_load;
248:
249:        for (int i = 0; i &lt; 1; i++)
250:        {
251:            res = _module1_(
252:                res
253:                , a2_load
254:                );
255:        }
256:
257:        dest[i] = res;
258:    }
259:<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_lsh[c_from_asm-cuda:0:0-32]</td>
          <td class="col-duration">0.03</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7fadb5b550&gt;, exp_range = 32, method = &#x27;c_from_asm&#x27;<br/><br/>    @pytest.mark.parametrize(&quot;exp_range&quot;, [32, 64, 96, 128, 160, 192])<br/>    def test_lsh(thread, exp_range, method):<br/>        if method == &quot;cuda_asm&quot; and thread.api.get_id() != cluda.cuda_id():<br/>            pytest.skip()<br/>        exp_dtype = numpy.uint32<br/>        check_func(<br/>            thread, ntt.lsh(exp_range, exp_dtype, method=method),<br/>            ref_lsh, &#x27;ff_number&#x27;, [&#x27;ff_number&#x27;, exp_dtype],<br/>            ranges=[None, (exp_range - 32, exp_range)],<br/>            test_values=[<br/>&gt;               (11509900421665959066, exp_range - 1)<br/>            ])<br/><br/>test/test_transform/test_arithmetic.py:248: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_transform/test_arithmetic.py:101: in check_func<br/>    test = get_func_kernel(thread, func_module, output_type, input_types)<br/>test/test_transform/test_arithmetic.py:90: in get_func_kernel<br/>    repetitions=repetitions))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:537: in compile<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:655: in __init__<br/>    self.source, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:504: in _create_program<br/>    src, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:227: in _compile<br/>    return SourceModule(src, no_extern_c=True, options=options, keep=keep)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;pycuda.compiler.SourceModule object at 0x7fae5ae8d0&gt;<br/>source = &#x27;\n\n\n    #define CUDA\n    // taken from pycuda._cluda\n    #define LOCAL_BARRIER __syncthreads()\n\n    #define WIT...\n                res\n                , a2_load\n                );\n        }\n\n        dest[i] = res;\n    }\n    &#x27;<br/>nvcc = &#x27;nvcc&#x27;, options = [], keep = False, no_extern_c = True, arch = None, code = None, cache_dir = None, include_dirs = []<br/><br/>    def __init__(self, source, nvcc=&quot;nvcc&quot;, options=None, keep=False,<br/>            no_extern_c=False, arch=None, code=None, cache_dir=None,<br/>            include_dirs=[]):<br/>        self._check_arch(arch)<br/>    <br/>        cubin = compile(source, nvcc, options, keep, no_extern_c,<br/>                arch, code, cache_dir, include_dirs)<br/>    <br/>        from pycuda.driver import module_from_buffer<br/>&gt;       self.module = module_from_buffer(cubin)<br/><span class="error">E       pycuda._driver.LogicError: cuModuleLoadDataEx failed: invalid device context -</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/compiler.py:294: LogicError<br/> -------------------------------Captured log call-------------------------------- <br/>[31m[1mERROR   [0m root:api.py:507 Failed to compile:
1:
2:
3:
4:    #define CUDA
5:    // taken from pycuda._cluda
6:    #define LOCAL_BARRIER __syncthreads()
7:
8:    #define WITHIN_KERNEL __device__
9:    #define KERNEL extern &quot;C&quot; __global__
10:    #define GLOBAL_MEM /* empty */
11:    #define GLOBAL_MEM_ARG /* empty */
12:    #define LOCAL_MEM __shared__
13:    #define LOCAL_MEM_DYNAMIC extern __shared__
14:    #define LOCAL_MEM_ARG /* empty */
15:    #define CONSTANT_MEM __constant__
16:    #define CONSTANT_MEM_ARG /* empty */
17:    #define INLINE __forceinline__
18:    #define SIZE_T int
19:    #define VSIZE_T int
20:
21:    // used to align fields in structures
22:    #define ALIGN(bytes) __align__(bytes)
23:
24:    
25:
26:    WITHIN_KERNEL SIZE_T get_local_id(unsigned int dim)
27:    {
28:        if(dim == 0) return threadIdx.x;
29:        if(dim == 1) return threadIdx.y;
30:        if(dim == 2) return threadIdx.z;
31:        return 0;
32:    }
33:
34:    WITHIN_KERNEL SIZE_T get_group_id(unsigned int dim)
35:    {
36:        if(dim == 0) return blockIdx.x;
37:        if(dim == 1) return blockIdx.y;
38:        if(dim == 2) return blockIdx.z;
39:        return 0;
40:    }
41:
42:    WITHIN_KERNEL SIZE_T get_local_size(unsigned int dim)
43:    {
44:        if(dim == 0) return blockDim.x;
45:        if(dim == 1) return blockDim.y;
46:        if(dim == 2) return blockDim.z;
47:        return 1;
48:    }
49:
50:    WITHIN_KERNEL SIZE_T get_num_groups(unsigned int dim)
51:    {
52:        if(dim == 0) return gridDim.x;
53:        if(dim == 1) return gridDim.y;
54:        if(dim == 2) return gridDim.z;
55:        return 1;
56:    }
57:
58:    WITHIN_KERNEL SIZE_T get_global_size(unsigned int dim)
59:    {
60:        return get_num_groups(dim) * get_local_size(dim);
61:    }
62:
63:    WITHIN_KERNEL SIZE_T get_global_id(unsigned int dim)
64:    {
65:        return get_local_id(dim) + get_group_id(dim) * get_local_size(dim);
66:    }
67:
68:
69:
70:
71:    #define COMPLEX_CTR(T) make_##T
72:
73:    WITHIN_KERNEL float2 operator+(float2 a, float2 b)
74:    {
75:        return COMPLEX_CTR(float2)(a.x + b.x, a.y + b.y);
76:    }
77:    WITHIN_KERNEL float2 operator-(float2 a, float2 b)
78:    {
79:        return COMPLEX_CTR(float2)(a.x - b.x, a.y - b.y);
80:    }
81:    WITHIN_KERNEL float2 operator+(float2 a) { return a; }
82:    WITHIN_KERNEL float2 operator-(float2 a) { return COMPLEX_CTR(float2)(-a.x, -a.y); }
83:    WITHIN_KERNEL double2 operator+(double2 a, double2 b)
84:    {
85:        return COMPLEX_CTR(double2)(a.x + b.x, a.y + b.y);
86:    }
87:    WITHIN_KERNEL double2 operator-(double2 a, double2 b)
88:    {
89:        return COMPLEX_CTR(double2)(a.x - b.x, a.y - b.y);
90:    }
91:    WITHIN_KERNEL double2 operator+(double2 a) { return a; }
92:    WITHIN_KERNEL double2 operator-(double2 a) { return COMPLEX_CTR(double2)(-a.x, -a.y); }
93:
94:
95:typedef struct __module0_
96:{
97:    unsigned long val;
98:} _module0_;
99:
100:
101:WITHIN_KERNEL INLINE _module0_ _module0_pack(unsigned long x)
102:{
103:    _module0_ res = {x};
104:    return res;
105:}
106:
107:WITHIN_KERNEL INLINE unsigned long _module0_unpack(_module0_ x)
108:{
109:    return x.val;
110:}
111:
112:#define _module0_zero (_module0_pack(0));
113:
114:
115:#define _module0_UNPACK(hi, lo, src) {lo = (src); hi = (src) &gt;&gt; 32;}
116:
117:#ifdef CUDA
118:#define _module0_PACK(hi, lo) (((unsigned long)(hi) &lt;&lt; 32) | (lo))
119:#else
120:#define _module0_PACK(hi, lo) upsample(hi, lo)
121:#endif
122:
123:
124:#define _module0_SUB_CC(hi, lo, x1, x2) { bool cc = (x1) &lt; (x2); lo = (x1) - (x2); if (cc) { hi -= 1; } }
125:#define _module0_ADD_CC(hi, lo, x1, x2, hi_prev) { lo = (x1) + (x2); bool cc = lo &lt; (x2); hi = hi_prev; if (cc) { hi += 1; } }
126:
127:
128:
129:
130:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
131:WITHIN_KERNEL INLINE _module0_ _module2_(unsigned long a)
132:{
133:
134:    // uses the fact that 2 * P &gt; max(UInt64)
135:    // and that a::UInt64 - P == a + 2^32 - 1
136:
137:    _module0_ res = {a};
138:    res.val += (res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
139:    return res;
140:
141:}
142:
143:
144:
145:
146:/**
147:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
148:* @param[in] l An integer in [0, 32)
149:*/
150:WITHIN_KERNEL INLINE _module0_ _module1_(_module0_ x, unsigned int l)
151:{
152:    /*
153:    Algorithm:
154:
155:    We can decompose the shift as
156:
157:        res = x * 2^l = x * M^k * 2^j,
158:
159:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
160:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
161:
162:    After the multiplication by 2^j, the result contains 3 32-bit parts:
163:
164:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
165:
166:    Thus
167:
168:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
169:
170:    Taking the modulus P = M^2 - M + 1, we get
171:
172:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
173:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
174:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
175:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
176:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
177:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
178:
179:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
180:
181:    The processing for the things inside the parentheses is simpler:
182:
183:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
184:    - (x + y) = PACK(s &lt; y, s), where s = x + y
185:      (that is, check for overflow and add 1 in the high half)
186:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
187:      (that is, check for overflow and add (M-1) in the low half)
188:    */
189:
190:
191:
192:
193:        unsigned long res = x.val;
194:
195:        unsigned int r0, r1, t0, t1, t2, n;
196:        unsigned long s;
197:
198:        // t[2] = (uint32_t)(x &gt;&gt; (64-l));
199:        // t[1] = (uint32_t)(x &gt;&gt; (32-l));
200:        // t[0] = (uint32_t)(x &lt;&lt; l);
201:
202:        _module0_UNPACK(r1, r0, res);
203:
204:        t0 = r0 &lt;&lt; l;
205:        n = 32 - l;
206:        s = res &gt;&gt; n;
207:
208:        _module0_UNPACK(t2, t1, s);
209:
210:        // mod P
211:        r1 = t1 + t2;
212:        _module0_SUB_CC(r1, r0, t0, t2);
213:        res = _module0_PACK(r1, r0);
214:
215:        // ret += (uint32_t)(-(ret &lt; ((uint64_t *)t)[0]));
216:        s = _module0_PACK(t1, t0);
217:        t2 = -(res &lt; s);
218:        _module0_ADD_CC(r1, r0, r0, t2, r1);
219:        res = _module0_PACK(r1, r0);
220:
221:        return _module2_(res);
222:
223:
224:
225:}
226:
227:
228:
229:    
230:    KERNEL void test(
231:        GLOBAL_MEM _module0_ *dest
232:        , GLOBAL_MEM _module0_ *a1
233:        , GLOBAL_MEM unsigned int *a2
234:        )
235:    {
236:        const SIZE_T i = get_global_id(0);
237:        _module0_ a1_load = a1[i];
238:        unsigned int a2_load = a2[i];
239:
240:        // To stop the compiler from optimizing away the code,
241:        // we need to use the return value somehow.
242:        // We&#x27;re using it as the first argument to the next invocation.
243:        // Assuming here that the first argument of the tested function is either
244:        // a finite field element, or something convertable to it (e.g. uint64).
245:        _module0_ res =
246:            a1_load;
247:
248:        for (int i = 0; i &lt; 1; i++)
249:        {
250:            res = _module1_(
251:                res
252:                , a2_load
253:                );
254:        }
255:
256:        dest[i] = res;
257:    }
258:<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_lsh[c_from_asm-cuda:0:0-64]</td>
          <td class="col-duration">0.03</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7fadb5b550&gt;, exp_range = 64, method = &#x27;c_from_asm&#x27;<br/><br/>    @pytest.mark.parametrize(&quot;exp_range&quot;, [32, 64, 96, 128, 160, 192])<br/>    def test_lsh(thread, exp_range, method):<br/>        if method == &quot;cuda_asm&quot; and thread.api.get_id() != cluda.cuda_id():<br/>            pytest.skip()<br/>        exp_dtype = numpy.uint32<br/>        check_func(<br/>            thread, ntt.lsh(exp_range, exp_dtype, method=method),<br/>            ref_lsh, &#x27;ff_number&#x27;, [&#x27;ff_number&#x27;, exp_dtype],<br/>            ranges=[None, (exp_range - 32, exp_range)],<br/>            test_values=[<br/>&gt;               (11509900421665959066, exp_range - 1)<br/>            ])<br/><br/>test/test_transform/test_arithmetic.py:248: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_transform/test_arithmetic.py:101: in check_func<br/>    test = get_func_kernel(thread, func_module, output_type, input_types)<br/>test/test_transform/test_arithmetic.py:90: in get_func_kernel<br/>    repetitions=repetitions))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:537: in compile<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:655: in __init__<br/>    self.source, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:504: in _create_program<br/>    src, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:227: in _compile<br/>    return SourceModule(src, no_extern_c=True, options=options, keep=keep)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;pycuda.compiler.SourceModule object at 0x7fae5e2128&gt;<br/>source = &#x27;\n\n\n    #define CUDA\n    // taken from pycuda._cluda\n    #define LOCAL_BARRIER __syncthreads()\n\n    #define WIT...\n                res\n                , a2_load\n                );\n        }\n\n        dest[i] = res;\n    }\n    &#x27;<br/>nvcc = &#x27;nvcc&#x27;, options = [], keep = False, no_extern_c = True, arch = None, code = None, cache_dir = None, include_dirs = []<br/><br/>    def __init__(self, source, nvcc=&quot;nvcc&quot;, options=None, keep=False,<br/>            no_extern_c=False, arch=None, code=None, cache_dir=None,<br/>            include_dirs=[]):<br/>        self._check_arch(arch)<br/>    <br/>        cubin = compile(source, nvcc, options, keep, no_extern_c,<br/>                arch, code, cache_dir, include_dirs)<br/>    <br/>        from pycuda.driver import module_from_buffer<br/>&gt;       self.module = module_from_buffer(cubin)<br/><span class="error">E       pycuda._driver.LogicError: cuModuleLoadDataEx failed: invalid device context -</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/compiler.py:294: LogicError<br/> -------------------------------Captured log call-------------------------------- <br/>[31m[1mERROR   [0m root:api.py:507 Failed to compile:
1:
2:
3:
4:    #define CUDA
5:    // taken from pycuda._cluda
6:    #define LOCAL_BARRIER __syncthreads()
7:
8:    #define WITHIN_KERNEL __device__
9:    #define KERNEL extern &quot;C&quot; __global__
10:    #define GLOBAL_MEM /* empty */
11:    #define GLOBAL_MEM_ARG /* empty */
12:    #define LOCAL_MEM __shared__
13:    #define LOCAL_MEM_DYNAMIC extern __shared__
14:    #define LOCAL_MEM_ARG /* empty */
15:    #define CONSTANT_MEM __constant__
16:    #define CONSTANT_MEM_ARG /* empty */
17:    #define INLINE __forceinline__
18:    #define SIZE_T int
19:    #define VSIZE_T int
20:
21:    // used to align fields in structures
22:    #define ALIGN(bytes) __align__(bytes)
23:
24:    
25:
26:    WITHIN_KERNEL SIZE_T get_local_id(unsigned int dim)
27:    {
28:        if(dim == 0) return threadIdx.x;
29:        if(dim == 1) return threadIdx.y;
30:        if(dim == 2) return threadIdx.z;
31:        return 0;
32:    }
33:
34:    WITHIN_KERNEL SIZE_T get_group_id(unsigned int dim)
35:    {
36:        if(dim == 0) return blockIdx.x;
37:        if(dim == 1) return blockIdx.y;
38:        if(dim == 2) return blockIdx.z;
39:        return 0;
40:    }
41:
42:    WITHIN_KERNEL SIZE_T get_local_size(unsigned int dim)
43:    {
44:        if(dim == 0) return blockDim.x;
45:        if(dim == 1) return blockDim.y;
46:        if(dim == 2) return blockDim.z;
47:        return 1;
48:    }
49:
50:    WITHIN_KERNEL SIZE_T get_num_groups(unsigned int dim)
51:    {
52:        if(dim == 0) return gridDim.x;
53:        if(dim == 1) return gridDim.y;
54:        if(dim == 2) return gridDim.z;
55:        return 1;
56:    }
57:
58:    WITHIN_KERNEL SIZE_T get_global_size(unsigned int dim)
59:    {
60:        return get_num_groups(dim) * get_local_size(dim);
61:    }
62:
63:    WITHIN_KERNEL SIZE_T get_global_id(unsigned int dim)
64:    {
65:        return get_local_id(dim) + get_group_id(dim) * get_local_size(dim);
66:    }
67:
68:
69:
70:
71:    #define COMPLEX_CTR(T) make_##T
72:
73:    WITHIN_KERNEL float2 operator+(float2 a, float2 b)
74:    {
75:        return COMPLEX_CTR(float2)(a.x + b.x, a.y + b.y);
76:    }
77:    WITHIN_KERNEL float2 operator-(float2 a, float2 b)
78:    {
79:        return COMPLEX_CTR(float2)(a.x - b.x, a.y - b.y);
80:    }
81:    WITHIN_KERNEL float2 operator+(float2 a) { return a; }
82:    WITHIN_KERNEL float2 operator-(float2 a) { return COMPLEX_CTR(float2)(-a.x, -a.y); }
83:    WITHIN_KERNEL double2 operator+(double2 a, double2 b)
84:    {
85:        return COMPLEX_CTR(double2)(a.x + b.x, a.y + b.y);
86:    }
87:    WITHIN_KERNEL double2 operator-(double2 a, double2 b)
88:    {
89:        return COMPLEX_CTR(double2)(a.x - b.x, a.y - b.y);
90:    }
91:    WITHIN_KERNEL double2 operator+(double2 a) { return a; }
92:    WITHIN_KERNEL double2 operator-(double2 a) { return COMPLEX_CTR(double2)(-a.x, -a.y); }
93:
94:
95:typedef struct __module0_
96:{
97:    unsigned long val;
98:} _module0_;
99:
100:
101:WITHIN_KERNEL INLINE _module0_ _module0_pack(unsigned long x)
102:{
103:    _module0_ res = {x};
104:    return res;
105:}
106:
107:WITHIN_KERNEL INLINE unsigned long _module0_unpack(_module0_ x)
108:{
109:    return x.val;
110:}
111:
112:#define _module0_zero (_module0_pack(0));
113:
114:
115:#define _module0_UNPACK(hi, lo, src) {lo = (src); hi = (src) &gt;&gt; 32;}
116:
117:#ifdef CUDA
118:#define _module0_PACK(hi, lo) (((unsigned long)(hi) &lt;&lt; 32) | (lo))
119:#else
120:#define _module0_PACK(hi, lo) upsample(hi, lo)
121:#endif
122:
123:
124:#define _module0_SUB_CC(hi, lo, x1, x2) { bool cc = (x1) &lt; (x2); lo = (x1) - (x2); if (cc) { hi -= 1; } }
125:#define _module0_ADD_CC(hi, lo, x1, x2, hi_prev) { lo = (x1) + (x2); bool cc = lo &lt; (x2); hi = hi_prev; if (cc) { hi += 1; } }
126:
127:
128:
129:
130:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
131:WITHIN_KERNEL INLINE _module0_ _module2_(unsigned long a)
132:{
133:
134:    // uses the fact that 2 * P &gt; max(UInt64)
135:    // and that a::UInt64 - P == a + 2^32 - 1
136:
137:    _module0_ res = {a};
138:    res.val += (res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
139:    return res;
140:
141:}
142:
143:
144:
145:
146:/**
147:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
148:* @param[in] l An integer in [0, 32)
149:*/
150:WITHIN_KERNEL INLINE _module0_ _module1_(_module0_ x, unsigned int l)
151:{
152:    /*
153:    Algorithm:
154:
155:    We can decompose the shift as
156:
157:        res = x * 2^l = x * M^k * 2^j,
158:
159:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
160:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
161:
162:    After the multiplication by 2^j, the result contains 3 32-bit parts:
163:
164:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
165:
166:    Thus
167:
168:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
169:
170:    Taking the modulus P = M^2 - M + 1, we get
171:
172:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
173:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
174:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
175:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
176:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
177:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
178:
179:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
180:
181:    The processing for the things inside the parentheses is simpler:
182:
183:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
184:    - (x + y) = PACK(s &lt; y, s), where s = x + y
185:      (that is, check for overflow and add 1 in the high half)
186:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
187:      (that is, check for overflow and add (M-1) in the low half)
188:    */
189:
190:
191:
192:
193:        unsigned long res = x.val;
194:
195:        unsigned int r0, r1, t0, t1, t2, n;
196:        unsigned long s;
197:        bool p, q;
198:
199:        // t[2] = (uint32_t)(x &gt;&gt; (96-l));
200:        // t[1] = (uint32_t)(x &gt;&gt; (64-l));
201:        // t[0] = (uint32_t)(x &lt;&lt; (l-32));
202:
203:        _module0_UNPACK(r1, r0, res);
204:
205:        n = l - 32;
206:        t0 = r0 &lt;&lt; n;
207:        n = 32 - n;
208:        s = res &gt;&gt; n;
209:
210:        _module0_UNPACK(t2, t1, s);
211:
212:        // mod P
213:        r1 = t0 + t1;
214:        _module0_SUB_CC(r1, r0, 0, t1);
215:        _module0_SUB_CC(r1, r0, r0, t2);
216:        res = _module0_PACK(r1, r0);
217:
218:        // ret -= (uint32_t)(-(ret &gt; ((uint64_t)t[0] &lt;&lt; 32) &amp;&amp; t[1] == 0));
219:        p = t1 == 0;
220:        q = !p;
221:        s = _module0_PACK(t0, 0);
222:        t2 = (res &gt; s &amp;&amp; p) ? 0xffffffff : 0;
223:        _module0_SUB_CC(r1, r0, r0, t2);
224:        res = _module0_PACK(r1, r0);
225:
226:        // ret += (uint32_t)(-(ret &lt; ((uint64_t)t[0] &lt;&lt; 32) &amp;&amp; t[1] != 0));
227:        t2 = (res &lt; s &amp;&amp; q) ? 0xffffffff : 0;
228:        _module0_ADD_CC(r1, r0, r0, t2, r1);
229:        res = _module0_PACK(r1, r0);
230:
231:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
232:        return _module2_(res);
233:
234:
235:
236:}
237:
238:
239:
240:    
241:    KERNEL void test(
242:        GLOBAL_MEM _module0_ *dest
243:        , GLOBAL_MEM _module0_ *a1
244:        , GLOBAL_MEM unsigned int *a2
245:        )
246:    {
247:        const SIZE_T i = get_global_id(0);
248:        _module0_ a1_load = a1[i];
249:        unsigned int a2_load = a2[i];
250:
251:        // To stop the compiler from optimizing away the code,
252:        // we need to use the return value somehow.
253:        // We&#x27;re using it as the first argument to the next invocation.
254:        // Assuming here that the first argument of the tested function is either
255:        // a finite field element, or something convertable to it (e.g. uint64).
256:        _module0_ res =
257:            a1_load;
258:
259:        for (int i = 0; i &lt; 1; i++)
260:        {
261:            res = _module1_(
262:                res
263:                , a2_load
264:                );
265:        }
266:
267:        dest[i] = res;
268:    }
269:<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_lsh[c_from_asm-cuda:0:0-96]</td>
          <td class="col-duration">0.04</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7fadb5b550&gt;, exp_range = 96, method = &#x27;c_from_asm&#x27;<br/><br/>    @pytest.mark.parametrize(&quot;exp_range&quot;, [32, 64, 96, 128, 160, 192])<br/>    def test_lsh(thread, exp_range, method):<br/>        if method == &quot;cuda_asm&quot; and thread.api.get_id() != cluda.cuda_id():<br/>            pytest.skip()<br/>        exp_dtype = numpy.uint32<br/>        check_func(<br/>            thread, ntt.lsh(exp_range, exp_dtype, method=method),<br/>            ref_lsh, &#x27;ff_number&#x27;, [&#x27;ff_number&#x27;, exp_dtype],<br/>            ranges=[None, (exp_range - 32, exp_range)],<br/>            test_values=[<br/>&gt;               (11509900421665959066, exp_range - 1)<br/>            ])<br/><br/>test/test_transform/test_arithmetic.py:248: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_transform/test_arithmetic.py:101: in check_func<br/>    test = get_func_kernel(thread, func_module, output_type, input_types)<br/>test/test_transform/test_arithmetic.py:90: in get_func_kernel<br/>    repetitions=repetitions))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:537: in compile<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:655: in __init__<br/>    self.source, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:504: in _create_program<br/>    src, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:227: in _compile<br/>    return SourceModule(src, no_extern_c=True, options=options, keep=keep)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;pycuda.compiler.SourceModule object at 0x7fae5b7320&gt;<br/>source = &#x27;\n\n\n    #define CUDA\n    // taken from pycuda._cluda\n    #define LOCAL_BARRIER __syncthreads()\n\n    #define WIT...\n                res\n                , a2_load\n                );\n        }\n\n        dest[i] = res;\n    }\n    &#x27;<br/>nvcc = &#x27;nvcc&#x27;, options = [], keep = False, no_extern_c = True, arch = None, code = None, cache_dir = None, include_dirs = []<br/><br/>    def __init__(self, source, nvcc=&quot;nvcc&quot;, options=None, keep=False,<br/>            no_extern_c=False, arch=None, code=None, cache_dir=None,<br/>            include_dirs=[]):<br/>        self._check_arch(arch)<br/>    <br/>        cubin = compile(source, nvcc, options, keep, no_extern_c,<br/>                arch, code, cache_dir, include_dirs)<br/>    <br/>        from pycuda.driver import module_from_buffer<br/>&gt;       self.module = module_from_buffer(cubin)<br/><span class="error">E       pycuda._driver.LogicError: cuModuleLoadDataEx failed: invalid device context -</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/compiler.py:294: LogicError<br/> -------------------------------Captured log call-------------------------------- <br/>[31m[1mERROR   [0m root:api.py:507 Failed to compile:
1:
2:
3:
4:    #define CUDA
5:    // taken from pycuda._cluda
6:    #define LOCAL_BARRIER __syncthreads()
7:
8:    #define WITHIN_KERNEL __device__
9:    #define KERNEL extern &quot;C&quot; __global__
10:    #define GLOBAL_MEM /* empty */
11:    #define GLOBAL_MEM_ARG /* empty */
12:    #define LOCAL_MEM __shared__
13:    #define LOCAL_MEM_DYNAMIC extern __shared__
14:    #define LOCAL_MEM_ARG /* empty */
15:    #define CONSTANT_MEM __constant__
16:    #define CONSTANT_MEM_ARG /* empty */
17:    #define INLINE __forceinline__
18:    #define SIZE_T int
19:    #define VSIZE_T int
20:
21:    // used to align fields in structures
22:    #define ALIGN(bytes) __align__(bytes)
23:
24:    
25:
26:    WITHIN_KERNEL SIZE_T get_local_id(unsigned int dim)
27:    {
28:        if(dim == 0) return threadIdx.x;
29:        if(dim == 1) return threadIdx.y;
30:        if(dim == 2) return threadIdx.z;
31:        return 0;
32:    }
33:
34:    WITHIN_KERNEL SIZE_T get_group_id(unsigned int dim)
35:    {
36:        if(dim == 0) return blockIdx.x;
37:        if(dim == 1) return blockIdx.y;
38:        if(dim == 2) return blockIdx.z;
39:        return 0;
40:    }
41:
42:    WITHIN_KERNEL SIZE_T get_local_size(unsigned int dim)
43:    {
44:        if(dim == 0) return blockDim.x;
45:        if(dim == 1) return blockDim.y;
46:        if(dim == 2) return blockDim.z;
47:        return 1;
48:    }
49:
50:    WITHIN_KERNEL SIZE_T get_num_groups(unsigned int dim)
51:    {
52:        if(dim == 0) return gridDim.x;
53:        if(dim == 1) return gridDim.y;
54:        if(dim == 2) return gridDim.z;
55:        return 1;
56:    }
57:
58:    WITHIN_KERNEL SIZE_T get_global_size(unsigned int dim)
59:    {
60:        return get_num_groups(dim) * get_local_size(dim);
61:    }
62:
63:    WITHIN_KERNEL SIZE_T get_global_id(unsigned int dim)
64:    {
65:        return get_local_id(dim) + get_group_id(dim) * get_local_size(dim);
66:    }
67:
68:
69:
70:
71:    #define COMPLEX_CTR(T) make_##T
72:
73:    WITHIN_KERNEL float2 operator+(float2 a, float2 b)
74:    {
75:        return COMPLEX_CTR(float2)(a.x + b.x, a.y + b.y);
76:    }
77:    WITHIN_KERNEL float2 operator-(float2 a, float2 b)
78:    {
79:        return COMPLEX_CTR(float2)(a.x - b.x, a.y - b.y);
80:    }
81:    WITHIN_KERNEL float2 operator+(float2 a) { return a; }
82:    WITHIN_KERNEL float2 operator-(float2 a) { return COMPLEX_CTR(float2)(-a.x, -a.y); }
83:    WITHIN_KERNEL double2 operator+(double2 a, double2 b)
84:    {
85:        return COMPLEX_CTR(double2)(a.x + b.x, a.y + b.y);
86:    }
87:    WITHIN_KERNEL double2 operator-(double2 a, double2 b)
88:    {
89:        return COMPLEX_CTR(double2)(a.x - b.x, a.y - b.y);
90:    }
91:    WITHIN_KERNEL double2 operator+(double2 a) { return a; }
92:    WITHIN_KERNEL double2 operator-(double2 a) { return COMPLEX_CTR(double2)(-a.x, -a.y); }
93:
94:
95:typedef struct __module0_
96:{
97:    unsigned long val;
98:} _module0_;
99:
100:
101:WITHIN_KERNEL INLINE _module0_ _module0_pack(unsigned long x)
102:{
103:    _module0_ res = {x};
104:    return res;
105:}
106:
107:WITHIN_KERNEL INLINE unsigned long _module0_unpack(_module0_ x)
108:{
109:    return x.val;
110:}
111:
112:#define _module0_zero (_module0_pack(0));
113:
114:
115:#define _module0_UNPACK(hi, lo, src) {lo = (src); hi = (src) &gt;&gt; 32;}
116:
117:#ifdef CUDA
118:#define _module0_PACK(hi, lo) (((unsigned long)(hi) &lt;&lt; 32) | (lo))
119:#else
120:#define _module0_PACK(hi, lo) upsample(hi, lo)
121:#endif
122:
123:
124:#define _module0_SUB_CC(hi, lo, x1, x2) { bool cc = (x1) &lt; (x2); lo = (x1) - (x2); if (cc) { hi -= 1; } }
125:#define _module0_ADD_CC(hi, lo, x1, x2, hi_prev) { lo = (x1) + (x2); bool cc = lo &lt; (x2); hi = hi_prev; if (cc) { hi += 1; } }
126:
127:
128:
129:
130:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
131:WITHIN_KERNEL INLINE _module0_ _module2_(unsigned long a)
132:{
133:
134:    // uses the fact that 2 * P &gt; max(UInt64)
135:    // and that a::UInt64 - P == a + 2^32 - 1
136:
137:    _module0_ res = {a};
138:    res.val += (res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
139:    return res;
140:
141:}
142:
143:
144:
145:
146:/**
147:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
148:* @param[in] l An integer in [0, 32)
149:*/
150:WITHIN_KERNEL INLINE _module0_ _module1_(_module0_ x, unsigned int l)
151:{
152:    /*
153:    Algorithm:
154:
155:    We can decompose the shift as
156:
157:        res = x * 2^l = x * M^k * 2^j,
158:
159:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
160:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
161:
162:    After the multiplication by 2^j, the result contains 3 32-bit parts:
163:
164:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
165:
166:    Thus
167:
168:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
169:
170:    Taking the modulus P = M^2 - M + 1, we get
171:
172:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
173:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
174:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
175:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
176:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
177:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
178:
179:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
180:
181:    The processing for the things inside the parentheses is simpler:
182:
183:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
184:    - (x + y) = PACK(s &lt; y, s), where s = x + y
185:      (that is, check for overflow and add 1 in the high half)
186:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
187:      (that is, check for overflow and add (M-1) in the low half)
188:    */
189:
190:
191:
192:
193:        unsigned long res = x.val;
194:
195:        unsigned int r0, r1, t0, t1, t2, n;
196:        unsigned long s;
197:
198:        // t[2] = (uint32_t)(x &gt;&gt; (128-l));
199:        // t[1] = (uint32_t)(x &gt;&gt; (96-l));
200:        // t[0] = (uint32_t)(x &lt;&lt; (l-64));
201:
202:        _module0_UNPACK(r1, r0, res);
203:        n = l - 64;
204:        t0 = r0 &lt;&lt; n;
205:        n = 32 - n;
206:        s = res &gt;&gt; n;
207:        _module0_UNPACK(t2, t1, s);
208:
209:        // mod P
210:        _module0_ADD_CC(r1, r0, t1, t0, t2);
211:
212:        r1 -= t0;
213:        res = _module0_PACK(r1, r0);
214:
215:        // ret -= (uint32_t)(-(ret &gt; ((uint64_t *)t)[1]));
216:        s = _module0_PACK(t2, t1);
217:        t2 = (res &gt; s) ? 0xffffffff : 0;
218:        _module0_SUB_CC(r1, r0, r0, t2);
219:        res = _module0_PACK(r1, r0);
220:
221:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
222:        _module0_ ret = _module2_(res);
223:        ret.val = 18446744069414584321UL - ret.val;
224:        return ret;
225:
226:
227:
228:}
229:
230:
231:
232:    
233:    KERNEL void test(
234:        GLOBAL_MEM _module0_ *dest
235:        , GLOBAL_MEM _module0_ *a1
236:        , GLOBAL_MEM unsigned int *a2
237:        )
238:    {
239:        const SIZE_T i = get_global_id(0);
240:        _module0_ a1_load = a1[i];
241:        unsigned int a2_load = a2[i];
242:
243:        // To stop the compiler from optimizing away the code,
244:        // we need to use the return value somehow.
245:        // We&#x27;re using it as the first argument to the next invocation.
246:        // Assuming here that the first argument of the tested function is either
247:        // a finite field element, or something convertable to it (e.g. uint64).
248:        _module0_ res =
249:            a1_load;
250:
251:        for (int i = 0; i &lt; 1; i++)
252:        {
253:            res = _module1_(
254:                res
255:                , a2_load
256:                );
257:        }
258:
259:        dest[i] = res;
260:    }
261:<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_lsh[c_from_asm-cuda:0:0-128]</td>
          <td class="col-duration">0.04</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7fadb5b550&gt;, exp_range = 128, method = &#x27;c_from_asm&#x27;<br/><br/>    @pytest.mark.parametrize(&quot;exp_range&quot;, [32, 64, 96, 128, 160, 192])<br/>    def test_lsh(thread, exp_range, method):<br/>        if method == &quot;cuda_asm&quot; and thread.api.get_id() != cluda.cuda_id():<br/>            pytest.skip()<br/>        exp_dtype = numpy.uint32<br/>        check_func(<br/>            thread, ntt.lsh(exp_range, exp_dtype, method=method),<br/>            ref_lsh, &#x27;ff_number&#x27;, [&#x27;ff_number&#x27;, exp_dtype],<br/>            ranges=[None, (exp_range - 32, exp_range)],<br/>            test_values=[<br/>&gt;               (11509900421665959066, exp_range - 1)<br/>            ])<br/><br/>test/test_transform/test_arithmetic.py:248: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_transform/test_arithmetic.py:101: in check_func<br/>    test = get_func_kernel(thread, func_module, output_type, input_types)<br/>test/test_transform/test_arithmetic.py:90: in get_func_kernel<br/>    repetitions=repetitions))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:537: in compile<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:655: in __init__<br/>    self.source, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:504: in _create_program<br/>    src, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:227: in _compile<br/>    return SourceModule(src, no_extern_c=True, options=options, keep=keep)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;pycuda.compiler.SourceModule object at 0x7fae5f28d0&gt;<br/>source = &#x27;\n\n\n    #define CUDA\n    // taken from pycuda._cluda\n    #define LOCAL_BARRIER __syncthreads()\n\n    #define WIT...\n                res\n                , a2_load\n                );\n        }\n\n        dest[i] = res;\n    }\n    &#x27;<br/>nvcc = &#x27;nvcc&#x27;, options = [], keep = False, no_extern_c = True, arch = None, code = None, cache_dir = None, include_dirs = []<br/><br/>    def __init__(self, source, nvcc=&quot;nvcc&quot;, options=None, keep=False,<br/>            no_extern_c=False, arch=None, code=None, cache_dir=None,<br/>            include_dirs=[]):<br/>        self._check_arch(arch)<br/>    <br/>        cubin = compile(source, nvcc, options, keep, no_extern_c,<br/>                arch, code, cache_dir, include_dirs)<br/>    <br/>        from pycuda.driver import module_from_buffer<br/>&gt;       self.module = module_from_buffer(cubin)<br/><span class="error">E       pycuda._driver.LogicError: cuModuleLoadDataEx failed: invalid device context -</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/compiler.py:294: LogicError<br/> -------------------------------Captured log call-------------------------------- <br/>[31m[1mERROR   [0m root:api.py:507 Failed to compile:
1:
2:
3:
4:    #define CUDA
5:    // taken from pycuda._cluda
6:    #define LOCAL_BARRIER __syncthreads()
7:
8:    #define WITHIN_KERNEL __device__
9:    #define KERNEL extern &quot;C&quot; __global__
10:    #define GLOBAL_MEM /* empty */
11:    #define GLOBAL_MEM_ARG /* empty */
12:    #define LOCAL_MEM __shared__
13:    #define LOCAL_MEM_DYNAMIC extern __shared__
14:    #define LOCAL_MEM_ARG /* empty */
15:    #define CONSTANT_MEM __constant__
16:    #define CONSTANT_MEM_ARG /* empty */
17:    #define INLINE __forceinline__
18:    #define SIZE_T int
19:    #define VSIZE_T int
20:
21:    // used to align fields in structures
22:    #define ALIGN(bytes) __align__(bytes)
23:
24:    
25:
26:    WITHIN_KERNEL SIZE_T get_local_id(unsigned int dim)
27:    {
28:        if(dim == 0) return threadIdx.x;
29:        if(dim == 1) return threadIdx.y;
30:        if(dim == 2) return threadIdx.z;
31:        return 0;
32:    }
33:
34:    WITHIN_KERNEL SIZE_T get_group_id(unsigned int dim)
35:    {
36:        if(dim == 0) return blockIdx.x;
37:        if(dim == 1) return blockIdx.y;
38:        if(dim == 2) return blockIdx.z;
39:        return 0;
40:    }
41:
42:    WITHIN_KERNEL SIZE_T get_local_size(unsigned int dim)
43:    {
44:        if(dim == 0) return blockDim.x;
45:        if(dim == 1) return blockDim.y;
46:        if(dim == 2) return blockDim.z;
47:        return 1;
48:    }
49:
50:    WITHIN_KERNEL SIZE_T get_num_groups(unsigned int dim)
51:    {
52:        if(dim == 0) return gridDim.x;
53:        if(dim == 1) return gridDim.y;
54:        if(dim == 2) return gridDim.z;
55:        return 1;
56:    }
57:
58:    WITHIN_KERNEL SIZE_T get_global_size(unsigned int dim)
59:    {
60:        return get_num_groups(dim) * get_local_size(dim);
61:    }
62:
63:    WITHIN_KERNEL SIZE_T get_global_id(unsigned int dim)
64:    {
65:        return get_local_id(dim) + get_group_id(dim) * get_local_size(dim);
66:    }
67:
68:
69:
70:
71:    #define COMPLEX_CTR(T) make_##T
72:
73:    WITHIN_KERNEL float2 operator+(float2 a, float2 b)
74:    {
75:        return COMPLEX_CTR(float2)(a.x + b.x, a.y + b.y);
76:    }
77:    WITHIN_KERNEL float2 operator-(float2 a, float2 b)
78:    {
79:        return COMPLEX_CTR(float2)(a.x - b.x, a.y - b.y);
80:    }
81:    WITHIN_KERNEL float2 operator+(float2 a) { return a; }
82:    WITHIN_KERNEL float2 operator-(float2 a) { return COMPLEX_CTR(float2)(-a.x, -a.y); }
83:    WITHIN_KERNEL double2 operator+(double2 a, double2 b)
84:    {
85:        return COMPLEX_CTR(double2)(a.x + b.x, a.y + b.y);
86:    }
87:    WITHIN_KERNEL double2 operator-(double2 a, double2 b)
88:    {
89:        return COMPLEX_CTR(double2)(a.x - b.x, a.y - b.y);
90:    }
91:    WITHIN_KERNEL double2 operator+(double2 a) { return a; }
92:    WITHIN_KERNEL double2 operator-(double2 a) { return COMPLEX_CTR(double2)(-a.x, -a.y); }
93:
94:
95:typedef struct __module0_
96:{
97:    unsigned long val;
98:} _module0_;
99:
100:
101:WITHIN_KERNEL INLINE _module0_ _module0_pack(unsigned long x)
102:{
103:    _module0_ res = {x};
104:    return res;
105:}
106:
107:WITHIN_KERNEL INLINE unsigned long _module0_unpack(_module0_ x)
108:{
109:    return x.val;
110:}
111:
112:#define _module0_zero (_module0_pack(0));
113:
114:
115:#define _module0_UNPACK(hi, lo, src) {lo = (src); hi = (src) &gt;&gt; 32;}
116:
117:#ifdef CUDA
118:#define _module0_PACK(hi, lo) (((unsigned long)(hi) &lt;&lt; 32) | (lo))
119:#else
120:#define _module0_PACK(hi, lo) upsample(hi, lo)
121:#endif
122:
123:
124:#define _module0_SUB_CC(hi, lo, x1, x2) { bool cc = (x1) &lt; (x2); lo = (x1) - (x2); if (cc) { hi -= 1; } }
125:#define _module0_ADD_CC(hi, lo, x1, x2, hi_prev) { lo = (x1) + (x2); bool cc = lo &lt; (x2); hi = hi_prev; if (cc) { hi += 1; } }
126:
127:
128:
129:
130:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
131:WITHIN_KERNEL INLINE _module0_ _module2_(unsigned long a)
132:{
133:
134:    // uses the fact that 2 * P &gt; max(UInt64)
135:    // and that a::UInt64 - P == a + 2^32 - 1
136:
137:    _module0_ res = {a};
138:    res.val += (res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
139:    return res;
140:
141:}
142:
143:
144:
145:
146:/**
147:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
148:* @param[in] l An integer in [0, 32)
149:*/
150:WITHIN_KERNEL INLINE _module0_ _module1_(_module0_ x, unsigned int l)
151:{
152:    /*
153:    Algorithm:
154:
155:    We can decompose the shift as
156:
157:        res = x * 2^l = x * M^k * 2^j,
158:
159:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
160:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
161:
162:    After the multiplication by 2^j, the result contains 3 32-bit parts:
163:
164:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
165:
166:    Thus
167:
168:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
169:
170:    Taking the modulus P = M^2 - M + 1, we get
171:
172:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
173:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
174:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
175:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
176:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
177:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
178:
179:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
180:
181:    The processing for the things inside the parentheses is simpler:
182:
183:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
184:    - (x + y) = PACK(s &lt; y, s), where s = x + y
185:      (that is, check for overflow and add 1 in the high half)
186:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
187:      (that is, check for overflow and add (M-1) in the low half)
188:    */
189:
190:
191:
192:
193:        unsigned long res = x.val;
194:
195:        unsigned int r0, r1, t0, t1, t2, n;
196:        unsigned long s;
197:
198:        // t[2] = (uint32_t)(x &gt;&gt; (160-l));
199:        // t[1] = (uint32_t)(x &gt;&gt; (128-l));
200:        // t[0] = (uint32_t)(x &lt;&lt; (l-96));
201:
202:        _module0_UNPACK(r1, r0, res);
203:        n = l - 96;
204:        t0 = r0 &lt;&lt; n;
205:        n = 32 - n;
206:        s = res &gt;&gt; n;
207:        _module0_UNPACK(t2, t1, s);
208:
209:        // mod P
210:        r1 = t1 + t2;
211:        _module0_SUB_CC(r1, r0, t0, t2);
212:        res = _module0_PACK(r1, r0);
213:
214:        // ret += (uint32_t)(-(ret &lt; ((uint64_t *)t)[0]));
215:        s = _module0_PACK(t1, t0);
216:        t2 = (res &lt; s) ? 0xffffffff : 0;
217:        _module0_ADD_CC(r1, r0, r0, t2, r1);
218:        res = _module0_PACK(r1, r0);
219:
220:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
221:        _module0_ ret = _module2_(res);
222:        ret.val = 18446744069414584321UL - ret.val;
223:        return ret;
224:
225:
226:
227:}
228:
229:
230:
231:    
232:    KERNEL void test(
233:        GLOBAL_MEM _module0_ *dest
234:        , GLOBAL_MEM _module0_ *a1
235:        , GLOBAL_MEM unsigned int *a2
236:        )
237:    {
238:        const SIZE_T i = get_global_id(0);
239:        _module0_ a1_load = a1[i];
240:        unsigned int a2_load = a2[i];
241:
242:        // To stop the compiler from optimizing away the code,
243:        // we need to use the return value somehow.
244:        // We&#x27;re using it as the first argument to the next invocation.
245:        // Assuming here that the first argument of the tested function is either
246:        // a finite field element, or something convertable to it (e.g. uint64).
247:        _module0_ res =
248:            a1_load;
249:
250:        for (int i = 0; i &lt; 1; i++)
251:        {
252:            res = _module1_(
253:                res
254:                , a2_load
255:                );
256:        }
257:
258:        dest[i] = res;
259:    }
260:<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_lsh[c_from_asm-cuda:0:0-160]</td>
          <td class="col-duration">0.04</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7fadb5b550&gt;, exp_range = 160, method = &#x27;c_from_asm&#x27;<br/><br/>    @pytest.mark.parametrize(&quot;exp_range&quot;, [32, 64, 96, 128, 160, 192])<br/>    def test_lsh(thread, exp_range, method):<br/>        if method == &quot;cuda_asm&quot; and thread.api.get_id() != cluda.cuda_id():<br/>            pytest.skip()<br/>        exp_dtype = numpy.uint32<br/>        check_func(<br/>            thread, ntt.lsh(exp_range, exp_dtype, method=method),<br/>            ref_lsh, &#x27;ff_number&#x27;, [&#x27;ff_number&#x27;, exp_dtype],<br/>            ranges=[None, (exp_range - 32, exp_range)],<br/>            test_values=[<br/>&gt;               (11509900421665959066, exp_range - 1)<br/>            ])<br/><br/>test/test_transform/test_arithmetic.py:248: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_transform/test_arithmetic.py:101: in check_func<br/>    test = get_func_kernel(thread, func_module, output_type, input_types)<br/>test/test_transform/test_arithmetic.py:90: in get_func_kernel<br/>    repetitions=repetitions))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:537: in compile<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:655: in __init__<br/>    self.source, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:504: in _create_program<br/>    src, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:227: in _compile<br/>    return SourceModule(src, no_extern_c=True, options=options, keep=keep)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;pycuda.compiler.SourceModule object at 0x7fae5b7438&gt;<br/>source = &#x27;\n\n\n    #define CUDA\n    // taken from pycuda._cluda\n    #define LOCAL_BARRIER __syncthreads()\n\n    #define WIT...\n                res\n                , a2_load\n                );\n        }\n\n        dest[i] = res;\n    }\n    &#x27;<br/>nvcc = &#x27;nvcc&#x27;, options = [], keep = False, no_extern_c = True, arch = None, code = None, cache_dir = None, include_dirs = []<br/><br/>    def __init__(self, source, nvcc=&quot;nvcc&quot;, options=None, keep=False,<br/>            no_extern_c=False, arch=None, code=None, cache_dir=None,<br/>            include_dirs=[]):<br/>        self._check_arch(arch)<br/>    <br/>        cubin = compile(source, nvcc, options, keep, no_extern_c,<br/>                arch, code, cache_dir, include_dirs)<br/>    <br/>        from pycuda.driver import module_from_buffer<br/>&gt;       self.module = module_from_buffer(cubin)<br/><span class="error">E       pycuda._driver.LogicError: cuModuleLoadDataEx failed: invalid device context -</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/compiler.py:294: LogicError<br/> -------------------------------Captured log call-------------------------------- <br/>[31m[1mERROR   [0m root:api.py:507 Failed to compile:
1:
2:
3:
4:    #define CUDA
5:    // taken from pycuda._cluda
6:    #define LOCAL_BARRIER __syncthreads()
7:
8:    #define WITHIN_KERNEL __device__
9:    #define KERNEL extern &quot;C&quot; __global__
10:    #define GLOBAL_MEM /* empty */
11:    #define GLOBAL_MEM_ARG /* empty */
12:    #define LOCAL_MEM __shared__
13:    #define LOCAL_MEM_DYNAMIC extern __shared__
14:    #define LOCAL_MEM_ARG /* empty */
15:    #define CONSTANT_MEM __constant__
16:    #define CONSTANT_MEM_ARG /* empty */
17:    #define INLINE __forceinline__
18:    #define SIZE_T int
19:    #define VSIZE_T int
20:
21:    // used to align fields in structures
22:    #define ALIGN(bytes) __align__(bytes)
23:
24:    
25:
26:    WITHIN_KERNEL SIZE_T get_local_id(unsigned int dim)
27:    {
28:        if(dim == 0) return threadIdx.x;
29:        if(dim == 1) return threadIdx.y;
30:        if(dim == 2) return threadIdx.z;
31:        return 0;
32:    }
33:
34:    WITHIN_KERNEL SIZE_T get_group_id(unsigned int dim)
35:    {
36:        if(dim == 0) return blockIdx.x;
37:        if(dim == 1) return blockIdx.y;
38:        if(dim == 2) return blockIdx.z;
39:        return 0;
40:    }
41:
42:    WITHIN_KERNEL SIZE_T get_local_size(unsigned int dim)
43:    {
44:        if(dim == 0) return blockDim.x;
45:        if(dim == 1) return blockDim.y;
46:        if(dim == 2) return blockDim.z;
47:        return 1;
48:    }
49:
50:    WITHIN_KERNEL SIZE_T get_num_groups(unsigned int dim)
51:    {
52:        if(dim == 0) return gridDim.x;
53:        if(dim == 1) return gridDim.y;
54:        if(dim == 2) return gridDim.z;
55:        return 1;
56:    }
57:
58:    WITHIN_KERNEL SIZE_T get_global_size(unsigned int dim)
59:    {
60:        return get_num_groups(dim) * get_local_size(dim);
61:    }
62:
63:    WITHIN_KERNEL SIZE_T get_global_id(unsigned int dim)
64:    {
65:        return get_local_id(dim) + get_group_id(dim) * get_local_size(dim);
66:    }
67:
68:
69:
70:
71:    #define COMPLEX_CTR(T) make_##T
72:
73:    WITHIN_KERNEL float2 operator+(float2 a, float2 b)
74:    {
75:        return COMPLEX_CTR(float2)(a.x + b.x, a.y + b.y);
76:    }
77:    WITHIN_KERNEL float2 operator-(float2 a, float2 b)
78:    {
79:        return COMPLEX_CTR(float2)(a.x - b.x, a.y - b.y);
80:    }
81:    WITHIN_KERNEL float2 operator+(float2 a) { return a; }
82:    WITHIN_KERNEL float2 operator-(float2 a) { return COMPLEX_CTR(float2)(-a.x, -a.y); }
83:    WITHIN_KERNEL double2 operator+(double2 a, double2 b)
84:    {
85:        return COMPLEX_CTR(double2)(a.x + b.x, a.y + b.y);
86:    }
87:    WITHIN_KERNEL double2 operator-(double2 a, double2 b)
88:    {
89:        return COMPLEX_CTR(double2)(a.x - b.x, a.y - b.y);
90:    }
91:    WITHIN_KERNEL double2 operator+(double2 a) { return a; }
92:    WITHIN_KERNEL double2 operator-(double2 a) { return COMPLEX_CTR(double2)(-a.x, -a.y); }
93:
94:
95:typedef struct __module0_
96:{
97:    unsigned long val;
98:} _module0_;
99:
100:
101:WITHIN_KERNEL INLINE _module0_ _module0_pack(unsigned long x)
102:{
103:    _module0_ res = {x};
104:    return res;
105:}
106:
107:WITHIN_KERNEL INLINE unsigned long _module0_unpack(_module0_ x)
108:{
109:    return x.val;
110:}
111:
112:#define _module0_zero (_module0_pack(0));
113:
114:
115:#define _module0_UNPACK(hi, lo, src) {lo = (src); hi = (src) &gt;&gt; 32;}
116:
117:#ifdef CUDA
118:#define _module0_PACK(hi, lo) (((unsigned long)(hi) &lt;&lt; 32) | (lo))
119:#else
120:#define _module0_PACK(hi, lo) upsample(hi, lo)
121:#endif
122:
123:
124:#define _module0_SUB_CC(hi, lo, x1, x2) { bool cc = (x1) &lt; (x2); lo = (x1) - (x2); if (cc) { hi -= 1; } }
125:#define _module0_ADD_CC(hi, lo, x1, x2, hi_prev) { lo = (x1) + (x2); bool cc = lo &lt; (x2); hi = hi_prev; if (cc) { hi += 1; } }
126:
127:
128:
129:
130:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
131:WITHIN_KERNEL INLINE _module0_ _module2_(unsigned long a)
132:{
133:
134:    // uses the fact that 2 * P &gt; max(UInt64)
135:    // and that a::UInt64 - P == a + 2^32 - 1
136:
137:    _module0_ res = {a};
138:    res.val += (res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
139:    return res;
140:
141:}
142:
143:
144:
145:
146:/**
147:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
148:* @param[in] l An integer in [0, 32)
149:*/
150:WITHIN_KERNEL INLINE _module0_ _module1_(_module0_ x, unsigned int l)
151:{
152:    /*
153:    Algorithm:
154:
155:    We can decompose the shift as
156:
157:        res = x * 2^l = x * M^k * 2^j,
158:
159:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
160:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
161:
162:    After the multiplication by 2^j, the result contains 3 32-bit parts:
163:
164:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
165:
166:    Thus
167:
168:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
169:
170:    Taking the modulus P = M^2 - M + 1, we get
171:
172:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
173:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
174:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
175:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
176:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
177:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
178:
179:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
180:
181:    The processing for the things inside the parentheses is simpler:
182:
183:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
184:    - (x + y) = PACK(s &lt; y, s), where s = x + y
185:      (that is, check for overflow and add 1 in the high half)
186:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
187:      (that is, check for overflow and add (M-1) in the low half)
188:    */
189:
190:
191:
192:
193:        unsigned long res = x.val;
194:
195:        unsigned int r0, r1, t0, t1, t2, n;
196:        unsigned long s;
197:        bool p, q;
198:
199:        // t[2] = (uint32_t)(x &gt;&gt; (192-l));
200:        // t[1] = (uint32_t)(x &gt;&gt; (160-l));
201:        // t[0] = (uint32_t)(x &lt;&lt; (l-128));
202:        _module0_UNPACK(r1, r0, res);
203:        n = l - 128;
204:        t0 = r0 &lt;&lt; n;
205:        n = 32 - n;
206:        s = res &gt;&gt; n;
207:        _module0_UNPACK(t2, t1, s);
208:
209:        // mod P
210:        r1 = t0 + t1;
211:        _module0_SUB_CC(r1, r0, 0, t1);
212:        _module0_SUB_CC(r1, r0, r0, t2);
213:        res = _module0_PACK(r1, r0);
214:
215:        // ret -= (uint32_t)(-(ret &gt; ((uint64_t)t[0] &lt;&lt; 32) &amp;&amp; t[1] == 0));
216:        p = t1 == 0;
217:        q = !p;
218:        s = _module0_PACK(t0, 0);
219:        t2 = (res &gt; s &amp;&amp; p) ? 0xffffffff : 0;
220:        _module0_SUB_CC(r1, r0, r0, t2);
221:        res = _module0_PACK(r1, r0);
222:
223:        // ret += (uint32_t)(-(ret &lt; ((uint64_t)t[0] &lt;&lt; 32) &amp;&amp; t[1] != 0));
224:        t2 = (res &lt; s &amp;&amp; q) ? 0xffffffff : 0;
225:        _module0_ADD_CC(r1, r0, r0, t2, r1);
226:        res = _module0_PACK(r1, r0);
227:
228:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
229:        _module0_ ret = _module2_(res);
230:        ret.val = 18446744069414584321UL - ret.val;
231:        return ret;
232:
233:
234:}
235:
236:
237:
238:    
239:    KERNEL void test(
240:        GLOBAL_MEM _module0_ *dest
241:        , GLOBAL_MEM _module0_ *a1
242:        , GLOBAL_MEM unsigned int *a2
243:        )
244:    {
245:        const SIZE_T i = get_global_id(0);
246:        _module0_ a1_load = a1[i];
247:        unsigned int a2_load = a2[i];
248:
249:        // To stop the compiler from optimizing away the code,
250:        // we need to use the return value somehow.
251:        // We&#x27;re using it as the first argument to the next invocation.
252:        // Assuming here that the first argument of the tested function is either
253:        // a finite field element, or something convertable to it (e.g. uint64).
254:        _module0_ res =
255:            a1_load;
256:
257:        for (int i = 0; i &lt; 1; i++)
258:        {
259:            res = _module1_(
260:                res
261:                , a2_load
262:                );
263:        }
264:
265:        dest[i] = res;
266:    }
267:<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_lsh[c_from_asm-cuda:0:0-192]</td>
          <td class="col-duration">0.04</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7fadb5b550&gt;, exp_range = 192, method = &#x27;c_from_asm&#x27;<br/><br/>    @pytest.mark.parametrize(&quot;exp_range&quot;, [32, 64, 96, 128, 160, 192])<br/>    def test_lsh(thread, exp_range, method):<br/>        if method == &quot;cuda_asm&quot; and thread.api.get_id() != cluda.cuda_id():<br/>            pytest.skip()<br/>        exp_dtype = numpy.uint32<br/>        check_func(<br/>            thread, ntt.lsh(exp_range, exp_dtype, method=method),<br/>            ref_lsh, &#x27;ff_number&#x27;, [&#x27;ff_number&#x27;, exp_dtype],<br/>            ranges=[None, (exp_range - 32, exp_range)],<br/>            test_values=[<br/>&gt;               (11509900421665959066, exp_range - 1)<br/>            ])<br/><br/>test/test_transform/test_arithmetic.py:248: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_transform/test_arithmetic.py:101: in check_func<br/>    test = get_func_kernel(thread, func_module, output_type, input_types)<br/>test/test_transform/test_arithmetic.py:90: in get_func_kernel<br/>    repetitions=repetitions))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:537: in compile<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:655: in __init__<br/>    self.source, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:504: in _create_program<br/>    src, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:227: in _compile<br/>    return SourceModule(src, no_extern_c=True, options=options, keep=keep)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;pycuda.compiler.SourceModule object at 0x7fae549630&gt;<br/>source = &#x27;\n\n\n    #define CUDA\n    // taken from pycuda._cluda\n    #define LOCAL_BARRIER __syncthreads()\n\n    #define WIT...\n                res\n                , a2_load\n                );\n        }\n\n        dest[i] = res;\n    }\n    &#x27;<br/>nvcc = &#x27;nvcc&#x27;, options = [], keep = False, no_extern_c = True, arch = None, code = None, cache_dir = None, include_dirs = []<br/><br/>    def __init__(self, source, nvcc=&quot;nvcc&quot;, options=None, keep=False,<br/>            no_extern_c=False, arch=None, code=None, cache_dir=None,<br/>            include_dirs=[]):<br/>        self._check_arch(arch)<br/>    <br/>        cubin = compile(source, nvcc, options, keep, no_extern_c,<br/>                arch, code, cache_dir, include_dirs)<br/>    <br/>        from pycuda.driver import module_from_buffer<br/>&gt;       self.module = module_from_buffer(cubin)<br/><span class="error">E       pycuda._driver.LogicError: cuModuleLoadDataEx failed: invalid device context -</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/compiler.py:294: LogicError<br/> -------------------------------Captured log call-------------------------------- <br/>[31m[1mERROR   [0m root:api.py:507 Failed to compile:
1:
2:
3:
4:    #define CUDA
5:    // taken from pycuda._cluda
6:    #define LOCAL_BARRIER __syncthreads()
7:
8:    #define WITHIN_KERNEL __device__
9:    #define KERNEL extern &quot;C&quot; __global__
10:    #define GLOBAL_MEM /* empty */
11:    #define GLOBAL_MEM_ARG /* empty */
12:    #define LOCAL_MEM __shared__
13:    #define LOCAL_MEM_DYNAMIC extern __shared__
14:    #define LOCAL_MEM_ARG /* empty */
15:    #define CONSTANT_MEM __constant__
16:    #define CONSTANT_MEM_ARG /* empty */
17:    #define INLINE __forceinline__
18:    #define SIZE_T int
19:    #define VSIZE_T int
20:
21:    // used to align fields in structures
22:    #define ALIGN(bytes) __align__(bytes)
23:
24:    
25:
26:    WITHIN_KERNEL SIZE_T get_local_id(unsigned int dim)
27:    {
28:        if(dim == 0) return threadIdx.x;
29:        if(dim == 1) return threadIdx.y;
30:        if(dim == 2) return threadIdx.z;
31:        return 0;
32:    }
33:
34:    WITHIN_KERNEL SIZE_T get_group_id(unsigned int dim)
35:    {
36:        if(dim == 0) return blockIdx.x;
37:        if(dim == 1) return blockIdx.y;
38:        if(dim == 2) return blockIdx.z;
39:        return 0;
40:    }
41:
42:    WITHIN_KERNEL SIZE_T get_local_size(unsigned int dim)
43:    {
44:        if(dim == 0) return blockDim.x;
45:        if(dim == 1) return blockDim.y;
46:        if(dim == 2) return blockDim.z;
47:        return 1;
48:    }
49:
50:    WITHIN_KERNEL SIZE_T get_num_groups(unsigned int dim)
51:    {
52:        if(dim == 0) return gridDim.x;
53:        if(dim == 1) return gridDim.y;
54:        if(dim == 2) return gridDim.z;
55:        return 1;
56:    }
57:
58:    WITHIN_KERNEL SIZE_T get_global_size(unsigned int dim)
59:    {
60:        return get_num_groups(dim) * get_local_size(dim);
61:    }
62:
63:    WITHIN_KERNEL SIZE_T get_global_id(unsigned int dim)
64:    {
65:        return get_local_id(dim) + get_group_id(dim) * get_local_size(dim);
66:    }
67:
68:
69:
70:
71:    #define COMPLEX_CTR(T) make_##T
72:
73:    WITHIN_KERNEL float2 operator+(float2 a, float2 b)
74:    {
75:        return COMPLEX_CTR(float2)(a.x + b.x, a.y + b.y);
76:    }
77:    WITHIN_KERNEL float2 operator-(float2 a, float2 b)
78:    {
79:        return COMPLEX_CTR(float2)(a.x - b.x, a.y - b.y);
80:    }
81:    WITHIN_KERNEL float2 operator+(float2 a) { return a; }
82:    WITHIN_KERNEL float2 operator-(float2 a) { return COMPLEX_CTR(float2)(-a.x, -a.y); }
83:    WITHIN_KERNEL double2 operator+(double2 a, double2 b)
84:    {
85:        return COMPLEX_CTR(double2)(a.x + b.x, a.y + b.y);
86:    }
87:    WITHIN_KERNEL double2 operator-(double2 a, double2 b)
88:    {
89:        return COMPLEX_CTR(double2)(a.x - b.x, a.y - b.y);
90:    }
91:    WITHIN_KERNEL double2 operator+(double2 a) { return a; }
92:    WITHIN_KERNEL double2 operator-(double2 a) { return COMPLEX_CTR(double2)(-a.x, -a.y); }
93:
94:
95:typedef struct __module0_
96:{
97:    unsigned long val;
98:} _module0_;
99:
100:
101:WITHIN_KERNEL INLINE _module0_ _module0_pack(unsigned long x)
102:{
103:    _module0_ res = {x};
104:    return res;
105:}
106:
107:WITHIN_KERNEL INLINE unsigned long _module0_unpack(_module0_ x)
108:{
109:    return x.val;
110:}
111:
112:#define _module0_zero (_module0_pack(0));
113:
114:
115:#define _module0_UNPACK(hi, lo, src) {lo = (src); hi = (src) &gt;&gt; 32;}
116:
117:#ifdef CUDA
118:#define _module0_PACK(hi, lo) (((unsigned long)(hi) &lt;&lt; 32) | (lo))
119:#else
120:#define _module0_PACK(hi, lo) upsample(hi, lo)
121:#endif
122:
123:
124:#define _module0_SUB_CC(hi, lo, x1, x2) { bool cc = (x1) &lt; (x2); lo = (x1) - (x2); if (cc) { hi -= 1; } }
125:#define _module0_ADD_CC(hi, lo, x1, x2, hi_prev) { lo = (x1) + (x2); bool cc = lo &lt; (x2); hi = hi_prev; if (cc) { hi += 1; } }
126:
127:
128:
129:
130:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
131:WITHIN_KERNEL INLINE _module0_ _module2_(unsigned long a)
132:{
133:
134:    // uses the fact that 2 * P &gt; max(UInt64)
135:    // and that a::UInt64 - P == a + 2^32 - 1
136:
137:    _module0_ res = {a};
138:    res.val += (res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
139:    return res;
140:
141:}
142:
143:
144:
145:
146:/**
147:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
148:* @param[in] l An integer in [0, 32)
149:*/
150:WITHIN_KERNEL INLINE _module0_ _module1_(_module0_ x, unsigned int l)
151:{
152:    /*
153:    Algorithm:
154:
155:    We can decompose the shift as
156:
157:        res = x * 2^l = x * M^k * 2^j,
158:
159:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
160:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
161:
162:    After the multiplication by 2^j, the result contains 3 32-bit parts:
163:
164:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
165:
166:    Thus
167:
168:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
169:
170:    Taking the modulus P = M^2 - M + 1, we get
171:
172:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
173:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
174:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
175:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
176:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
177:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
178:
179:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
180:
181:    The processing for the things inside the parentheses is simpler:
182:
183:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
184:    - (x + y) = PACK(s &lt; y, s), where s = x + y
185:      (that is, check for overflow and add 1 in the high half)
186:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
187:      (that is, check for overflow and add (M-1) in the low half)
188:    */
189:
190:
191:
192:
193:        unsigned long res = x.val;
194:
195:        unsigned int r0, r1, t0, t1, t2, n;
196:        unsigned long s;
197:
198:        // t[2] = (uint32_t)(x &lt;&lt; (l-160));
199:        // t[1] = (uint32_t)(x &gt;&gt; (224-l));
200:        // t[0] = (uint32_t)(x &gt;&gt; (192-l));
201:        _module0_UNPACK(r1, r0, res);
202:        n = l - 160;
203:        t2 = r0 &lt;&lt; n;
204:        n = 32 - n;
205:        s = res &gt;&gt; n;
206:        _module0_UNPACK(t1, t0, s);
207:
208:        // mod P
209:        _module0_ADD_CC(r1, r0, t0, t2, t1);
210:        r1 -= t2;
211:        res = _module0_PACK(r1, r0);
212:
213:        // ret += (uint32_t)(-(ret &gt; ((uint64_t *)t)[0]));
214:        s = _module0_PACK(t1, t0);
215:        t2 = (res &gt; s) ? 0xffffffff : 0;
216:        _module0_SUB_CC(r1, r0, r0, t2);
217:        res = _module0_PACK(r1, r0);
218:
219:        return _module2_(res);
220:
221:
222:}
223:
224:
225:
226:    
227:    KERNEL void test(
228:        GLOBAL_MEM _module0_ *dest
229:        , GLOBAL_MEM _module0_ *a1
230:        , GLOBAL_MEM unsigned int *a2
231:        )
232:    {
233:        const SIZE_T i = get_global_id(0);
234:        _module0_ a1_load = a1[i];
235:        unsigned int a2_load = a2[i];
236:
237:        // To stop the compiler from optimizing away the code,
238:        // we need to use the return value somehow.
239:        // We&#x27;re using it as the first argument to the next invocation.
240:        // Assuming here that the first argument of the tested function is either
241:        // a finite field element, or something convertable to it (e.g. uint64).
242:        _module0_ res =
243:            a1_load;
244:
245:        for (int i = 0; i &lt; 1; i++)
246:        {
247:            res = _module1_(
248:                res
249:                , a2_load
250:                );
251:        }
252:
253:        dest[i] = res;
254:    }
255:<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_add_perf[c-cuda:0:0]</td>
          <td class="col-duration">0.74</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7fae66f358&gt;, method = &#x27;c&#x27;, heavy_performance_load = False<br/><br/>    @pytest.mark.perf<br/>    def test_add_perf(thread, method, heavy_performance_load):<br/>        if method == &quot;cuda_asm&quot; and thread.api.get_id() != cluda.cuda_id():<br/>            pytest.skip()<br/>        if method == &quot;c_from_asm&quot;:<br/>            pytest.skip()<br/>    <br/>        check_func_performance(<br/>            &quot;add(), &quot; + method,<br/>            thread, ntt.add(method=method), ref_add, &#x27;ff_number&#x27;, [&#x27;ff_number&#x27;, &#x27;ff_number&#x27;],<br/>&gt;           heavy_performance_load=heavy_performance_load)<br/><br/>test/test_transform/test_arithmetic.py:309: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_transform/test_arithmetic.py:280: in check_func_performance<br/>    perf_test(dest_dev, *arrays_dev, global_size=N)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:731: in __call__<br/>    return self.prepared_call(*args)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:347: in prepared_call<br/>    stream=self._thr._queue, shared=self._local_mem)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>func = &lt;pycuda._driver.Function object at 0x7fae4ec8f0&gt;<br/>args = (&lt;pycuda._driver.DeviceAllocation object at 0x7fae5f7350&gt;, &lt;pycuda._driver.DeviceAllocation object at 0x7fae5f72b0&gt;, &lt;pycuda._driver.DeviceAllocation object at 0x7faee91bc0&gt;)<br/>kwargs = {}, grid = (32, 1, 1), stream = &lt;pycuda._driver.Stream object at 0x7fae66f458&gt;, block = (1024, 1, 1), shared = 0, texrefs = [], time_kernel = False<br/><br/>    def function_call(func, *args, **kwargs):<br/>        grid = kwargs.pop(&quot;grid&quot;, (1, 1))<br/>        stream = kwargs.pop(&quot;stream&quot;, None)<br/>        block = kwargs.pop(&quot;block&quot;, None)<br/>        shared = kwargs.pop(&quot;shared&quot;, 0)<br/>        texrefs = kwargs.pop(&quot;texrefs&quot;, [])<br/>        time_kernel = kwargs.pop(&quot;time_kernel&quot;, False)<br/>    <br/>        if kwargs:<br/>            raise ValueError(<br/>                    &quot;extra keyword arguments: %s&quot;<br/>                    % (&quot;,&quot;.join(six.iterkeys(kwargs))))<br/>    <br/>        if block is None:<br/>            raise ValueError(&quot;must specify block size&quot;)<br/>    <br/>&gt;       func._set_block_shape(*block)<br/><span class="error">E       pycuda._driver.LogicError: cuFuncSetBlockShape failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/driver.py:436: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_add_perf[cuda_asm-cuda:0:0]</td>
          <td class="col-duration">0.03</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7fae66f358&gt;, method = &#x27;cuda_asm&#x27;, heavy_performance_load = False<br/><br/>    @pytest.mark.perf<br/>    def test_add_perf(thread, method, heavy_performance_load):<br/>        if method == &quot;cuda_asm&quot; and thread.api.get_id() != cluda.cuda_id():<br/>            pytest.skip()<br/>        if method == &quot;c_from_asm&quot;:<br/>            pytest.skip()<br/>    <br/>        check_func_performance(<br/>            &quot;add(), &quot; + method,<br/>            thread, ntt.add(method=method), ref_add, &#x27;ff_number&#x27;, [&#x27;ff_number&#x27;, &#x27;ff_number&#x27;],<br/>&gt;           heavy_performance_load=heavy_performance_load)<br/><br/>test/test_transform/test_arithmetic.py:309: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_transform/test_arithmetic.py:259: in check_func_performance<br/>    test = get_func_kernel(thread, func_module, output_type, input_types)<br/>test/test_transform/test_arithmetic.py:90: in get_func_kernel<br/>    repetitions=repetitions))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:537: in compile<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:655: in __init__<br/>    self.source, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:504: in _create_program<br/>    src, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:227: in _compile<br/>    return SourceModule(src, no_extern_c=True, options=options, keep=keep)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;pycuda.compiler.SourceModule object at 0x7fae5c2550&gt;<br/>source = &#x27;\n\n\n    #define CUDA\n    // taken from pycuda._cluda\n    #define LOCAL_BARRIER __syncthreads()\n\n    #define WIT...\n                res\n                , a2_load\n                );\n        }\n\n        dest[i] = res;\n    }\n    &#x27;<br/>nvcc = &#x27;nvcc&#x27;, options = [], keep = False, no_extern_c = True, arch = None, code = None, cache_dir = None, include_dirs = []<br/><br/>    def __init__(self, source, nvcc=&quot;nvcc&quot;, options=None, keep=False,<br/>            no_extern_c=False, arch=None, code=None, cache_dir=None,<br/>            include_dirs=[]):<br/>        self._check_arch(arch)<br/>    <br/>        cubin = compile(source, nvcc, options, keep, no_extern_c,<br/>                arch, code, cache_dir, include_dirs)<br/>    <br/>        from pycuda.driver import module_from_buffer<br/>&gt;       self.module = module_from_buffer(cubin)<br/><span class="error">E       pycuda._driver.LogicError: cuModuleLoadDataEx failed: invalid device context -</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/compiler.py:294: LogicError<br/> -------------------------------Captured log call-------------------------------- <br/>[31m[1mERROR   [0m root:api.py:507 Failed to compile:
1:
2:
3:
4:    #define CUDA
5:    // taken from pycuda._cluda
6:    #define LOCAL_BARRIER __syncthreads()
7:
8:    #define WITHIN_KERNEL __device__
9:    #define KERNEL extern &quot;C&quot; __global__
10:    #define GLOBAL_MEM /* empty */
11:    #define GLOBAL_MEM_ARG /* empty */
12:    #define LOCAL_MEM __shared__
13:    #define LOCAL_MEM_DYNAMIC extern __shared__
14:    #define LOCAL_MEM_ARG /* empty */
15:    #define CONSTANT_MEM __constant__
16:    #define CONSTANT_MEM_ARG /* empty */
17:    #define INLINE __forceinline__
18:    #define SIZE_T int
19:    #define VSIZE_T int
20:
21:    // used to align fields in structures
22:    #define ALIGN(bytes) __align__(bytes)
23:
24:    
25:
26:    WITHIN_KERNEL SIZE_T get_local_id(unsigned int dim)
27:    {
28:        if(dim == 0) return threadIdx.x;
29:        if(dim == 1) return threadIdx.y;
30:        if(dim == 2) return threadIdx.z;
31:        return 0;
32:    }
33:
34:    WITHIN_KERNEL SIZE_T get_group_id(unsigned int dim)
35:    {
36:        if(dim == 0) return blockIdx.x;
37:        if(dim == 1) return blockIdx.y;
38:        if(dim == 2) return blockIdx.z;
39:        return 0;
40:    }
41:
42:    WITHIN_KERNEL SIZE_T get_local_size(unsigned int dim)
43:    {
44:        if(dim == 0) return blockDim.x;
45:        if(dim == 1) return blockDim.y;
46:        if(dim == 2) return blockDim.z;
47:        return 1;
48:    }
49:
50:    WITHIN_KERNEL SIZE_T get_num_groups(unsigned int dim)
51:    {
52:        if(dim == 0) return gridDim.x;
53:        if(dim == 1) return gridDim.y;
54:        if(dim == 2) return gridDim.z;
55:        return 1;
56:    }
57:
58:    WITHIN_KERNEL SIZE_T get_global_size(unsigned int dim)
59:    {
60:        return get_num_groups(dim) * get_local_size(dim);
61:    }
62:
63:    WITHIN_KERNEL SIZE_T get_global_id(unsigned int dim)
64:    {
65:        return get_local_id(dim) + get_group_id(dim) * get_local_size(dim);
66:    }
67:
68:
69:
70:
71:    #define COMPLEX_CTR(T) make_##T
72:
73:    WITHIN_KERNEL float2 operator+(float2 a, float2 b)
74:    {
75:        return COMPLEX_CTR(float2)(a.x + b.x, a.y + b.y);
76:    }
77:    WITHIN_KERNEL float2 operator-(float2 a, float2 b)
78:    {
79:        return COMPLEX_CTR(float2)(a.x - b.x, a.y - b.y);
80:    }
81:    WITHIN_KERNEL float2 operator+(float2 a) { return a; }
82:    WITHIN_KERNEL float2 operator-(float2 a) { return COMPLEX_CTR(float2)(-a.x, -a.y); }
83:    WITHIN_KERNEL double2 operator+(double2 a, double2 b)
84:    {
85:        return COMPLEX_CTR(double2)(a.x + b.x, a.y + b.y);
86:    }
87:    WITHIN_KERNEL double2 operator-(double2 a, double2 b)
88:    {
89:        return COMPLEX_CTR(double2)(a.x - b.x, a.y - b.y);
90:    }
91:    WITHIN_KERNEL double2 operator+(double2 a) { return a; }
92:    WITHIN_KERNEL double2 operator-(double2 a) { return COMPLEX_CTR(double2)(-a.x, -a.y); }
93:
94:
95:typedef struct __module0_
96:{
97:    unsigned long val;
98:} _module0_;
99:
100:
101:WITHIN_KERNEL INLINE _module0_ _module0_pack(unsigned long x)
102:{
103:    _module0_ res = {x};
104:    return res;
105:}
106:
107:WITHIN_KERNEL INLINE unsigned long _module0_unpack(_module0_ x)
108:{
109:    return x.val;
110:}
111:
112:#define _module0_zero (_module0_pack(0));
113:
114:
115:#define _module0_UNPACK(hi, lo, src) {lo = (src); hi = (src) &gt;&gt; 32;}
116:
117:#ifdef CUDA
118:#define _module0_PACK(hi, lo) (((unsigned long)(hi) &lt;&lt; 32) | (lo))
119:#else
120:#define _module0_PACK(hi, lo) upsample(hi, lo)
121:#endif
122:
123:
124:#define _module0_SUB_CC(hi, lo, x1, x2) { bool cc = (x1) &lt; (x2); lo = (x1) - (x2); if (cc) { hi -= 1; } }
125:#define _module0_ADD_CC(hi, lo, x1, x2, hi_prev) { lo = (x1) + (x2); bool cc = lo &lt; (x2); hi = hi_prev; if (cc) { hi += 1; } }
126:
127:
128:
129:
130:// Addition in FF(P): val_ = a + b mod P.
131:WITHIN_KERNEL INLINE _module0_ _module1_(_module0_ a, _module0_ b)
132:{
133:
134:    _module0_ res = {0};
135:    asm(&quot;{\n\t&quot;
136:        &quot;.reg .u32          m;\n\t&quot;
137:        &quot;.reg .u64          t;\n\t&quot;
138:        &quot;.reg .pred         p;\n\t&quot;
139:        // this = a + b;
140:        &quot;add.u64            %0, %1, %2;\n\t&quot;
141:        // this += (uint32_t)(-(this &lt; b || this &gt;= FFP_MODULUS));
142:        &quot;setp.lt.u64        p, %0, %2;\n\t&quot;
143:        &quot;set.ge.or.u32.u64  m, %0, %3, p;\n\t&quot;
144:        &quot;mov.b64            t, {m, 0};\n\t&quot;
145:        &quot;add.u64            %0, %0, t;\n\t&quot;
146:        &quot;}&quot;
147:        : &quot;+l&quot;(res.val)
148:        : &quot;l&quot;(a.val), &quot;l&quot;(b.val), &quot;l&quot;(18446744069414584321UL));
149:    return res;
150:
151:}
152:
153:
154:
155:    
156:    KERNEL void test(
157:        GLOBAL_MEM _module0_ *dest
158:        , GLOBAL_MEM _module0_ *a1
159:        , GLOBAL_MEM _module0_ *a2
160:        )
161:    {
162:        const SIZE_T i = get_global_id(0);
163:        _module0_ a1_load = a1[i];
164:        _module0_ a2_load = a2[i];
165:
166:        // To stop the compiler from optimizing away the code,
167:        // we need to use the return value somehow.
168:        // We&#x27;re using it as the first argument to the next invocation.
169:        // Assuming here that the first argument of the tested function is either
170:        // a finite field element, or something convertable to it (e.g. uint64).
171:        _module0_ res =
172:            a1_load;
173:
174:        for (int i = 0; i &lt; 1; i++)
175:        {
176:            res = _module1_(
177:                res
178:                , a2_load
179:                );
180:        }
181:
182:        dest[i] = res;
183:    }
184:<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_sub_perf[c-cuda:0:0]</td>
          <td class="col-duration">0.71</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7fae5c7390&gt;, method = &#x27;c&#x27;, heavy_performance_load = False<br/><br/>    @pytest.mark.perf<br/>    def test_sub_perf(thread, method, heavy_performance_load):<br/>        if method == &quot;cuda_asm&quot; and thread.api.get_id() != cluda.cuda_id():<br/>            pytest.skip()<br/>        if method == &quot;c_from_asm&quot;:<br/>            pytest.skip()<br/>    <br/>        check_func_performance(<br/>            &quot;sub(), &quot; + method,<br/>            thread, ntt.sub(method=method), ref_sub, &#x27;ff_number&#x27;, [&#x27;ff_number&#x27;, &#x27;ff_number&#x27;],<br/>&gt;           heavy_performance_load=heavy_performance_load)<br/><br/>test/test_transform/test_arithmetic.py:322: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_transform/test_arithmetic.py:280: in check_func_performance<br/>    perf_test(dest_dev, *arrays_dev, global_size=N)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:731: in __call__<br/>    return self.prepared_call(*args)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:347: in prepared_call<br/>    stream=self._thr._queue, shared=self._local_mem)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>func = &lt;pycuda._driver.Function object at 0x7fadaf9960&gt;<br/>args = (&lt;pycuda._driver.DeviceAllocation object at 0x7fae62e030&gt;, &lt;pycuda._driver.DeviceAllocation object at 0x7fae62e1c0&gt;, &lt;pycuda._driver.DeviceAllocation object at 0x7fae62e080&gt;)<br/>kwargs = {}, grid = (32, 1, 1), stream = &lt;pycuda._driver.Stream object at 0x7fae5c7e68&gt;, block = (1024, 1, 1), shared = 0, texrefs = [], time_kernel = False<br/><br/>    def function_call(func, *args, **kwargs):<br/>        grid = kwargs.pop(&quot;grid&quot;, (1, 1))<br/>        stream = kwargs.pop(&quot;stream&quot;, None)<br/>        block = kwargs.pop(&quot;block&quot;, None)<br/>        shared = kwargs.pop(&quot;shared&quot;, 0)<br/>        texrefs = kwargs.pop(&quot;texrefs&quot;, [])<br/>        time_kernel = kwargs.pop(&quot;time_kernel&quot;, False)<br/>    <br/>        if kwargs:<br/>            raise ValueError(<br/>                    &quot;extra keyword arguments: %s&quot;<br/>                    % (&quot;,&quot;.join(six.iterkeys(kwargs))))<br/>    <br/>        if block is None:<br/>            raise ValueError(&quot;must specify block size&quot;)<br/>    <br/>&gt;       func._set_block_shape(*block)<br/><span class="error">E       pycuda._driver.LogicError: cuFuncSetBlockShape failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/driver.py:436: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_sub_perf[cuda_asm-cuda:0:0]</td>
          <td class="col-duration">0.03</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7fae5c7390&gt;, method = &#x27;cuda_asm&#x27;, heavy_performance_load = False<br/><br/>    @pytest.mark.perf<br/>    def test_sub_perf(thread, method, heavy_performance_load):<br/>        if method == &quot;cuda_asm&quot; and thread.api.get_id() != cluda.cuda_id():<br/>            pytest.skip()<br/>        if method == &quot;c_from_asm&quot;:<br/>            pytest.skip()<br/>    <br/>        check_func_performance(<br/>            &quot;sub(), &quot; + method,<br/>            thread, ntt.sub(method=method), ref_sub, &#x27;ff_number&#x27;, [&#x27;ff_number&#x27;, &#x27;ff_number&#x27;],<br/>&gt;           heavy_performance_load=heavy_performance_load)<br/><br/>test/test_transform/test_arithmetic.py:322: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_transform/test_arithmetic.py:259: in check_func_performance<br/>    test = get_func_kernel(thread, func_module, output_type, input_types)<br/>test/test_transform/test_arithmetic.py:90: in get_func_kernel<br/>    repetitions=repetitions))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:537: in compile<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:655: in __init__<br/>    self.source, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:504: in _create_program<br/>    src, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:227: in _compile<br/>    return SourceModule(src, no_extern_c=True, options=options, keep=keep)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;pycuda.compiler.SourceModule object at 0x7fae5c7668&gt;<br/>source = &#x27;\n\n\n    #define CUDA\n    // taken from pycuda._cluda\n    #define LOCAL_BARRIER __syncthreads()\n\n    #define WIT...\n                res\n                , a2_load\n                );\n        }\n\n        dest[i] = res;\n    }\n    &#x27;<br/>nvcc = &#x27;nvcc&#x27;, options = [], keep = False, no_extern_c = True, arch = None, code = None, cache_dir = None, include_dirs = []<br/><br/>    def __init__(self, source, nvcc=&quot;nvcc&quot;, options=None, keep=False,<br/>            no_extern_c=False, arch=None, code=None, cache_dir=None,<br/>            include_dirs=[]):<br/>        self._check_arch(arch)<br/>    <br/>        cubin = compile(source, nvcc, options, keep, no_extern_c,<br/>                arch, code, cache_dir, include_dirs)<br/>    <br/>        from pycuda.driver import module_from_buffer<br/>&gt;       self.module = module_from_buffer(cubin)<br/><span class="error">E       pycuda._driver.LogicError: cuModuleLoadDataEx failed: invalid device context -</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/compiler.py:294: LogicError<br/> -------------------------------Captured log call-------------------------------- <br/>[31m[1mERROR   [0m root:api.py:507 Failed to compile:
1:
2:
3:
4:    #define CUDA
5:    // taken from pycuda._cluda
6:    #define LOCAL_BARRIER __syncthreads()
7:
8:    #define WITHIN_KERNEL __device__
9:    #define KERNEL extern &quot;C&quot; __global__
10:    #define GLOBAL_MEM /* empty */
11:    #define GLOBAL_MEM_ARG /* empty */
12:    #define LOCAL_MEM __shared__
13:    #define LOCAL_MEM_DYNAMIC extern __shared__
14:    #define LOCAL_MEM_ARG /* empty */
15:    #define CONSTANT_MEM __constant__
16:    #define CONSTANT_MEM_ARG /* empty */
17:    #define INLINE __forceinline__
18:    #define SIZE_T int
19:    #define VSIZE_T int
20:
21:    // used to align fields in structures
22:    #define ALIGN(bytes) __align__(bytes)
23:
24:    
25:
26:    WITHIN_KERNEL SIZE_T get_local_id(unsigned int dim)
27:    {
28:        if(dim == 0) return threadIdx.x;
29:        if(dim == 1) return threadIdx.y;
30:        if(dim == 2) return threadIdx.z;
31:        return 0;
32:    }
33:
34:    WITHIN_KERNEL SIZE_T get_group_id(unsigned int dim)
35:    {
36:        if(dim == 0) return blockIdx.x;
37:        if(dim == 1) return blockIdx.y;
38:        if(dim == 2) return blockIdx.z;
39:        return 0;
40:    }
41:
42:    WITHIN_KERNEL SIZE_T get_local_size(unsigned int dim)
43:    {
44:        if(dim == 0) return blockDim.x;
45:        if(dim == 1) return blockDim.y;
46:        if(dim == 2) return blockDim.z;
47:        return 1;
48:    }
49:
50:    WITHIN_KERNEL SIZE_T get_num_groups(unsigned int dim)
51:    {
52:        if(dim == 0) return gridDim.x;
53:        if(dim == 1) return gridDim.y;
54:        if(dim == 2) return gridDim.z;
55:        return 1;
56:    }
57:
58:    WITHIN_KERNEL SIZE_T get_global_size(unsigned int dim)
59:    {
60:        return get_num_groups(dim) * get_local_size(dim);
61:    }
62:
63:    WITHIN_KERNEL SIZE_T get_global_id(unsigned int dim)
64:    {
65:        return get_local_id(dim) + get_group_id(dim) * get_local_size(dim);
66:    }
67:
68:
69:
70:
71:    #define COMPLEX_CTR(T) make_##T
72:
73:    WITHIN_KERNEL float2 operator+(float2 a, float2 b)
74:    {
75:        return COMPLEX_CTR(float2)(a.x + b.x, a.y + b.y);
76:    }
77:    WITHIN_KERNEL float2 operator-(float2 a, float2 b)
78:    {
79:        return COMPLEX_CTR(float2)(a.x - b.x, a.y - b.y);
80:    }
81:    WITHIN_KERNEL float2 operator+(float2 a) { return a; }
82:    WITHIN_KERNEL float2 operator-(float2 a) { return COMPLEX_CTR(float2)(-a.x, -a.y); }
83:    WITHIN_KERNEL double2 operator+(double2 a, double2 b)
84:    {
85:        return COMPLEX_CTR(double2)(a.x + b.x, a.y + b.y);
86:    }
87:    WITHIN_KERNEL double2 operator-(double2 a, double2 b)
88:    {
89:        return COMPLEX_CTR(double2)(a.x - b.x, a.y - b.y);
90:    }
91:    WITHIN_KERNEL double2 operator+(double2 a) { return a; }
92:    WITHIN_KERNEL double2 operator-(double2 a) { return COMPLEX_CTR(double2)(-a.x, -a.y); }
93:
94:
95:typedef struct __module0_
96:{
97:    unsigned long val;
98:} _module0_;
99:
100:
101:WITHIN_KERNEL INLINE _module0_ _module0_pack(unsigned long x)
102:{
103:    _module0_ res = {x};
104:    return res;
105:}
106:
107:WITHIN_KERNEL INLINE unsigned long _module0_unpack(_module0_ x)
108:{
109:    return x.val;
110:}
111:
112:#define _module0_zero (_module0_pack(0));
113:
114:
115:#define _module0_UNPACK(hi, lo, src) {lo = (src); hi = (src) &gt;&gt; 32;}
116:
117:#ifdef CUDA
118:#define _module0_PACK(hi, lo) (((unsigned long)(hi) &lt;&lt; 32) | (lo))
119:#else
120:#define _module0_PACK(hi, lo) upsample(hi, lo)
121:#endif
122:
123:
124:#define _module0_SUB_CC(hi, lo, x1, x2) { bool cc = (x1) &lt; (x2); lo = (x1) - (x2); if (cc) { hi -= 1; } }
125:#define _module0_ADD_CC(hi, lo, x1, x2, hi_prev) { lo = (x1) + (x2); bool cc = lo &lt; (x2); hi = hi_prev; if (cc) { hi += 1; } }
126:
127:
128:
129:
130:/** Subtraction in FF(P): val_ = a + b mod P. */
131:WITHIN_KERNEL INLINE _module0_ _module1_(_module0_ a, _module0_ b)
132:{
133:
134:    _module0_ res = {0};
135:    asm(&quot;{\n\t&quot;
136:        &quot;.reg .u32          m;\n\t&quot;
137:        &quot;.reg .u64          t;\n\t&quot;
138:        // this = a - b;
139:        &quot;sub.u64            %0, %1, %2;\n\t&quot;
140:        // this -= (uint32_t)(-(this &gt; a));
141:        &quot;set.gt.u32.u64     m, %0, %1;\n\t&quot;
142:        &quot;mov.b64            t, {m, 0};\n\t&quot;
143:        &quot;sub.u64            %0, %0, t;\n\t&quot;
144:        &quot;}&quot;
145:        : &quot;+l&quot;(res.val)
146:        : &quot;l&quot;(a.val), &quot;l&quot;(b.val));
147:    return res;
148:
149:}
150:
151:
152:
153:    
154:    KERNEL void test(
155:        GLOBAL_MEM _module0_ *dest
156:        , GLOBAL_MEM _module0_ *a1
157:        , GLOBAL_MEM _module0_ *a2
158:        )
159:    {
160:        const SIZE_T i = get_global_id(0);
161:        _module0_ a1_load = a1[i];
162:        _module0_ a2_load = a2[i];
163:
164:        // To stop the compiler from optimizing away the code,
165:        // we need to use the return value somehow.
166:        // We&#x27;re using it as the first argument to the next invocation.
167:        // Assuming here that the first argument of the tested function is either
168:        // a finite field element, or something convertable to it (e.g. uint64).
169:        _module0_ res =
170:            a1_load;
171:
172:        for (int i = 0; i &lt; 1; i++)
173:        {
174:            res = _module1_(
175:                res
176:                , a2_load
177:                );
178:        }
179:
180:        dest[i] = res;
181:    }
182:<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_transform_correctness[cuda:0:0-NTT-global_mem-no_conversion-inverse]</td>
          <td class="col-duration">0.78</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, transform_type = &#x27;NTT&#x27;, inverse = True, i32_conversion = False, constant_memory = False<br/><br/>    @pytest.mark.parametrize(&#x27;inverse&#x27;, [False, True], ids=[&#x27;forward&#x27;, &#x27;inverse&#x27;])<br/>    @pytest.mark.parametrize(&#x27;i32_conversion&#x27;, [False, True], ids=[&#x27;no_conversion&#x27;, &#x27;poly_conversion&#x27;])<br/>    @pytest.mark.parametrize(&#x27;constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_transform_correctness(thread, transform_type, inverse, i32_conversion, constant_memory):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        batch_shape = (128,)<br/>    <br/>        if transform_type == &#x27;FFT&#x27;:<br/>            transform = fft512(use_constant_memory=constant_memory)<br/>            transform_ref = tr_fft.fft_transform_ref<br/>        else:<br/>            transform = ntt1024(use_constant_memory=constant_memory)<br/>            transform_ref = tr_ntt.ntt_transform_ref<br/>    <br/>        comp = Transform(<br/>            transform, batch_shape,<br/>            inverse=inverse, i32_conversion=i32_conversion, transforms_per_block=1,<br/>&gt;           ).compile(thread)<br/><br/>test/test_transform/test_computation.py:53: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/transform/computation.py:67: in _build_plan<br/>    cdata = plan.persistent_array(cdata_arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:304: in persistent_array<br/>    self._persistent_values[name] = self._thread.to_device(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, shape = (2048,), dtype = dtype(&#x27;uint64&#x27;), strides = (8,), offset = 0, nbytes = 16384<br/>allocator = &lt;Boost.Python.function object at 0x37fa4390&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_transform_correctness[cuda:0:0-NTT-global_mem-poly_conversion-forward]</td>
          <td class="col-duration">0.78</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, transform_type = &#x27;NTT&#x27;, inverse = False, i32_conversion = True, constant_memory = False<br/><br/>    @pytest.mark.parametrize(&#x27;inverse&#x27;, [False, True], ids=[&#x27;forward&#x27;, &#x27;inverse&#x27;])<br/>    @pytest.mark.parametrize(&#x27;i32_conversion&#x27;, [False, True], ids=[&#x27;no_conversion&#x27;, &#x27;poly_conversion&#x27;])<br/>    @pytest.mark.parametrize(&#x27;constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_transform_correctness(thread, transform_type, inverse, i32_conversion, constant_memory):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        batch_shape = (128,)<br/>    <br/>        if transform_type == &#x27;FFT&#x27;:<br/>            transform = fft512(use_constant_memory=constant_memory)<br/>            transform_ref = tr_fft.fft_transform_ref<br/>        else:<br/>            transform = ntt1024(use_constant_memory=constant_memory)<br/>            transform_ref = tr_ntt.ntt_transform_ref<br/>    <br/>        comp = Transform(<br/>            transform, batch_shape,<br/>            inverse=inverse, i32_conversion=i32_conversion, transforms_per_block=1,<br/>&gt;           ).compile(thread)<br/><br/>test/test_transform/test_computation.py:53: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/transform/computation.py:67: in _build_plan<br/>    cdata = plan.persistent_array(cdata_arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:304: in persistent_array<br/>    self._persistent_values[name] = self._thread.to_device(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, shape = (2048,), dtype = dtype(&#x27;uint64&#x27;), strides = (8,), offset = 0, nbytes = 16384<br/>allocator = &lt;Boost.Python.function object at 0x37fa4390&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_transform_correctness[cuda:0:0-NTT-global_mem-poly_conversion-inverse]</td>
          <td class="col-duration">0.79</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, transform_type = &#x27;NTT&#x27;, inverse = True, i32_conversion = True, constant_memory = False<br/><br/>    @pytest.mark.parametrize(&#x27;inverse&#x27;, [False, True], ids=[&#x27;forward&#x27;, &#x27;inverse&#x27;])<br/>    @pytest.mark.parametrize(&#x27;i32_conversion&#x27;, [False, True], ids=[&#x27;no_conversion&#x27;, &#x27;poly_conversion&#x27;])<br/>    @pytest.mark.parametrize(&#x27;constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_transform_correctness(thread, transform_type, inverse, i32_conversion, constant_memory):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        batch_shape = (128,)<br/>    <br/>        if transform_type == &#x27;FFT&#x27;:<br/>            transform = fft512(use_constant_memory=constant_memory)<br/>            transform_ref = tr_fft.fft_transform_ref<br/>        else:<br/>            transform = ntt1024(use_constant_memory=constant_memory)<br/>            transform_ref = tr_ntt.ntt_transform_ref<br/>    <br/>        comp = Transform(<br/>            transform, batch_shape,<br/>            inverse=inverse, i32_conversion=i32_conversion, transforms_per_block=1,<br/>&gt;           ).compile(thread)<br/><br/>test/test_transform/test_computation.py:53: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/transform/computation.py:67: in _build_plan<br/>    cdata = plan.persistent_array(cdata_arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:304: in persistent_array<br/>    self._persistent_values[name] = self._thread.to_device(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, shape = (2048,), dtype = dtype(&#x27;uint64&#x27;), strides = (8,), offset = 0, nbytes = 16384<br/>allocator = &lt;Boost.Python.function object at 0x37fa4390&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_transform_correctness[cuda:0:0-NTT-constant_mem-no_conversion-forward]</td>
          <td class="col-duration">0.78</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, transform_type = &#x27;NTT&#x27;, inverse = False, i32_conversion = False, constant_memory = True<br/><br/>    @pytest.mark.parametrize(&#x27;inverse&#x27;, [False, True], ids=[&#x27;forward&#x27;, &#x27;inverse&#x27;])<br/>    @pytest.mark.parametrize(&#x27;i32_conversion&#x27;, [False, True], ids=[&#x27;no_conversion&#x27;, &#x27;poly_conversion&#x27;])<br/>    @pytest.mark.parametrize(&#x27;constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_transform_correctness(thread, transform_type, inverse, i32_conversion, constant_memory):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        batch_shape = (128,)<br/>    <br/>        if transform_type == &#x27;FFT&#x27;:<br/>            transform = fft512(use_constant_memory=constant_memory)<br/>            transform_ref = tr_fft.fft_transform_ref<br/>        else:<br/>            transform = ntt1024(use_constant_memory=constant_memory)<br/>            transform_ref = tr_ntt.ntt_transform_ref<br/>    <br/>        comp = Transform(<br/>            transform, batch_shape,<br/>            inverse=inverse, i32_conversion=i32_conversion, transforms_per_block=1,<br/>&gt;           ).compile(thread)<br/><br/>test/test_transform/test_computation.py:53: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/transform/computation.py:65: in _build_plan<br/>    cdata = plan.constant_array(cdata_arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:332: in constant_array<br/>    self._constant_arrays[name] = self._thread.to_device(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, shape = (2048,), dtype = dtype(&#x27;uint64&#x27;), strides = (8,), offset = 0, nbytes = 16384<br/>allocator = &lt;Boost.Python.function object at 0x37fa4390&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_transform_correctness[cuda:0:0-NTT-constant_mem-no_conversion-inverse]</td>
          <td class="col-duration">0.78</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, transform_type = &#x27;NTT&#x27;, inverse = True, i32_conversion = False, constant_memory = True<br/><br/>    @pytest.mark.parametrize(&#x27;inverse&#x27;, [False, True], ids=[&#x27;forward&#x27;, &#x27;inverse&#x27;])<br/>    @pytest.mark.parametrize(&#x27;i32_conversion&#x27;, [False, True], ids=[&#x27;no_conversion&#x27;, &#x27;poly_conversion&#x27;])<br/>    @pytest.mark.parametrize(&#x27;constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_transform_correctness(thread, transform_type, inverse, i32_conversion, constant_memory):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        batch_shape = (128,)<br/>    <br/>        if transform_type == &#x27;FFT&#x27;:<br/>            transform = fft512(use_constant_memory=constant_memory)<br/>            transform_ref = tr_fft.fft_transform_ref<br/>        else:<br/>            transform = ntt1024(use_constant_memory=constant_memory)<br/>            transform_ref = tr_ntt.ntt_transform_ref<br/>    <br/>        comp = Transform(<br/>            transform, batch_shape,<br/>            inverse=inverse, i32_conversion=i32_conversion, transforms_per_block=1,<br/>&gt;           ).compile(thread)<br/><br/>test/test_transform/test_computation.py:53: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/transform/computation.py:65: in _build_plan<br/>    cdata = plan.constant_array(cdata_arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:332: in constant_array<br/>    self._constant_arrays[name] = self._thread.to_device(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, shape = (2048,), dtype = dtype(&#x27;uint64&#x27;), strides = (8,), offset = 0, nbytes = 16384<br/>allocator = &lt;Boost.Python.function object at 0x37fa4390&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_transform_correctness[cuda:0:0-NTT-constant_mem-poly_conversion-forward]</td>
          <td class="col-duration">0.84</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, transform_type = &#x27;NTT&#x27;, inverse = False, i32_conversion = True, constant_memory = True<br/><br/>    @pytest.mark.parametrize(&#x27;inverse&#x27;, [False, True], ids=[&#x27;forward&#x27;, &#x27;inverse&#x27;])<br/>    @pytest.mark.parametrize(&#x27;i32_conversion&#x27;, [False, True], ids=[&#x27;no_conversion&#x27;, &#x27;poly_conversion&#x27;])<br/>    @pytest.mark.parametrize(&#x27;constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_transform_correctness(thread, transform_type, inverse, i32_conversion, constant_memory):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        batch_shape = (128,)<br/>    <br/>        if transform_type == &#x27;FFT&#x27;:<br/>            transform = fft512(use_constant_memory=constant_memory)<br/>            transform_ref = tr_fft.fft_transform_ref<br/>        else:<br/>            transform = ntt1024(use_constant_memory=constant_memory)<br/>            transform_ref = tr_ntt.ntt_transform_ref<br/>    <br/>        comp = Transform(<br/>            transform, batch_shape,<br/>            inverse=inverse, i32_conversion=i32_conversion, transforms_per_block=1,<br/>&gt;           ).compile(thread)<br/><br/>test/test_transform/test_computation.py:53: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/transform/computation.py:65: in _build_plan<br/>    cdata = plan.constant_array(cdata_arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:332: in constant_array<br/>    self._constant_arrays[name] = self._thread.to_device(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, shape = (2048,), dtype = dtype(&#x27;uint64&#x27;), strides = (8,), offset = 0, nbytes = 16384<br/>allocator = &lt;Boost.Python.function object at 0x37fa4390&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_transform_correctness[cuda:0:0-NTT-constant_mem-poly_conversion-inverse]</td>
          <td class="col-duration">0.85</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, transform_type = &#x27;NTT&#x27;, inverse = True, i32_conversion = True, constant_memory = True<br/><br/>    @pytest.mark.parametrize(&#x27;inverse&#x27;, [False, True], ids=[&#x27;forward&#x27;, &#x27;inverse&#x27;])<br/>    @pytest.mark.parametrize(&#x27;i32_conversion&#x27;, [False, True], ids=[&#x27;no_conversion&#x27;, &#x27;poly_conversion&#x27;])<br/>    @pytest.mark.parametrize(&#x27;constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_transform_correctness(thread, transform_type, inverse, i32_conversion, constant_memory):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        batch_shape = (128,)<br/>    <br/>        if transform_type == &#x27;FFT&#x27;:<br/>            transform = fft512(use_constant_memory=constant_memory)<br/>            transform_ref = tr_fft.fft_transform_ref<br/>        else:<br/>            transform = ntt1024(use_constant_memory=constant_memory)<br/>            transform_ref = tr_ntt.ntt_transform_ref<br/>    <br/>        comp = Transform(<br/>            transform, batch_shape,<br/>            inverse=inverse, i32_conversion=i32_conversion, transforms_per_block=1,<br/>&gt;           ).compile(thread)<br/><br/>test/test_transform/test_computation.py:53: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/transform/computation.py:65: in _build_plan<br/>    cdata = plan.constant_array(cdata_arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:332: in constant_array<br/>    self._constant_arrays[name] = self._thread.to_device(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, shape = (2048,), dtype = dtype(&#x27;uint64&#x27;), strides = (8,), offset = 0, nbytes = 16384<br/>allocator = &lt;Boost.Python.function object at 0x37fa4390&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_transform_correctness[cuda:0:0-FFT-global_mem-no_conversion-forward]</td>
          <td class="col-duration">0.01</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, transform_type = &#x27;FFT&#x27;, inverse = False, i32_conversion = False, constant_memory = False<br/><br/>    @pytest.mark.parametrize(&#x27;inverse&#x27;, [False, True], ids=[&#x27;forward&#x27;, &#x27;inverse&#x27;])<br/>    @pytest.mark.parametrize(&#x27;i32_conversion&#x27;, [False, True], ids=[&#x27;no_conversion&#x27;, &#x27;poly_conversion&#x27;])<br/>    @pytest.mark.parametrize(&#x27;constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_transform_correctness(thread, transform_type, inverse, i32_conversion, constant_memory):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        batch_shape = (128,)<br/>    <br/>        if transform_type == &#x27;FFT&#x27;:<br/>            transform = fft512(use_constant_memory=constant_memory)<br/>            transform_ref = tr_fft.fft_transform_ref<br/>        else:<br/>            transform = ntt1024(use_constant_memory=constant_memory)<br/>            transform_ref = tr_ntt.ntt_transform_ref<br/>    <br/>        comp = Transform(<br/>            transform, batch_shape,<br/>            inverse=inverse, i32_conversion=i32_conversion, transforms_per_block=1,<br/>&gt;           ).compile(thread)<br/><br/>test/test_transform/test_computation.py:53: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/transform/computation.py:67: in _build_plan<br/>    cdata = plan.persistent_array(cdata_arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:304: in persistent_array<br/>    self._persistent_values[name] = self._thread.to_device(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, shape = (1024,), dtype = dtype(&#x27;complex128&#x27;), strides = (16,), offset = 0, nbytes = 16384<br/>allocator = &lt;Boost.Python.function object at 0x37fa4390&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_transform_correctness[cuda:0:0-FFT-global_mem-no_conversion-inverse]</td>
          <td class="col-duration">0.01</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, transform_type = &#x27;FFT&#x27;, inverse = True, i32_conversion = False, constant_memory = False<br/><br/>    @pytest.mark.parametrize(&#x27;inverse&#x27;, [False, True], ids=[&#x27;forward&#x27;, &#x27;inverse&#x27;])<br/>    @pytest.mark.parametrize(&#x27;i32_conversion&#x27;, [False, True], ids=[&#x27;no_conversion&#x27;, &#x27;poly_conversion&#x27;])<br/>    @pytest.mark.parametrize(&#x27;constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_transform_correctness(thread, transform_type, inverse, i32_conversion, constant_memory):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        batch_shape = (128,)<br/>    <br/>        if transform_type == &#x27;FFT&#x27;:<br/>            transform = fft512(use_constant_memory=constant_memory)<br/>            transform_ref = tr_fft.fft_transform_ref<br/>        else:<br/>            transform = ntt1024(use_constant_memory=constant_memory)<br/>            transform_ref = tr_ntt.ntt_transform_ref<br/>    <br/>        comp = Transform(<br/>            transform, batch_shape,<br/>            inverse=inverse, i32_conversion=i32_conversion, transforms_per_block=1,<br/>&gt;           ).compile(thread)<br/><br/>test/test_transform/test_computation.py:53: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/transform/computation.py:67: in _build_plan<br/>    cdata = plan.persistent_array(cdata_arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:304: in persistent_array<br/>    self._persistent_values[name] = self._thread.to_device(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, shape = (1024,), dtype = dtype(&#x27;complex128&#x27;), strides = (16,), offset = 0, nbytes = 16384<br/>allocator = &lt;Boost.Python.function object at 0x37fa4390&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_transform_correctness[cuda:0:0-FFT-global_mem-poly_conversion-forward]</td>
          <td class="col-duration">0.01</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, transform_type = &#x27;FFT&#x27;, inverse = False, i32_conversion = True, constant_memory = False<br/><br/>    @pytest.mark.parametrize(&#x27;inverse&#x27;, [False, True], ids=[&#x27;forward&#x27;, &#x27;inverse&#x27;])<br/>    @pytest.mark.parametrize(&#x27;i32_conversion&#x27;, [False, True], ids=[&#x27;no_conversion&#x27;, &#x27;poly_conversion&#x27;])<br/>    @pytest.mark.parametrize(&#x27;constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_transform_correctness(thread, transform_type, inverse, i32_conversion, constant_memory):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        batch_shape = (128,)<br/>    <br/>        if transform_type == &#x27;FFT&#x27;:<br/>            transform = fft512(use_constant_memory=constant_memory)<br/>            transform_ref = tr_fft.fft_transform_ref<br/>        else:<br/>            transform = ntt1024(use_constant_memory=constant_memory)<br/>            transform_ref = tr_ntt.ntt_transform_ref<br/>    <br/>        comp = Transform(<br/>            transform, batch_shape,<br/>            inverse=inverse, i32_conversion=i32_conversion, transforms_per_block=1,<br/>&gt;           ).compile(thread)<br/><br/>test/test_transform/test_computation.py:53: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/transform/computation.py:67: in _build_plan<br/>    cdata = plan.persistent_array(cdata_arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:304: in persistent_array<br/>    self._persistent_values[name] = self._thread.to_device(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, shape = (1024,), dtype = dtype(&#x27;complex128&#x27;), strides = (16,), offset = 0, nbytes = 16384<br/>allocator = &lt;Boost.Python.function object at 0x37fa4390&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_transform_correctness[cuda:0:0-FFT-global_mem-poly_conversion-inverse]</td>
          <td class="col-duration">0.01</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, transform_type = &#x27;FFT&#x27;, inverse = True, i32_conversion = True, constant_memory = False<br/><br/>    @pytest.mark.parametrize(&#x27;inverse&#x27;, [False, True], ids=[&#x27;forward&#x27;, &#x27;inverse&#x27;])<br/>    @pytest.mark.parametrize(&#x27;i32_conversion&#x27;, [False, True], ids=[&#x27;no_conversion&#x27;, &#x27;poly_conversion&#x27;])<br/>    @pytest.mark.parametrize(&#x27;constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_transform_correctness(thread, transform_type, inverse, i32_conversion, constant_memory):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        batch_shape = (128,)<br/>    <br/>        if transform_type == &#x27;FFT&#x27;:<br/>            transform = fft512(use_constant_memory=constant_memory)<br/>            transform_ref = tr_fft.fft_transform_ref<br/>        else:<br/>            transform = ntt1024(use_constant_memory=constant_memory)<br/>            transform_ref = tr_ntt.ntt_transform_ref<br/>    <br/>        comp = Transform(<br/>            transform, batch_shape,<br/>            inverse=inverse, i32_conversion=i32_conversion, transforms_per_block=1,<br/>&gt;           ).compile(thread)<br/><br/>test/test_transform/test_computation.py:53: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/transform/computation.py:67: in _build_plan<br/>    cdata = plan.persistent_array(cdata_arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:304: in persistent_array<br/>    self._persistent_values[name] = self._thread.to_device(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, shape = (1024,), dtype = dtype(&#x27;complex128&#x27;), strides = (16,), offset = 0, nbytes = 16384<br/>allocator = &lt;Boost.Python.function object at 0x37fa4390&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_transform_correctness[cuda:0:0-FFT-constant_mem-no_conversion-forward]</td>
          <td class="col-duration">0.01</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, transform_type = &#x27;FFT&#x27;, inverse = False, i32_conversion = False, constant_memory = True<br/><br/>    @pytest.mark.parametrize(&#x27;inverse&#x27;, [False, True], ids=[&#x27;forward&#x27;, &#x27;inverse&#x27;])<br/>    @pytest.mark.parametrize(&#x27;i32_conversion&#x27;, [False, True], ids=[&#x27;no_conversion&#x27;, &#x27;poly_conversion&#x27;])<br/>    @pytest.mark.parametrize(&#x27;constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_transform_correctness(thread, transform_type, inverse, i32_conversion, constant_memory):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        batch_shape = (128,)<br/>    <br/>        if transform_type == &#x27;FFT&#x27;:<br/>            transform = fft512(use_constant_memory=constant_memory)<br/>            transform_ref = tr_fft.fft_transform_ref<br/>        else:<br/>            transform = ntt1024(use_constant_memory=constant_memory)<br/>            transform_ref = tr_ntt.ntt_transform_ref<br/>    <br/>        comp = Transform(<br/>            transform, batch_shape,<br/>            inverse=inverse, i32_conversion=i32_conversion, transforms_per_block=1,<br/>&gt;           ).compile(thread)<br/><br/>test/test_transform/test_computation.py:53: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/transform/computation.py:65: in _build_plan<br/>    cdata = plan.constant_array(cdata_arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:332: in constant_array<br/>    self._constant_arrays[name] = self._thread.to_device(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, shape = (1024,), dtype = dtype(&#x27;complex128&#x27;), strides = (16,), offset = 0, nbytes = 16384<br/>allocator = &lt;Boost.Python.function object at 0x37fa4390&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_transform_correctness[cuda:0:0-FFT-constant_mem-no_conversion-inverse]</td>
          <td class="col-duration">0.01</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, transform_type = &#x27;FFT&#x27;, inverse = True, i32_conversion = False, constant_memory = True<br/><br/>    @pytest.mark.parametrize(&#x27;inverse&#x27;, [False, True], ids=[&#x27;forward&#x27;, &#x27;inverse&#x27;])<br/>    @pytest.mark.parametrize(&#x27;i32_conversion&#x27;, [False, True], ids=[&#x27;no_conversion&#x27;, &#x27;poly_conversion&#x27;])<br/>    @pytest.mark.parametrize(&#x27;constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_transform_correctness(thread, transform_type, inverse, i32_conversion, constant_memory):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        batch_shape = (128,)<br/>    <br/>        if transform_type == &#x27;FFT&#x27;:<br/>            transform = fft512(use_constant_memory=constant_memory)<br/>            transform_ref = tr_fft.fft_transform_ref<br/>        else:<br/>            transform = ntt1024(use_constant_memory=constant_memory)<br/>            transform_ref = tr_ntt.ntt_transform_ref<br/>    <br/>        comp = Transform(<br/>            transform, batch_shape,<br/>            inverse=inverse, i32_conversion=i32_conversion, transforms_per_block=1,<br/>&gt;           ).compile(thread)<br/><br/>test/test_transform/test_computation.py:53: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/transform/computation.py:65: in _build_plan<br/>    cdata = plan.constant_array(cdata_arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:332: in constant_array<br/>    self._constant_arrays[name] = self._thread.to_device(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, shape = (1024,), dtype = dtype(&#x27;complex128&#x27;), strides = (16,), offset = 0, nbytes = 16384<br/>allocator = &lt;Boost.Python.function object at 0x37fa4390&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_transform_correctness[cuda:0:0-FFT-constant_mem-poly_conversion-forward]</td>
          <td class="col-duration">0.01</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, transform_type = &#x27;FFT&#x27;, inverse = False, i32_conversion = True, constant_memory = True<br/><br/>    @pytest.mark.parametrize(&#x27;inverse&#x27;, [False, True], ids=[&#x27;forward&#x27;, &#x27;inverse&#x27;])<br/>    @pytest.mark.parametrize(&#x27;i32_conversion&#x27;, [False, True], ids=[&#x27;no_conversion&#x27;, &#x27;poly_conversion&#x27;])<br/>    @pytest.mark.parametrize(&#x27;constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_transform_correctness(thread, transform_type, inverse, i32_conversion, constant_memory):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        batch_shape = (128,)<br/>    <br/>        if transform_type == &#x27;FFT&#x27;:<br/>            transform = fft512(use_constant_memory=constant_memory)<br/>            transform_ref = tr_fft.fft_transform_ref<br/>        else:<br/>            transform = ntt1024(use_constant_memory=constant_memory)<br/>            transform_ref = tr_ntt.ntt_transform_ref<br/>    <br/>        comp = Transform(<br/>            transform, batch_shape,<br/>            inverse=inverse, i32_conversion=i32_conversion, transforms_per_block=1,<br/>&gt;           ).compile(thread)<br/><br/>test/test_transform/test_computation.py:53: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/transform/computation.py:65: in _build_plan<br/>    cdata = plan.constant_array(cdata_arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:332: in constant_array<br/>    self._constant_arrays[name] = self._thread.to_device(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, shape = (1024,), dtype = dtype(&#x27;complex128&#x27;), strides = (16,), offset = 0, nbytes = 16384<br/>allocator = &lt;Boost.Python.function object at 0x37fa4390&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_transform_correctness[cuda:0:0-FFT-constant_mem-poly_conversion-inverse]</td>
          <td class="col-duration">0.01</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, transform_type = &#x27;FFT&#x27;, inverse = True, i32_conversion = True, constant_memory = True<br/><br/>    @pytest.mark.parametrize(&#x27;inverse&#x27;, [False, True], ids=[&#x27;forward&#x27;, &#x27;inverse&#x27;])<br/>    @pytest.mark.parametrize(&#x27;i32_conversion&#x27;, [False, True], ids=[&#x27;no_conversion&#x27;, &#x27;poly_conversion&#x27;])<br/>    @pytest.mark.parametrize(&#x27;constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_transform_correctness(thread, transform_type, inverse, i32_conversion, constant_memory):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        batch_shape = (128,)<br/>    <br/>        if transform_type == &#x27;FFT&#x27;:<br/>            transform = fft512(use_constant_memory=constant_memory)<br/>            transform_ref = tr_fft.fft_transform_ref<br/>        else:<br/>            transform = ntt1024(use_constant_memory=constant_memory)<br/>            transform_ref = tr_ntt.ntt_transform_ref<br/>    <br/>        comp = Transform(<br/>            transform, batch_shape,<br/>            inverse=inverse, i32_conversion=i32_conversion, transforms_per_block=1,<br/>&gt;           ).compile(thread)<br/><br/>test/test_transform/test_computation.py:53: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/transform/computation.py:65: in _build_plan<br/>    cdata = plan.constant_array(cdata_arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:332: in constant_array<br/>    self._constant_arrays[name] = self._thread.to_device(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7faee7fef0&gt;, shape = (1024,), dtype = dtype(&#x27;complex128&#x27;), strides = (16,), offset = 0, nbytes = 16384<br/>allocator = &lt;Boost.Python.function object at 0x37fa4390&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_polynomial_multiplication[cuda:0:0-NTT]</td>
          <td class="col-duration">2.69</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f8d56a4a8&gt;, transform_type = &#x27;NTT&#x27;<br/><br/>    def test_polynomial_multiplication(thread, transform_type):<br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        batch_shape = (10,)<br/>    <br/>        if transform_type == &#x27;FFT&#x27;:<br/>            transform = fft512()<br/>            transform_ref = tr_fft.fft_transform_ref<br/>            tr_mul_ref = tr_fft.fft_transformed_mul_ref<br/>        else:<br/>            transform = ntt1024()<br/>            transform_ref = tr_ntt.ntt_transform_ref<br/>            tr_mul_ref = tr_ntt.ntt_transformed_mul_ref<br/>    <br/>        tr_forward = Transform(<br/>            transform, batch_shape,<br/>            inverse=False, i32_conversion=True, transforms_per_block=1,<br/>            ).compile(thread)<br/>        tr_inverse = Transform(<br/>            transform, batch_shape,<br/>            inverse=True, i32_conversion=True, transforms_per_block=1,<br/>            ).compile(thread)<br/>    <br/>        a = numpy.random.randint(-2**31, 2**31, size=batch_shape + (1024,), dtype=numpy.int32)<br/>        b = numpy.random.randint(-1000, 1000, size=batch_shape + (1024,), dtype=numpy.int32)<br/>    <br/>        a_dev = thread.to_device(a)<br/>        b_dev = thread.to_device(b)<br/>        a_tr_dev = thread.empty_like(tr_forward.parameter.output)<br/>        b_tr_dev = thread.empty_like(tr_forward.parameter.output)<br/>        res_dev = thread.empty_like(tr_inverse.parameter.output)<br/>    <br/>        tr_forward(a_tr_dev, a_dev)<br/>        tr_forward(b_tr_dev, b_dev)<br/>        res_tr = tr_mul_ref(a_tr_dev.get(), b_tr_dev.get())<br/>&gt;       res_tr_dev = thread.to_device(res_tr)<br/><br/>test/test_transform/test_computation.py:118: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f8d56a4a8&gt;, shape = (10, 1024), dtype = dtype(&#x27;uint64&#x27;), strides = (8192, 8), offset = 0, nbytes = 81920<br/>allocator = &lt;Boost.Python.function object at 0x37fa4390&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_polynomial_multiplication[cuda:0:0-FFT]</td>
          <td class="col-duration">0.01</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f8d56a4a8&gt;, transform_type = &#x27;FFT&#x27;<br/><br/>    def test_polynomial_multiplication(thread, transform_type):<br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        batch_shape = (10,)<br/>    <br/>        if transform_type == &#x27;FFT&#x27;:<br/>            transform = fft512()<br/>            transform_ref = tr_fft.fft_transform_ref<br/>            tr_mul_ref = tr_fft.fft_transformed_mul_ref<br/>        else:<br/>            transform = ntt1024()<br/>            transform_ref = tr_ntt.ntt_transform_ref<br/>            tr_mul_ref = tr_ntt.ntt_transformed_mul_ref<br/>    <br/>        tr_forward = Transform(<br/>            transform, batch_shape,<br/>            inverse=False, i32_conversion=True, transforms_per_block=1,<br/>&gt;           ).compile(thread)<br/><br/>test/test_transform/test_computation.py:100: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/transform/computation.py:67: in _build_plan<br/>    cdata = plan.persistent_array(cdata_arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:304: in persistent_array<br/>    self._persistent_values[name] = self._thread.to_device(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f8d56a4a8&gt;, shape = (1024,), dtype = dtype(&#x27;complex128&#x27;), strides = (16,), offset = 0, nbytes = 16384<br/>allocator = &lt;Boost.Python.function object at 0x37fa4390&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_ntt_performance[cuda:0:0-global_mem-2]</td>
          <td class="col-duration">33.76</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f8d55de80&gt;, transforms_per_block = 2, constant_memory = False, heavy_performance_load = False<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(&#x27;transforms_per_block&#x27;, [1, 2, 4])<br/>    @pytest.mark.parametrize(&#x27;constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_ntt_performance(thread, transforms_per_block, constant_memory, heavy_performance_load):<br/>    <br/>        if not transform_supported(thread.device_params, &#x27;NTT&#x27;):<br/>            pytest.skip()<br/>    <br/>        if transforms_per_block &gt; max_supported_transforms_per_block(thread.device_params, &#x27;NTT&#x27;):<br/>            pytest.skip()<br/>    <br/>        is_cuda = thread.api.get_id() == cuda_id()<br/>    <br/>        methods = list(itertools.product(<br/>            [&#x27;cuda_asm&#x27;, &#x27;c&#x27;], # base method<br/>            [&#x27;cuda_asm&#x27;, &#x27;c_from_asm&#x27;, &#x27;c&#x27;], # mul method<br/>            [&#x27;cuda_asm&#x27;, &#x27;c_from_asm&#x27;, &#x27;c&#x27;] # lsh method<br/>            ))<br/>    <br/>        if not is_cuda:<br/>            # filter out all usage of CUDA asm if we&#x27;re on OpenCL<br/>            methods = [ms for ms in methods if &#x27;cuda_asm&#x27; not in ms]<br/>    <br/>        batch_shape = (2**14,)<br/>        a = get_test_array(batch_shape + (1024,), &quot;ff_number&quot;)<br/>    <br/>        kernel_repetitions = 100 if heavy_performance_load else 5<br/>    <br/>        a_dev = thread.to_device(a)<br/>        res_dev = thread.empty_like(a_dev)<br/>    <br/>        # TODO: compute a reference NTT when it&#x27;s fast enough on CPU<br/>        #res_ref = tr_ntt.ntt_transform_ref(a)<br/>    <br/>        print()<br/>        min_times = []<br/>        for base_method, mul_method, lsh_method in methods:<br/>    <br/>            transform = ntt1024(<br/>                base_method=base_method, mul_method=mul_method, lsh_method=lsh_method,<br/>                use_constant_memory=constant_memory)<br/>    <br/>            ntt_comp = Transform(<br/>                transform, batch_shape, transforms_per_block=transforms_per_block,<br/>&gt;               ).compile(thread)<br/><br/>test/test_transform/test_computation.py:189: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/transform/computation.py:91: in _build_plan<br/>    slices=(len(output.shape) - 1, 1)))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:473: in kernel_call<br/>    keep=self._keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:566: in compile_static<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:786: in __init__<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:655: in __init__<br/>    self.source, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:504: in _create_program<br/>    src, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:227: in _compile<br/>    return SourceModule(src, no_extern_c=True, options=options, keep=keep)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;pycuda.compiler.SourceModule object at 0x7fae4c74a8&gt;<br/>source = &#x27;\n\n\n    #define CUDA\n    // taken from pycuda._cluda\n    #define LOCAL_BARRIER __syncthreads()\n\n    #define WIT...(\n            batch_id * 2 + transform_in_block, thread_in_transform + i * 128,\n            r_out[i]);\n    }\n}\n\n&#x27;<br/>nvcc = &#x27;nvcc&#x27;, options = [], keep = False, no_extern_c = True, arch = None, code = None, cache_dir = None, include_dirs = []<br/><br/>    def __init__(self, source, nvcc=&quot;nvcc&quot;, options=None, keep=False,<br/>            no_extern_c=False, arch=None, code=None, cache_dir=None,<br/>            include_dirs=[]):<br/>        self._check_arch(arch)<br/>    <br/>        cubin = compile(source, nvcc, options, keep, no_extern_c,<br/>                arch, code, cache_dir, include_dirs)<br/>    <br/>        from pycuda.driver import module_from_buffer<br/>&gt;       self.module = module_from_buffer(cubin)<br/><span class="error">E       pycuda._driver.LogicError: cuModuleLoadDataEx failed: invalid device context -</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/compiler.py:294: LogicError<br/> ------------------------------Captured stdout call------------------------------ <br/>
  base: cuda_asm, mul: cuda_asm, lsh: cuda_asm
  cuda, 2 per block, test --- min: 0.2611, mean: 0.2631, std: 0.0013
  base: cuda_asm, mul: cuda_asm, lsh: c_from_asm
  cuda, 2 per block, test --- min: 0.3010, mean: 0.3034, std: 0.0013
  base: cuda_asm, mul: cuda_asm, lsh: c
  cuda, 2 per block, test --- min: 0.2619, mean: 0.2632, std: 0.0013
  base: cuda_asm, mul: c_from_asm, lsh: cuda_asm
  cuda, 2 per block, test --- min: 0.2618, mean: 0.2636, std: 0.0011
  base: cuda_asm, mul: c_from_asm, lsh: c_from_asm
  cuda, 2 per block, test --- min: 0.3008, mean: 0.3022, std: 0.0014
  base: cuda_asm, mul: c_from_asm, lsh: c
  cuda, 2 per block, test --- min: 0.2621, mean: 0.2634, std: 0.0012
<br/> -------------------------------Captured log call-------------------------------- <br/>[31m[1mERROR   [0m root:api.py:507 Failed to compile:
1:
2:
3:
4:    #define CUDA
5:    // taken from pycuda._cluda
6:    #define LOCAL_BARRIER __syncthreads()
7:
8:    #define WITHIN_KERNEL __device__
9:    #define KERNEL extern &quot;C&quot; __global__
10:    #define GLOBAL_MEM /* empty */
11:    #define GLOBAL_MEM_ARG /* empty */
12:    #define LOCAL_MEM __shared__
13:    #define LOCAL_MEM_DYNAMIC extern __shared__
14:    #define LOCAL_MEM_ARG /* empty */
15:    #define CONSTANT_MEM __constant__
16:    #define CONSTANT_MEM_ARG /* empty */
17:    #define INLINE __forceinline__
18:    #define SIZE_T int
19:    #define VSIZE_T int
20:
21:    // used to align fields in structures
22:    #define ALIGN(bytes) __align__(bytes)
23:
24:    
25:
26:    WITHIN_KERNEL SIZE_T get_local_id(unsigned int dim)
27:    {
28:        if(dim == 0) return threadIdx.x;
29:        if(dim == 1) return threadIdx.y;
30:        if(dim == 2) return threadIdx.z;
31:        return 0;
32:    }
33:
34:    WITHIN_KERNEL SIZE_T get_group_id(unsigned int dim)
35:    {
36:        if(dim == 0) return blockIdx.x;
37:        if(dim == 1) return blockIdx.y;
38:        if(dim == 2) return blockIdx.z;
39:        return 0;
40:    }
41:
42:    WITHIN_KERNEL SIZE_T get_local_size(unsigned int dim)
43:    {
44:        if(dim == 0) return blockDim.x;
45:        if(dim == 1) return blockDim.y;
46:        if(dim == 2) return blockDim.z;
47:        return 1;
48:    }
49:
50:    WITHIN_KERNEL SIZE_T get_num_groups(unsigned int dim)
51:    {
52:        if(dim == 0) return gridDim.x;
53:        if(dim == 1) return gridDim.y;
54:        if(dim == 2) return gridDim.z;
55:        return 1;
56:    }
57:
58:    WITHIN_KERNEL SIZE_T get_global_size(unsigned int dim)
59:    {
60:        return get_num_groups(dim) * get_local_size(dim);
61:    }
62:
63:    WITHIN_KERNEL SIZE_T get_global_id(unsigned int dim)
64:    {
65:        return get_local_id(dim) + get_group_id(dim) * get_local_size(dim);
66:    }
67:
68:
69:
70:
71:    #define COMPLEX_CTR(T) make_##T
72:
73:    WITHIN_KERNEL float2 operator+(float2 a, float2 b)
74:    {
75:        return COMPLEX_CTR(float2)(a.x + b.x, a.y + b.y);
76:    }
77:    WITHIN_KERNEL float2 operator-(float2 a, float2 b)
78:    {
79:        return COMPLEX_CTR(float2)(a.x - b.x, a.y - b.y);
80:    }
81:    WITHIN_KERNEL float2 operator+(float2 a) { return a; }
82:    WITHIN_KERNEL float2 operator-(float2 a) { return COMPLEX_CTR(float2)(-a.x, -a.y); }
83:    WITHIN_KERNEL double2 operator+(double2 a, double2 b)
84:    {
85:        return COMPLEX_CTR(double2)(a.x + b.x, a.y + b.y);
86:    }
87:    WITHIN_KERNEL double2 operator-(double2 a, double2 b)
88:    {
89:        return COMPLEX_CTR(double2)(a.x - b.x, a.y - b.y);
90:    }
91:    WITHIN_KERNEL double2 operator+(double2 a) { return a; }
92:    WITHIN_KERNEL double2 operator-(double2 a) { return COMPLEX_CTR(double2)(-a.x, -a.y); }
93:
94:WITHIN_KERNEL VSIZE_T virtual_local_id(unsigned int dim)
95:{
96:    if (dim == 1)
97:    {
98:
99:        SIZE_T flat_id =
100:            get_local_id(0) * 1 +
101:            0;
102:
103:        return (flat_id / 1);
104:
105:    }
106:    if (dim == 0)
107:    {
108:
109:        return 0;
110:
111:    }
112:
113:    return 0;
114:}
115:
116:WITHIN_KERNEL VSIZE_T virtual_local_size(unsigned int dim)
117:{
118:    if (dim == 1)
119:    {
120:        return 256;
121:    }
122:    if (dim == 0)
123:    {
124:        return 1;
125:    }
126:
127:    return 1;
128:}
129:
130:WITHIN_KERNEL VSIZE_T virtual_group_id(unsigned int dim)
131:{
132:    if (dim == 1)
133:    {
134:
135:        return 0;
136:
137:    }
138:    if (dim == 0)
139:    {
140:
141:        SIZE_T flat_id =
142:            get_group_id(1) * 1 +
143:            0;
144:
145:        return (flat_id / 1);
146:
147:    }
148:
149:    return 0;
150:}
151:
152:WITHIN_KERNEL VSIZE_T virtual_num_groups(unsigned int dim)
153:{
154:    if (dim == 1)
155:    {
156:        return 1;
157:    }
158:    if (dim == 0)
159:    {
160:        return 8192;
161:    }
162:
163:    return 1;
164:}
165:
166:WITHIN_KERNEL VSIZE_T virtual_global_id(unsigned int dim)
167:{
168:    return virtual_local_id(dim) + virtual_group_id(dim) * virtual_local_size(dim);
169:}
170:
171:WITHIN_KERNEL VSIZE_T virtual_global_size(unsigned int dim)
172:{
173:    if(dim == 1)
174:    {
175:        return 256;
176:    }
177:    if(dim == 0)
178:    {
179:        return 8192;
180:    }
181:
182:    return 1;
183:}
184:
185:WITHIN_KERNEL VSIZE_T virtual_global_flat_id()
186:{
187:    return
188:        virtual_global_id(1) * 1 +
189:        virtual_global_id(0) * 256 +
190:        0;
191:}
192:
193:WITHIN_KERNEL VSIZE_T virtual_global_flat_size()
194:{
195:    return
196:        virtual_global_size(1) *
197:        virtual_global_size(0) *
198:        1;
199:}
200:
201:
202:WITHIN_KERNEL bool virtual_skip_local_threads()
203:{
204:
205:    return false;
206:}
207:
208:WITHIN_KERNEL bool virtual_skip_groups()
209:{
210:
211:    return false;
212:}
213:
214:WITHIN_KERNEL bool virtual_skip_global_threads()
215:{
216:
217:    return false;
218:}
219:
220:
221:
222:#ifndef CUDA
223:#define MARK_VIRTUAL_FUNCTIONS_AS_USED (void)(virtual_num_groups(0)); (void)(virtual_global_flat_id()); (void)(virtual_global_flat_size())
224:#else
225:#define MARK_VIRTUAL_FUNCTIONS_AS_USED
226:#endif
227:
228:#define VIRTUAL_SKIP_THREADS MARK_VIRTUAL_FUNCTIONS_AS_USED; if(virtual_skip_local_threads() || virtual_skip_groups() || virtual_skip_global_threads()) return
229:
230:
231:    // leaf input macro for &quot;input&quot;
232:    #define _module1_(_idx0, _idx1) (_leaf_input[(_idx0) * (1024) + (_idx1) * (1) + (0)])
233:    
234:
235:
236:
237:
238:    
239:    INLINE WITHIN_KERNEL unsigned long _module0_func(
240:        GLOBAL_MEM unsigned long *_leaf_input, VSIZE_T _c_idx0, VSIZE_T _c_idx1)
241:    {
242:        
243:
244:    
245:        
246:        VSIZE_T _idx0 = _c_idx0 / 1;
247:    
248:        
249:        VSIZE_T _idx1 = _c_idx1 / 1;
250:    
251:
252:
253:        return
254:        _module1_(_idx0, _idx1);
255:    }
256:    
257:    #define _module0_(_c_idx0, _c_idx1) _module0_func(        _leaf_input, _c_idx0, _c_idx1)
258:    
259:
260:
261:
262:typedef struct __module3_
263:{
264:    unsigned long val;
265:} _module3_;
266:
267:
268:WITHIN_KERNEL INLINE _module3_ _module3_pack(unsigned long x)
269:{
270:    _module3_ res = {x};
271:    return res;
272:}
273:
274:WITHIN_KERNEL INLINE unsigned long _module3_unpack(_module3_ x)
275:{
276:    return x.val;
277:}
278:
279:#define _module3_zero (_module3_pack(0));
280:
281:
282:#define _module3_UNPACK(hi, lo, src) {lo = (src); hi = (src) &gt;&gt; 32;}
283:
284:#ifdef CUDA
285:#define _module3_PACK(hi, lo) (((unsigned long)(hi) &lt;&lt; 32) | (lo))
286:#else
287:#define _module3_PACK(hi, lo) upsample(hi, lo)
288:#endif
289:
290:
291:#define _module3_SUB_CC(hi, lo, x1, x2) { bool cc = (x1) &lt; (x2); lo = (x1) - (x2); if (cc) { hi -= 1; } }
292:#define _module3_ADD_CC(hi, lo, x1, x2, hi_prev) { lo = (x1) + (x2); bool cc = lo &lt; (x2); hi = hi_prev; if (cc) { hi += 1; } }
293:
294:
295:
296:
297:/** Subtraction in FF(P): val_ = a + b mod P. */
298:WITHIN_KERNEL INLINE _module3_ _module4_(_module3_ a, _module3_ b)
299:{
300:
301:    _module3_ res = {0};
302:    asm(&quot;{\n\t&quot;
303:        &quot;.reg .u32          m;\n\t&quot;
304:        &quot;.reg .u64          t;\n\t&quot;
305:        // this = a - b;
306:        &quot;sub.u64            %0, %1, %2;\n\t&quot;
307:        // this -= (uint32_t)(-(this &gt; a));
308:        &quot;set.gt.u32.u64     m, %0, %1;\n\t&quot;
309:        &quot;mov.b64            t, {m, 0};\n\t&quot;
310:        &quot;sub.u64            %0, %0, t;\n\t&quot;
311:        &quot;}&quot;
312:        : &quot;+l&quot;(res.val)
313:        : &quot;l&quot;(a.val), &quot;l&quot;(b.val));
314:    return res;
315:
316:}
317:
318:
319:
320:// Addition in FF(P): val_ = a + b mod P.
321:WITHIN_KERNEL INLINE _module3_ _module5_(_module3_ a, _module3_ b)
322:{
323:
324:    _module3_ res = {0};
325:    asm(&quot;{\n\t&quot;
326:        &quot;.reg .u32          m;\n\t&quot;
327:        &quot;.reg .u64          t;\n\t&quot;
328:        &quot;.reg .pred         p;\n\t&quot;
329:        // this = a + b;
330:        &quot;add.u64            %0, %1, %2;\n\t&quot;
331:        // this += (uint32_t)(-(this &lt; b || this &gt;= FFP_MODULUS));
332:        &quot;setp.lt.u64        p, %0, %2;\n\t&quot;
333:        &quot;set.ge.or.u32.u64  m, %0, %3, p;\n\t&quot;
334:        &quot;mov.b64            t, {m, 0};\n\t&quot;
335:        &quot;add.u64            %0, %0, t;\n\t&quot;
336:        &quot;}&quot;
337:        : &quot;+l&quot;(res.val)
338:        : &quot;l&quot;(a.val), &quot;l&quot;(b.val), &quot;l&quot;(18446744069414584321UL));
339:    return res;
340:
341:}
342:
343:
344:
345:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
346:WITHIN_KERNEL INLINE _module3_ _module7_(unsigned long a)
347:{
348:
349:    _module3_ res = {0};
350:    asm(&quot;{\n\t&quot;
351:        &quot;.reg .u32        m;\n\t&quot;
352:        &quot;.reg .u64        t;\n\t&quot;
353:        &quot;mov.u64          %0, %1;\n\t&quot;
354:        &quot;set.ge.u32.u64   m, %0, %2;\n\t&quot;
355:        &quot;mov.b64          t, {m, 0};\n\t&quot;
356:        &quot;add.u64         %0, %0, t;\n\t&quot;
357:        &quot;}&quot;
358:        : &quot;+l&quot;(res.val)
359:        : &quot;l&quot;(a), &quot;l&quot;(18446744069414584321UL));
360:    return res;
361:
362:}
363:
364:
365:
366:
367:/**
368:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
369:* @param[in] l An integer in [0, 32)
370:*/
371:WITHIN_KERNEL INLINE _module3_ _module6_(_module3_ x, unsigned int l)
372:{
373:    /*
374:    Algorithm:
375:
376:    We can decompose the shift as
377:
378:        res = x * 2^l = x * M^k * 2^j,
379:
380:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
381:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
382:
383:    After the multiplication by 2^j, the result contains 3 32-bit parts:
384:
385:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
386:
387:    Thus
388:
389:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
390:
391:    Taking the modulus P = M^2 - M + 1, we get
392:
393:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
394:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
395:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
396:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
397:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
398:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
399:
400:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
401:
402:    The processing for the things inside the parentheses is simpler:
403:
404:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
405:    - (x + y) = PACK(s &lt; y, s), where s = x + y
406:      (that is, check for overflow and add 1 in the high half)
407:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
408:      (that is, check for overflow and add (M-1) in the low half)
409:    */
410:
411:
412:
413:
414:        asm(&quot;{\n\t&quot;
415:            &quot;.reg .u32      r0, r1;\n\t&quot;
416:            &quot;.reg .u32      t0, t1, t2;\n\t&quot;
417:            &quot;.reg .u32      n;\n\t&quot;
418:            &quot;.reg .u64      s;\n\t&quot;
419:            // t[2] = (uint32_t)(x &gt;&gt; (64-l));
420:            // t[1] = (uint32_t)(x &gt;&gt; (32-l));
421:            // t[0] = (uint32_t)(x &lt;&lt; l);
422:            &quot;mov.b64        {r0, r1}, %0;\n\t&quot;
423:            &quot;shl.b32        t0, r0, %1;\n\t&quot;
424:            &quot;sub.u32        n, 32, %1;\n\t&quot;
425:            &quot;shr.b64        s, %0, n;\n\t&quot;
426:            &quot;mov.b64        {t1, t2}, s;\n\t&quot;
427:            // mod P
428:            &quot;add.u32        r1, t1, t2;\n\t&quot;
429:            &quot;sub.cc.u32     r0, t0, t2;\n\t&quot;
430:            &quot;subc.u32       r1, r1, 0;\n\t&quot;
431:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
432:            // ret += (uint32_t)(-(ret &lt; ((uint64_t *)t)[0]));
433:            &quot;mov.b64        s, {t0, t1};\n\t&quot;
434:            &quot;set.lt.u32.u64 t2, %0, s;\n\t&quot;
435:            &quot;add.cc.u32     r0, r0, t2;\n\t&quot;
436:            &quot;addc.u32       r1, r1, 0;\n\t&quot;
437:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
438:            &quot;}&quot;
439:            : &quot;+l&quot;(x.val)
440:            : &quot;r&quot;(l));
441:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
442:        return _module7_(x.val);
443:
444:
445:
446:}
447:
448:
449:
450:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
451:WITHIN_KERNEL INLINE _module3_ _module9_(unsigned long a)
452:{
453:
454:    _module3_ res = {0};
455:    asm(&quot;{\n\t&quot;
456:        &quot;.reg .u32        m;\n\t&quot;
457:        &quot;.reg .u64        t;\n\t&quot;
458:        &quot;mov.u64          %0, %1;\n\t&quot;
459:        &quot;set.ge.u32.u64   m, %0, %2;\n\t&quot;
460:        &quot;mov.b64          t, {m, 0};\n\t&quot;
461:        &quot;add.u64         %0, %0, t;\n\t&quot;
462:        &quot;}&quot;
463:        : &quot;+l&quot;(res.val)
464:        : &quot;l&quot;(a), &quot;l&quot;(18446744069414584321UL));
465:    return res;
466:
467:}
468:
469:
470:
471:
472:/**
473:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
474:* @param[in] l An integer in [0, 32)
475:*/
476:WITHIN_KERNEL INLINE _module3_ _module8_(_module3_ x, unsigned int l)
477:{
478:    /*
479:    Algorithm:
480:
481:    We can decompose the shift as
482:
483:        res = x * 2^l = x * M^k * 2^j,
484:
485:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
486:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
487:
488:    After the multiplication by 2^j, the result contains 3 32-bit parts:
489:
490:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
491:
492:    Thus
493:
494:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
495:
496:    Taking the modulus P = M^2 - M + 1, we get
497:
498:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
499:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
500:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
501:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
502:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
503:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
504:
505:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
506:
507:    The processing for the things inside the parentheses is simpler:
508:
509:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
510:    - (x + y) = PACK(s &lt; y, s), where s = x + y
511:      (that is, check for overflow and add 1 in the high half)
512:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
513:      (that is, check for overflow and add (M-1) in the low half)
514:    */
515:
516:
517:
518:
519:        asm(&quot;{\n\t&quot;
520:            &quot;.reg .u32          r0, r1;\n\t&quot;
521:            &quot;.reg .u32          t0, t1, t2;\n\t&quot;
522:            &quot;.reg .u32          n;\n\t&quot;
523:            &quot;.reg .u64          s;\n\t&quot;
524:            &quot;.reg .pred         p, q;\n\t&quot;
525:            // t[2] = (uint32_t)(x &gt;&gt; (96-l));
526:            // t[1] = (uint32_t)(x &gt;&gt; (64-l));
527:            // t[0] = (uint32_t)(x &lt;&lt; (l-32));
528:            &quot;mov.b64            {r0, r1}, %0;\n\t&quot;
529:            &quot;sub.u32            n, %1, 32;\n\t&quot;
530:            &quot;shl.b32            t0, r0, n;\n\t&quot;
531:            &quot;sub.u32            n, 32, n;\n\t&quot;
532:            &quot;shr.b64            s, %0, n;\n\t&quot;
533:            &quot;mov.b64            {t1, t2}, s;\n\t&quot;
534:            // mod P
535:            &quot;add.u32            r1, t0, t1;\n\t&quot;
536:            &quot;sub.cc.u32         r0, 0, t1;\n\t&quot;
537:            &quot;subc.u32           r1, r1, 0;\n\t&quot;
538:            &quot;sub.cc.u32         r0, r0, t2;\n\t&quot;
539:            &quot;subc.u32           r1, r1, 0;\n\t&quot;
540:            &quot;mov.b64            %0, {r0, r1};\n\t&quot;
541:            // ret -= (uint32_t)(-(ret &gt; ((uint64_t)t[0] &lt;&lt; 32) &amp;&amp; t[1] == 0));
542:            &quot;setp.eq.u32        p|q, t1, 0;\n\t&quot;
543:            &quot;mov.b64            s, {0, t0};\n\t&quot;
544:            &quot;set.gt.and.u32.u64 t2, %0, s, p;\n\t&quot;
545:            &quot;sub.cc.u32         r0, r0, t2;\n\t&quot;
546:            &quot;subc.u32           r1, r1, 0;\n\t&quot;
547:            &quot;mov.b64            %0, {r0, r1};\n\t&quot;
548:            // ret += (uint32_t)(-(ret &lt; ((uint64_t)t[0] &lt;&lt; 32) &amp;&amp; t[1] != 0));
549:            &quot;set.lt.and.u32.u64 t2, %0, s, q;\n\t&quot;
550:            &quot;add.cc.u32         r0, r0, t2;\n\t&quot;
551:            &quot;addc.u32           r1, r1, 0;\n\t&quot;
552:            &quot;mov.b64            %0, {r0, r1};\n\t&quot;
553:            &quot;}&quot;
554:            : &quot;+l&quot;(x.val)
555:            : &quot;r&quot;(l));
556:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
557:        return _module9_(x.val);
558:
559:
560:
561:}
562:
563:
564:
565:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
566:WITHIN_KERNEL INLINE _module3_ _module11_(unsigned long a)
567:{
568:
569:    _module3_ res = {0};
570:    asm(&quot;{\n\t&quot;
571:        &quot;.reg .u32        m;\n\t&quot;
572:        &quot;.reg .u64        t;\n\t&quot;
573:        &quot;mov.u64          %0, %1;\n\t&quot;
574:        &quot;set.ge.u32.u64   m, %0, %2;\n\t&quot;
575:        &quot;mov.b64          t, {m, 0};\n\t&quot;
576:        &quot;add.u64         %0, %0, t;\n\t&quot;
577:        &quot;}&quot;
578:        : &quot;+l&quot;(res.val)
579:        : &quot;l&quot;(a), &quot;l&quot;(18446744069414584321UL));
580:    return res;
581:
582:}
583:
584:
585:
586:
587:/**
588:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
589:* @param[in] l An integer in [0, 32)
590:*/
591:WITHIN_KERNEL INLINE _module3_ _module10_(_module3_ x, unsigned int l)
592:{
593:    /*
594:    Algorithm:
595:
596:    We can decompose the shift as
597:
598:        res = x * 2^l = x * M^k * 2^j,
599:
600:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
601:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
602:
603:    After the multiplication by 2^j, the result contains 3 32-bit parts:
604:
605:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
606:
607:    Thus
608:
609:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
610:
611:    Taking the modulus P = M^2 - M + 1, we get
612:
613:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
614:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
615:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
616:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
617:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
618:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
619:
620:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
621:
622:    The processing for the things inside the parentheses is simpler:
623:
624:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
625:    - (x + y) = PACK(s &lt; y, s), where s = x + y
626:      (that is, check for overflow and add 1 in the high half)
627:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
628:      (that is, check for overflow and add (M-1) in the low half)
629:    */
630:
631:
632:
633:
634:        asm(&quot;{\n\t&quot;
635:            &quot;.reg .u32      r0, r1;\n\t&quot;
636:            &quot;.reg .u32      t0, t1, t2;\n\t&quot;
637:            &quot;.reg .u32      n;\n\t&quot;
638:            &quot;.reg .u64      s;\n\t&quot;
639:            // t[2] = (uint32_t)(x &gt;&gt; (128-l));
640:            // t[1] = (uint32_t)(x &gt;&gt; (96-l));
641:            // t[0] = (uint32_t)(x &lt;&lt; (l-64));
642:            &quot;mov.b64        {r0, r1}, %0;\n\t&quot;
643:            &quot;sub.u32        n, %1, 64;\n\t&quot;
644:            &quot;shl.b32        t0, r0, n;\n\t&quot;
645:            &quot;sub.u32        n, 32, n;\n\t&quot;
646:            &quot;shr.b64        s, %0, n;\n\t&quot;
647:            &quot;mov.b64        {t1, t2}, s;\n\t&quot;
648:            // mod P
649:            &quot;add.cc.u32     r0, t1, t0;\n\t&quot;
650:            &quot;addc.u32       r1, t2, 0;\n\t&quot;
651:            &quot;sub.u32        r1, r1, t0;\n\t&quot;
652:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
653:            // ret -= (uint32_t)(-(ret &gt; ((uint64_t *)t)[1]));
654:            &quot;mov.b64        s, {t1, t2};\n\t&quot;
655:            &quot;set.gt.u32.u64 t2, %0, s;\n\t&quot;
656:            &quot;sub.cc.u32     r0, r0, t2;\n\t&quot;
657:            &quot;subc.u32       r1, r1, 0;\n\t&quot;
658:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
659:            &quot;}&quot;
660:            : &quot;+l&quot;(x.val)
661:            : &quot;r&quot;(l));
662:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
663:        x = _module11_(x.val);
664:        x.val = 18446744069414584321UL - x.val;
665:        return x;
666:
667:
668:
669:}
670:
671:
672:
673:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
674:WITHIN_KERNEL INLINE _module3_ _module13_(unsigned long a)
675:{
676:
677:    _module3_ res = {0};
678:    asm(&quot;{\n\t&quot;
679:        &quot;.reg .u32        m;\n\t&quot;
680:        &quot;.reg .u64        t;\n\t&quot;
681:        &quot;mov.u64          %0, %1;\n\t&quot;
682:        &quot;set.ge.u32.u64   m, %0, %2;\n\t&quot;
683:        &quot;mov.b64          t, {m, 0};\n\t&quot;
684:        &quot;add.u64         %0, %0, t;\n\t&quot;
685:        &quot;}&quot;
686:        : &quot;+l&quot;(res.val)
687:        : &quot;l&quot;(a), &quot;l&quot;(18446744069414584321UL));
688:    return res;
689:
690:}
691:
692:
693:
694:
695:/**
696:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
697:* @param[in] l An integer in [0, 32)
698:*/
699:WITHIN_KERNEL INLINE _module3_ _module12_(_module3_ x, unsigned int l)
700:{
701:    /*
702:    Algorithm:
703:
704:    We can decompose the shift as
705:
706:        res = x * 2^l = x * M^k * 2^j,
707:
708:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
709:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
710:
711:    After the multiplication by 2^j, the result contains 3 32-bit parts:
712:
713:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
714:
715:    Thus
716:
717:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
718:
719:    Taking the modulus P = M^2 - M + 1, we get
720:
721:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
722:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
723:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
724:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
725:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
726:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
727:
728:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
729:
730:    The processing for the things inside the parentheses is simpler:
731:
732:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
733:    - (x + y) = PACK(s &lt; y, s), where s = x + y
734:      (that is, check for overflow and add 1 in the high half)
735:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
736:      (that is, check for overflow and add (M-1) in the low half)
737:    */
738:
739:
740:
741:
742:        asm(&quot;{\n\t&quot;
743:            &quot;.reg .u32      r0, r1;\n\t&quot;
744:            &quot;.reg .u32      t0, t1, t2;\n\t&quot;
745:            &quot;.reg .u32      n;\n\t&quot;
746:            &quot;.reg .u64      s;\n\t&quot;
747:            // t[2] = (uint32_t)(x &lt;&lt; (l-160));
748:            // t[1] = (uint32_t)(x &gt;&gt; (224-l));
749:            // t[0] = (uint32_t)(x &gt;&gt; (192-l));
750:            &quot;mov.b64        {r0, r1}, %0;\n\t&quot;
751:            &quot;sub.u32        n, %1, 160;\n\t&quot;
752:            &quot;shl.b32        t2, r0, n;\n\t&quot;
753:            &quot;sub.u32        n, 32, n;\n\t&quot;
754:            &quot;shr.b64        s, %0, n;\n\t&quot;
755:            &quot;mov.b64        {t0, t1}, s;\n\t&quot;
756:            // mod P
757:            &quot;add.cc.u32     r0, t0, t2;\n\t&quot;
758:            &quot;addc.u32       r1, t1, 0;\n\t&quot;
759:            &quot;sub.u32        r1, r1, t2;\n\t&quot;
760:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
761:            // ret += (uint32_t)(-(ret &gt; ((uint64_t *)t)[0]));
762:            &quot;mov.b64        s, {t0, t1};\n\t&quot;
763:            &quot;set.gt.u32.u64 t2, %0, s;\n\t&quot;
764:            &quot;sub.cc.u32     r0, r0, t2;\n\t&quot;
765:            &quot;subc.u32       r1, r1, 0;\n\t&quot;
766:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
767:            &quot;}&quot;
768:            : &quot;+l&quot;(x.val)
769:            : &quot;r&quot;(l));
770:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
771:        return _module13_(x.val);
772:
773:
774:
775:}
776:
777:
778:
779:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
780:WITHIN_KERNEL INLINE _module3_ _module15_(unsigned long a)
781:{
782:
783:    _module3_ res = {0};
784:    asm(&quot;{\n\t&quot;
785:        &quot;.reg .u32        m;\n\t&quot;
786:        &quot;.reg .u64        t;\n\t&quot;
787:        &quot;mov.u64          %0, %1;\n\t&quot;
788:        &quot;set.ge.u32.u64   m, %0, %2;\n\t&quot;
789:        &quot;mov.b64          t, {m, 0};\n\t&quot;
790:        &quot;add.u64         %0, %0, t;\n\t&quot;
791:        &quot;}&quot;
792:        : &quot;+l&quot;(res.val)
793:        : &quot;l&quot;(a), &quot;l&quot;(18446744069414584321UL));
794:    return res;
795:
796:}
797:
798:
799:
800:
801:/**
802:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
803:* @param[in] l An integer in [0, 32)
804:*/
805:WITHIN_KERNEL INLINE _module3_ _module14_(_module3_ x, unsigned int l)
806:{
807:    /*
808:    Algorithm:
809:
810:    We can decompose the shift as
811:
812:        res = x * 2^l = x * M^k * 2^j,
813:
814:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
815:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
816:
817:    After the multiplication by 2^j, the result contains 3 32-bit parts:
818:
819:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
820:
821:    Thus
822:
823:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
824:
825:    Taking the modulus P = M^2 - M + 1, we get
826:
827:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
828:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
829:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
830:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
831:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
832:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
833:
834:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
835:
836:    The processing for the things inside the parentheses is simpler:
837:
838:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
839:    - (x + y) = PACK(s &lt; y, s), where s = x + y
840:      (that is, check for overflow and add 1 in the high half)
841:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
842:      (that is, check for overflow and add (M-1) in the low half)
843:    */
844:
845:
846:
847:
848:        asm(&quot;{\n\t&quot;
849:            &quot;.reg .u32          r0, r1;\n\t&quot;
850:            &quot;.reg .u32          t0, t1, t2;\n\t&quot;
851:            &quot;.reg .u32          n;\n\t&quot;
852:            &quot;.reg .u64          s;\n\t&quot;
853:            &quot;.reg .pred         p, q;\n\t&quot;
854:            // t[2] = (uint32_t)(x &gt;&gt; (192-l));
855:            // t[1] = (uint32_t)(x &gt;&gt; (160-l));
856:            // t[0] = (uint32_t)(x &lt;&lt; (l-128));
857:            &quot;mov.b64            {r0, r1}, %0;\n\t&quot;
858:            &quot;sub.u32            n, %1, 128;\n\t&quot;
859:            &quot;shl.b32            t0, r0, n;\n\t&quot;
860:            &quot;sub.u32            n, 32, n;\n\t&quot;
861:            &quot;shr.b64            s, %0, n;\n\t&quot;
862:            &quot;mov.b64            {t1, t2}, s;\n\t&quot;
863:            // mod P
864:            &quot;add.u32            r1, t0, t1;\n\t&quot;
865:            &quot;sub.cc.u32         r0, 0, t1;\n\t&quot;
866:            &quot;subc.u32           r1, r1, 0;\n\t&quot;
867:            &quot;sub.cc.u32         r0, r0, t2;\n\t&quot;
868:            &quot;subc.u32           r1, r1, 0;\n\t&quot;
869:            &quot;mov.b64            %0, {r0, r1};\n\t&quot;
870:            // ret -= (uint32_t)(-(ret &gt; ((uint64_t)t[0] &lt;&lt; 32) &amp;&amp; t[1] == 0));
871:            &quot;setp.eq.u32        p|q, t1, 0;\n\t&quot;
872:            &quot;mov.b64            s, {0, t0};\n\t&quot;
873:            &quot;set.gt.and.u32.u64 t2, %0, s, p;\n\t&quot;
874:            &quot;sub.cc.u32         r0, r0, t2;\n\t&quot;
875:            &quot;subc.u32           r1, r1, 0;\n\t&quot;
876:            &quot;mov.b64            %0, {r0, r1};\n\t&quot;
877:            // ret += (uint32_t)(-(ret &lt; ((uint64_t)t[0] &lt;&lt; 32) &amp;&amp; t[1] != 0));
878:            &quot;set.lt.and.u32.u64 t2, %0, s, q;\n\t&quot;
879:            &quot;add.cc.u32         r0, r0, t2;\n\t&quot;
880:            &quot;addc.u32           r1, r1, 0;\n\t&quot;
881:            &quot;mov.b64            %0, {r0, r1};\n\t&quot;
882:            &quot;}&quot;
883:            : &quot;+l&quot;(x.val)
884:            : &quot;r&quot;(l));
885:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
886:        x = _module15_(x.val);
887:        x.val = 18446744069414584321UL - x.val;
888:        return x;
889:
890:
891:
892:}
893:
894:
895:
896:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
897:WITHIN_KERNEL INLINE _module3_ _module17_(unsigned long a)
898:{
899:
900:    _module3_ res = {0};
901:    asm(&quot;{\n\t&quot;
902:        &quot;.reg .u32        m;\n\t&quot;
903:        &quot;.reg .u64        t;\n\t&quot;
904:        &quot;mov.u64          %0, %1;\n\t&quot;
905:        &quot;set.ge.u32.u64   m, %0, %2;\n\t&quot;
906:        &quot;mov.b64          t, {m, 0};\n\t&quot;
907:        &quot;add.u64         %0, %0, t;\n\t&quot;
908:        &quot;}&quot;
909:        : &quot;+l&quot;(res.val)
910:        : &quot;l&quot;(a), &quot;l&quot;(18446744069414584321UL));
911:    return res;
912:
913:}
914:
915:
916:
917:
918:/**
919:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
920:* @param[in] l An integer in [0, 32)
921:*/
922:WITHIN_KERNEL INLINE _module3_ _module16_(_module3_ x, unsigned int l)
923:{
924:    /*
925:    Algorithm:
926:
927:    We can decompose the shift as
928:
929:        res = x * 2^l = x * M^k * 2^j,
930:
931:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
932:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
933:
934:    After the multiplication by 2^j, the result contains 3 32-bit parts:
935:
936:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
937:
938:    Thus
939:
940:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
941:
942:    Taking the modulus P = M^2 - M + 1, we get
943:
944:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
945:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
946:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
947:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
948:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
949:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
950:
951:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
952:
953:    The processing for the things inside the parentheses is simpler:
954:
955:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
956:    - (x + y) = PACK(s &lt; y, s), where s = x + y
957:      (that is, check for overflow and add 1 in the high half)
958:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
959:      (that is, check for overflow and add (M-1) in the low half)
960:    */
961:
962:
963:
964:
965:        asm(&quot;{\n\t&quot;
966:            &quot;.reg .u32      r0, r1;\n\t&quot;
967:            &quot;.reg .u32      t0, t1, t2;\n\t&quot;
968:            &quot;.reg .u32      n;\n\t&quot;
969:            &quot;.reg .u64      s;\n\t&quot;
970:            // t[2] = (uint32_t)(x &gt;&gt; (160-l));
971:            // t[1] = (uint32_t)(x &gt;&gt; (128-l));
972:            // t[0] = (uint32_t)(x &lt;&lt; (l-96));
973:            &quot;mov.b64        {r0, r1}, %0;\n\t&quot;
974:            &quot;sub.u32        n, %1, 96;\n\t&quot;
975:            &quot;shl.b32        t0, r0, n;\n\t&quot;
976:            &quot;sub.u32        n, 32, n;\n\t&quot;
977:            &quot;shr.b64        s, %0, n;\n\t&quot;
978:            &quot;mov.b64        {t1, t2}, s;\n\t&quot;
979:            // mod P
980:            &quot;add.u32        r1, t1, t2;\n\t&quot;
981:            &quot;sub.cc.u32     r0, t0, t2;\n\t&quot;
982:            &quot;subc.u32       r1, r1, 0;\n\t&quot;
983:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
984:            // ret += (uint32_t)(-(ret &lt; ((uint64_t *)t)[0]));
985:            &quot;mov.b64        s, {t0, t1};\n\t&quot;
986:            &quot;set.lt.u32.u64 t2, %0, s;\n\t&quot;
987:            &quot;add.cc.u32     r0, r0, t2;\n\t&quot;
988:            &quot;addc.u32       r1, r1, 0;\n\t&quot;
989:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
990:            &quot;}&quot;
991:            : &quot;+l&quot;(x.val)
992:            : &quot;r&quot;(l));
993:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
994:        x = _module17_(x.val);
995:        x.val = 18446744069414584321UL - x.val;
996:        return x;
997:
998:
999:
1000:}
1001:
1002:
1003:
1004:/** Subtraction in FF(P): val_ = a + b mod P. */
1005:WITHIN_KERNEL INLINE _module3_ _module19_(_module3_ a, _module3_ b)
1006:{
1007:
1008:    _module3_ res = {0};
1009:    asm(&quot;{\n\t&quot;
1010:        &quot;.reg .u32          m;\n\t&quot;
1011:        &quot;.reg .u64          t;\n\t&quot;
1012:        // this = a - b;
1013:        &quot;sub.u64            %0, %1, %2;\n\t&quot;
1014:        // this -= (uint32_t)(-(this &gt; a));
1015:        &quot;set.gt.u32.u64     m, %0, %1;\n\t&quot;
1016:        &quot;mov.b64            t, {m, 0};\n\t&quot;
1017:        &quot;sub.u64            %0, %0, t;\n\t&quot;
1018:        &quot;}&quot;
1019:        : &quot;+l&quot;(res.val)
1020:        : &quot;l&quot;(a.val), &quot;l&quot;(b.val));
1021:    return res;
1022:
1023:}
1024:
1025:
1026:
1027:WITHIN_KERNEL INLINE _module3_ _module18_(_module3_ a, _module3_ b)
1028:{
1029:    /*
1030:    This function performs Montgomery multiplication with the fixed modulus (M=2**64-2**32+1)
1031:    and fixed word size (R=2**64), which helps simplify the algorithm.
1032:    The result is `a * b * R**(-1) mod M`.
1033:
1034:    Note that if you multiply two numbers `a` and `b` in Montgomery representation
1035:    (a&#x27; = a * R mod M, b&#x27; = b * R mod M), the result is the Montgomery representation of their
1036:    product (a&#x27; * b&#x27; * R**(-1) mod M = a * b * R mod M).
1037:    But if one of the numbers is in Montgomery representation and the other is not, you
1038:    get the normal product back (a * b&#x27; * R**(-1) mod M = a * b mod M).
1039:
1040:    This way if one of the factors is precalculated, you can convert it
1041:    into Montgomery representation, and use this function instead of the general
1042:    multiplication function (which is slower).
1043:    */
1044:
1045:
1046:    #ifdef CUDA
1047:    unsigned long hi = __umul64hi(a.val, b.val);
1048:    #else
1049:    unsigned long hi = mul_hi(a.val, b.val);
1050:    #endif
1051:    unsigned long lo = a.val * b.val;
1052:
1053:    unsigned long u = (lo &lt;&lt; 32) + lo;
1054:    unsigned long p2 = u - (u &gt;&gt; 32);
1055:    unsigned long uu = u &lt;&lt; 32;
1056:    if (uu &gt; u)
1057:    {
1058:        p2 -= 1;
1059:    }
1060:
1061:
1062:    _module3_ hi_ff = { hi };
1063:    _module3_ p2_ff = { p2 };
1064:
1065:    return _module19_(hi_ff, p2_ff);
1066:}
1067:
1068:
1069:
1070:
1071:
1072:#define _module2_CDATA_QUALIFIER GLOBAL_MEM_ARG
1073:
1074:
1075:WITHIN_KERNEL INLINE void _module2_swap(_module3_ *a, _module3_ *b)
1076:{
1077:    _module3_ t = *b;
1078:    *b = *a;
1079:    *a = t;
1080:}
1081:
1082:
1083:WITHIN_KERNEL INLINE void _module2_NTT2(_module3_* r0, _module3_* r1)
1084:{
1085:    _module3_ t = _module4_(*r0, *r1);
1086:    *r0 = _module5_(*r0, *r1);
1087:    *r1 = t;
1088:}
1089:
1090:
1091:WITHIN_KERNEL INLINE void _module2_NTT2_pair(_module3_* r)
1092:{
1093:    _module2_NTT2(&amp;r[0], &amp;r[1]);
1094:}
1095:
1096:
1097:WITHIN_KERNEL INLINE void _module2_NTTInv2(_module3_* r0, _module3_* r1)
1098:{
1099:    _module2_NTT2(r0, r1);
1100:}
1101:
1102:
1103:WITHIN_KERNEL INLINE void _module2_NTTInv2_pair(_module3_* r)
1104:{
1105:    _module2_NTT2(&amp;r[0], &amp;r[1]);
1106:}
1107:
1108:
1109:WITHIN_KERNEL INLINE void _module2_NTT8(_module3_* r)
1110:{
1111:    _module2_NTT2(&amp;r[0], &amp;r[4]);
1112:    _module2_NTT2(&amp;r[1], &amp;r[5]);
1113:    _module2_NTT2(&amp;r[2], &amp;r[6]);
1114:    _module2_NTT2(&amp;r[3], &amp;r[7]);
1115:    r[5] = _module6_(r[5], 24);
1116:    r[6] = _module8_(r[6], 48);
1117:    r[7] = _module10_(r[7], 72);
1118:    // instead of calling NTT4 ...
1119:    _module2_NTT2(&amp;r[0], &amp;r[2]);
1120:    _module2_NTT2(&amp;r[1], &amp;r[3]);
1121:    r[3] = _module8_(r[3], 48);
1122:    _module2_NTT2(&amp;r[4], &amp;r[6]);
1123:    _module2_NTT2(&amp;r[5], &amp;r[7]);
1124:    r[7] = _module8_(r[7], 48);
1125:    _module2_NTT2_pair(&amp;r[0]);
1126:    _module2_NTT2_pair(&amp;r[2]);
1127:    _module2_NTT2_pair(&amp;r[4]);
1128:    _module2_NTT2_pair(&amp;r[6]);
1129:    // ... we save 2 swaps (otherwise 4) here
1130:    _module2_swap(&amp;r[1], &amp;r[4]);
1131:    _module2_swap(&amp;r[3], &amp;r[6]);
1132:}
1133:
1134:
1135:WITHIN_KERNEL INLINE void _module2_NTTInv8(_module3_* r)
1136:{
1137:    _module2_NTTInv2(&amp;r[0], &amp;r[4]);
1138:    _module2_NTTInv2(&amp;r[1], &amp;r[5]);
1139:    _module2_NTTInv2(&amp;r[2], &amp;r[6]);
1140:    _module2_NTTInv2(&amp;r[3], &amp;r[7]);
1141:    r[5] = _module12_(r[5], 168);
1142:    r[6] = _module14_(r[6], 144);
1143:    r[7] = _module16_(r[7], 120);
1144:    // instead of calling NTT4 ...
1145:    _module2_NTTInv2(&amp;r[0], &amp;r[2]);
1146:    _module2_NTTInv2(&amp;r[1], &amp;r[3]);
1147:    r[3] = _module14_(r[3], 144);
1148:    _module2_NTTInv2(&amp;r[4], &amp;r[6]);
1149:    _module2_NTTInv2(&amp;r[5], &amp;r[7]);
1150:    r[7] = _module14_(r[7], 144);
1151:    _module2_NTTInv2_pair(&amp;r[0]);
1152:    _module2_NTTInv2_pair(&amp;r[2]);
1153:    _module2_NTTInv2_pair(&amp;r[4]);
1154:    _module2_NTTInv2_pair(&amp;r[6]);
1155:    // ... we save 2 swaps (otherwise 4) here
1156:    _module2_swap(&amp;r[1], &amp;r[4]);
1157:    _module2_swap(&amp;r[3], &amp;r[6]);
1158:}
1159:
1160:
1161:WITHIN_KERNEL INLINE void _module2_NTT8x2Lsh_1(_module3_* s)
1162:{
1163:    s[1] = _module6_(s[1], 12);
1164:    s[2] = _module6_(s[2], 24);
1165:    s[3] = _module8_(s[3], 36);
1166:    s[4] = _module8_(s[4], 48);
1167:    s[5] = _module8_(s[5], 60);
1168:    s[6] = _module10_(s[6], 72);
1169:    s[7] = _module10_(s[7], 84);
1170:}
1171:
1172:
1173:WITHIN_KERNEL INLINE void _module2_NTT8x2Lsh(_module3_* s, unsigned int col)
1174:{
1175:    if (1 == col)
1176:        _module2_NTT8x2Lsh_1(s);
1177:}
1178:
1179:
1180:WITHIN_KERNEL INLINE void _module2_NTTInv8x2Lsh_1(_module3_* s)
1181:{
1182:    s[1] = _module12_(s[1], 180);
1183:    s[2] = _module12_(s[2], 168);
1184:    s[3] = _module14_(s[3], 156);
1185:    s[4] = _module14_(s[4], 144);
1186:    s[5] = _module14_(s[5], 132);
1187:    s[6] = _module16_(s[6], 120);
1188:    s[7] = _module16_(s[7], 108);
1189:}
1190:
1191:
1192:WITHIN_KERNEL INLINE void _module2_NTTInv8x2Lsh(_module3_* s, unsigned int col)
1193:{
1194:    if (1 == col)
1195:        _module2_NTTInv8x2Lsh_1(s);
1196:}
1197:
1198:
1199:WITHIN_KERNEL INLINE void _module2_NTT8x8Lsh_1(_module3_* s)
1200:{
1201:    s[1] = _module6_(s[1], 3);
1202:    s[2] = _module6_(s[2], 6);
1203:    s[3] = _module6_(s[3], 9);
1204:    s[4] = _module6_(s[4], 12);
1205:    s[5] = _module6_(s[5], 15);
1206:    s[6] = _module6_(s[6], 18);
1207:    s[7] = _module6_(s[7], 21);
1208:}
1209:WITHIN_KERNEL INLINE void _module2_NTT8x8Lsh_2(_module3_* s)
1210:{
1211:    s[1] = _module6_(s[1], 6);
1212:    s[2] = _module6_(s[2], 12);
1213:    s[3] = _module6_(s[3], 18);
1214:    s[4] = _module6_(s[4], 24);
1215:    s[5] = _module6_(s[5], 30);
1216:    s[6] = _module8_(s[6], 36);
1217:    s[7] = _module8_(s[7], 42);
1218:}
1219:WITHIN_KERNEL INLINE void _module2_NTT8x8Lsh_3(_module3_* s)
1220:{
1221:    s[1] = _module6_(s[1], 9);
1222:    s[2] = _module6_(s[2], 18);
1223:    s[3] = _module6_(s[3], 27);
1224:    s[4] = _module8_(s[4], 36);
1225:    s[5] = _module8_(s[5], 45);
1226:    s[6] = _module8_(s[6], 54);
1227:    s[7] = _module8_(s[7], 63);
1228:}
1229:WITHIN_KERNEL INLINE void _module2_NTT8x8Lsh_4(_module3_* s)
1230:{
1231:    s[1] = _module6_(s[1], 12);
1232:    s[2] = _module6_(s[2], 24);
1233:    s[3] = _module8_(s[3], 36);
1234:    s[4] = _module8_(s[4], 48);
1235:    s[5] = _module8_(s[5], 60);
1236:    s[6] = _module10_(s[6], 72);
1237:    s[7] = _module10_(s[7], 84);
1238:}
1239:WITHIN_KERNEL INLINE void _module2_NTT8x8Lsh_5(_module3_* s)
1240:{
1241:    s[1] = _module6_(s[1], 15);
1242:    s[2] = _module6_(s[2], 30);
1243:    s[3] = _module8_(s[3], 45);
1244:    s[4] = _module8_(s[4], 60);
1245:    s[5] = _module10_(s[5], 75);
1246:    s[6] = _module10_(s[6], 90);
1247:    s[7] = _module16_(s[7], 105);
1248:}
1249:WITHIN_KERNEL INLINE void _module2_NTT8x8Lsh_6(_module3_* s)
1250:{
1251:    s[1] = _module6_(s[1], 18);
1252:    s[2] = _module8_(s[2], 36);
1253:    s[3] = _module8_(s[3], 54);
1254:    s[4] = _module10_(s[4], 72);
1255:    s[5] = _module10_(s[5], 90);
1256:    s[6] = _module16_(s[6], 108);
1257:    s[7] = _module16_(s[7], 126);
1258:}
1259:WITHIN_KERNEL INLINE void _module2_NTT8x8Lsh_7(_module3_* s)
1260:{
1261:    s[1] = _module6_(s[1], 21);
1262:    s[2] = _module8_(s[2], 42);
1263:    s[3] = _module8_(s[3], 63);
1264:    s[4] = _module10_(s[4], 84);
1265:    s[5] = _module16_(s[5], 105);
1266:    s[6] = _module16_(s[6], 126);
1267:    s[7] = _module14_(s[7], 147);
1268:}
1269:
1270:
1271:WITHIN_KERNEL INLINE void _module2_NTT8x8Lsh(_module3_* s, unsigned int col)
1272:{
1273:    if (1 == col)
1274:        _module2_NTT8x8Lsh_1(s);
1275:    else if (2 == col)
1276:        _module2_NTT8x8Lsh_2(s);
1277:    else if (3 == col)
1278:        _module2_NTT8x8Lsh_3(s);
1279:    else if (4 == col)
1280:        _module2_NTT8x8Lsh_4(s);
1281:    else if (5 == col)
1282:        _module2_NTT8x8Lsh_5(s);
1283:    else if (6 == col)
1284:        _module2_NTT8x8Lsh_6(s);
1285:    else if (7 == col)
1286:        _module2_NTT8x8Lsh_7(s);
1287:}
1288:
1289:
1290:WITHIN_KERNEL INLINE void _module2_NTTInv8x8Lsh_1(_module3_* s)
1291:{
1292:    s[1] = _module12_(s[1], 189);
1293:    s[2] = _module12_(s[2], 186);
1294:    s[3] = _module12_(s[3], 183);
1295:    s[4] = _module12_(s[4], 180);
1296:    s[5] = _module12_(s[5], 177);
1297:    s[6] = _module12_(s[6], 174);
1298:    s[7] = _module12_(s[7], 171);
1299:}
1300:WITHIN_KERNEL INLINE void _module2_NTTInv8x8Lsh_2(_module3_* s)
1301:{
1302:    s[1] = _module12_(s[1], 186);
1303:    s[2] = _module12_(s[2], 180);
1304:    s[3] = _module12_(s[3], 174);
1305:    s[4] = _module12_(s[4], 168);
1306:    s[5] = _module12_(s[5], 162);
1307:    s[6] = _module14_(s[6], 156);
1308:    s[7] = _module14_(s[7], 150);
1309:}
1310:WITHIN_KERNEL INLINE void _module2_NTTInv8x8Lsh_3(_module3_* s)
1311:{
1312:    s[1] = _module12_(s[1], 183);
1313:    s[2] = _module12_(s[2], 174);
1314:    s[3] = _module12_(s[3], 165);
1315:    s[4] = _module14_(s[4], 156);
1316:    s[5] = _module14_(s[5], 147);
1317:    s[6] = _module14_(s[6], 138);
1318:    s[7] = _module14_(s[7], 129);
1319:}
1320:WITHIN_KERNEL INLINE void _module2_NTTInv8x8Lsh_4(_module3_* s)
1321:{
1322:    s[1] = _module12_(s[1], 180);
1323:    s[2] = _module12_(s[2], 168);
1324:    s[3] = _module14_(s[3], 156);
1325:    s[4] = _module14_(s[4], 144);
1326:    s[5] = _module14_(s[5], 132);
1327:    s[6] = _module16_(s[6], 120);
1328:    s[7] = _module16_(s[7], 108);
1329:}
1330:WITHIN_KERNEL INLINE void _module2_NTTInv8x8Lsh_5(_module3_* s)
1331:{
1332:    s[1] = _module12_(s[1], 177);
1333:    s[2] = _module12_(s[2], 162);
1334:    s[3] = _module14_(s[3], 147);
1335:    s[4] = _module14_(s[4], 132);
1336:    s[5] = _module16_(s[5], 117);
1337:    s[6] = _module16_(s[6], 102);
1338:    s[7] = _module10_(s[7], 87);
1339:}
1340:WITHIN_KERNEL INLINE void _module2_NTTInv8x8Lsh_6(_module3_* s)
1341:{
1342:    s[1] = _module12_(s[1], 174);
1343:    s[2] = _module14_(s[2], 156);
1344:    s[3] = _module14_(s[3], 138);
1345:    s[4] = _module16_(s[4], 120);
1346:    s[5] = _module16_(s[5], 102);
1347:    s[6] = _module10_(s[6], 84);
1348:    s[7] = _module10_(s[7], 66);
1349:}
1350:WITHIN_KERNEL INLINE void _module2_NTTInv8x8Lsh_7(_module3_* s)
1351:{
1352:    s[1] = _module12_(s[1], 171);
1353:    s[2] = _module14_(s[2], 150);
1354:    s[3] = _module14_(s[3], 129);
1355:    s[4] = _module16_(s[4], 108);
1356:    s[5] = _module10_(s[5], 87);
1357:    s[6] = _module10_(s[6], 66);
1358:    s[7] = _module8_(s[7], 45);
1359:}
1360:
1361:
1362:WITHIN_KERNEL INLINE void _module2_NTTInv8x8Lsh(_module3_* s, unsigned int col)
1363:{
1364:    if (1 == col)
1365:        _module2_NTTInv8x8Lsh_1(s);
1366:    else if (2 == col)
1367:        _module2_NTTInv8x8Lsh_2(s);
1368:    else if (3 == col)
1369:        _module2_NTTInv8x8Lsh_3(s);
1370:    else if (4 == col)
1371:        _module2_NTTInv8x8Lsh_4(s);
1372:    else if (5 == col)
1373:        _module2_NTTInv8x8Lsh_5(s);
1374:    else if (6 == col)
1375:        _module2_NTTInv8x8Lsh_6(s);
1376:    else if (7 == col)
1377:        _module2_NTTInv8x8Lsh_7(s);
1378:}
1379:
1380:
1381:WITHIN_KERNEL INLINE void _module2_Index3DFrom1D(uint3 *t3d, unsigned int t1d, unsigned int dim_x, unsigned int dim_y, unsigned int dim_z)
1382:{
1383:    t3d-&gt;x = t1d % dim_x;
1384:    t1d /= dim_x;
1385:    t3d-&gt;y = t1d % dim_y;
1386:    t3d-&gt;z = t1d / dim_y;
1387:}
1388:
1389:
1390:WITHIN_KERNEL INLINE void _module2__forward(
1391:        _module3_* r,
1392:        LOCAL_MEM_ARG _module3_* s,
1393:        _module2_CDATA_QUALIFIER _module3_* twd,
1394:        const unsigned int t1d)
1395:{
1396:    uint3 t3d;
1397:    _module2_Index3DFrom1D(&amp;t3d, t1d, 8, 8, 2);
1398:
1399:    LOCAL_MEM_ARG _module3_* ptr;
1400:
1401:    _module2_NTT8(r);
1402:    _module2_NTT8x2Lsh(r, t3d.z);
1403:    ptr = &amp;s[(t3d.y &lt;&lt; 7) | (t3d.z &lt;&lt; 6) | (t3d.x &lt;&lt; 2)];
1404:    #pragma unroll
1405:    for (unsigned int i = 0; i &lt; 8; i ++)
1406:        ptr[(i &gt;&gt; 2 &lt;&lt; 5) | (i &amp; 0x3)] = r[i];
1407:    LOCAL_BARRIER;
1408:
1409:    ptr = &amp;s[(t3d.z &lt;&lt; 9) | (t3d.y &lt;&lt; 3) | t3d.x];
1410:    #pragma unroll
1411:    for (unsigned int i = 0; i &lt; 8; i ++)
1412:        r[i] = ptr[i &lt;&lt; 6];
1413:    _module2_NTT2_pair(r);
1414:    _module2_NTT2_pair(r + 2);
1415:    _module2_NTT2_pair(r + 4);
1416:    _module2_NTT2_pair(r + 6);
1417:    #pragma unroll
1418:    for (unsigned int i = 0; i &lt; 8; i ++)
1419:        ptr[i &lt;&lt; 6] = r[i];
1420:    LOCAL_BARRIER;
1421:
1422:    ptr = &amp;s[t1d];
1423:    #pragma unroll
1424:    for (unsigned int i = 0; i &lt; 8; i ++)
1425:        r[i] = _module18_(ptr[i &lt;&lt; 7], twd[i &lt;&lt; 7 | t1d]); // mult twiddle
1426:    _module2_NTT8(r);
1427:    #pragma unroll
1428:    for (unsigned int i = 0; i &lt; 8; i ++)
1429:        ptr[i &lt;&lt; 7] = r[i];
1430:    LOCAL_BARRIER;
1431:
1432:    ptr = &amp;s[(t1d &gt;&gt; 2 &lt;&lt; 5) | (t3d.x &amp; 0x3)];
1433:    #pragma unroll
1434:    for (unsigned int i = 0; i &lt; 8; i ++)
1435:        r[i] = ptr[i &lt;&lt; 2];
1436:    _module2_NTT8x8Lsh(r, t1d &gt;&gt; 4); // less divergence if put here!
1437:    _module2_NTT8(r);
1438:}
1439:
1440:
1441:WITHIN_KERNEL INLINE void _module2__inverse(
1442:        _module3_* r,
1443:        LOCAL_MEM_ARG _module3_* s,
1444:        _module2_CDATA_QUALIFIER _module3_* twd,
1445:        const unsigned int t1d)
1446:{
1447:    uint3 t3d;
1448:    _module2_Index3DFrom1D(&amp;t3d, t1d, 8, 8, 2);
1449:
1450:    LOCAL_MEM_ARG _module3_* ptr;
1451:
1452:    _module2_NTTInv8(r);
1453:    _module2_NTTInv8x2Lsh(r, t3d.z);
1454:    ptr = &amp;s[(t3d.y &lt;&lt; 7) | (t3d.z &lt;&lt; 6) | (t3d.x &lt;&lt; 2)];
1455:    #pragma unroll
1456:    for (unsigned int i = 0; i &lt; 8; i ++)
1457:        ptr[(i &gt;&gt; 2 &lt;&lt; 5) | (i &amp; 0x3)] = r[i];
1458:    LOCAL_BARRIER;
1459:
1460:    ptr = &amp;s[(t3d.z &lt;&lt; 9) | (t3d.y &lt;&lt; 3) | t3d.x];
1461:    #pragma unroll
1462:    for (unsigned int i = 0; i &lt; 8; i ++)
1463:        r[i] = ptr[i &lt;&lt; 6];
1464:    _module2_NTT2_pair(r);
1465:    _module2_NTT2_pair(r + 2);
1466:    _module2_NTT2_pair(r + 4);
1467:    _module2_NTT2_pair(r + 6);
1468:    #pragma unroll
1469:    for (unsigned int i = 0; i &lt; 8; i ++)
1470:        ptr[i &lt;&lt; 6] = r[i];
1471:    LOCAL_BARRIER;
1472:
1473:    ptr = &amp;s[t1d];
1474:    #pragma unroll
1475:    for (unsigned int i = 0; i &lt; 8; i ++)
1476:        r[i] = _module18_(ptr[i &lt;&lt; 7], twd[i &lt;&lt; 7 | t1d]); // mult twiddle
1477:    _module2_NTTInv8(r);
1478:    #pragma unroll
1479:    for (unsigned int i = 0; i &lt; 8; i ++)
1480:        ptr[i &lt;&lt; 7] = r[i];
1481:    LOCAL_BARRIER;
1482:
1483:    ptr = &amp;s[(t1d &gt;&gt; 2 &lt;&lt; 5) | (t3d.x &amp; 0x3)];
1484:    #pragma unroll
1485:        for (unsigned int i = 0; i &lt; 8; i ++)
1486:    r[i] = ptr[i &lt;&lt; 2];
1487:    _module2_NTTInv8x8Lsh(r, t1d &gt;&gt; 4); // less divergence if put here!
1488:    _module2_NTTInv8(r);
1489:}
1490:
1491:
1492:WITHIN_KERNEL INLINE void _module2_forward(
1493:        _module3_* r_out,
1494:        _module3_* r_in,
1495:        LOCAL_MEM_ARG _module3_* temp,
1496:        _module2_CDATA_QUALIFIER _module3_* cdata,
1497:        unsigned int thread_in_xform)
1498:{
1499:    // Preprocess
1500:    r_out[0] = _module18_(
1501:        r_in[0],
1502:        cdata[1024 + 0 + thread_in_xform]
1503:        );
1504:    r_out[1] = _module18_(
1505:        r_in[1],
1506:        cdata[1024 + 128 + thread_in_xform]
1507:        );
1508:    r_out[2] = _module18_(
1509:        r_in[2],
1510:        cdata[1024 + 256 + thread_in_xform]
1511:        );
1512:    r_out[3] = _module18_(
1513:        r_in[3],
1514:        cdata[1024 + 384 + thread_in_xform]
1515:        );
1516:    r_out[4] = _module18_(
1517:        r_in[4],
1518:        cdata[1024 + 512 + thread_in_xform]
1519:        );
1520:    r_out[5] = _module18_(
1521:        r_in[5],
1522:        cdata[1024 + 640 + thread_in_xform]
1523:        );
1524:    r_out[6] = _module18_(
1525:        r_in[6],
1526:        cdata[1024 + 768 + thread_in_xform]
1527:        );
1528:    r_out[7] = _module18_(
1529:        r_in[7],
1530:        cdata[1024 + 896 + thread_in_xform]
1531:        );
1532:
1533:    _module2__forward(r_out, temp, cdata, thread_in_xform);
1534:}
1535:
1536:
1537:WITHIN_KERNEL INLINE void _module2_inverse(
1538:        _module3_* r_out,
1539:        _module3_* r_in,
1540:        LOCAL_MEM_ARG _module3_* temp,
1541:        _module2_CDATA_QUALIFIER _module3_* cdata,
1542:        unsigned int thread_in_xform)
1543:{
1544:    _module2__inverse(r_in, temp, cdata, thread_in_xform);
1545:
1546:    // Postprocess
1547:    r_out[0] = _module18_(
1548:        r_in[0],
1549:        cdata[1024 + 0 + thread_in_xform]
1550:        );
1551:    r_out[1] = _module18_(
1552:        r_in[1],
1553:        cdata[1024 + 128 + thread_in_xform]
1554:        );
1555:    r_out[2] = _module18_(
1556:        r_in[2],
1557:        cdata[1024 + 256 + thread_in_xform]
1558:        );
1559:    r_out[3] = _module18_(
1560:        r_in[3],
1561:        cdata[1024 + 384 + thread_in_xform]
1562:        );
1563:    r_out[4] = _module18_(
1564:        r_in[4],
1565:        cdata[1024 + 512 + thread_in_xform]
1566:        );
1567:    r_out[5] = _module18_(
1568:        r_in[5],
1569:        cdata[1024 + 640 + thread_in_xform]
1570:        );
1571:    r_out[6] = _module18_(
1572:        r_in[6],
1573:        cdata[1024 + 768 + thread_in_xform]
1574:        );
1575:    r_out[7] = _module18_(
1576:        r_in[7],
1577:        cdata[1024 + 896 + thread_in_xform]
1578:        );
1579:}
1580:
1581:
1582:WITHIN_KERNEL INLINE _module3_ _module2_i32_to_elem(int x)
1583:{
1584:    _module3_ res = { (unsigned long)x - (unsigned int)(-(x &lt; 0)) };
1585:    return res;
1586:}
1587:
1588:
1589:WITHIN_KERNEL INLINE int _module2_ff_to_i32(_module3_ x)
1590:{
1591:    // Interpreting anything &gt; P/2 as a negative integer,
1592:    // then taking modulo 2^31
1593:    const unsigned long med = 18446744069414584321UL / 2;
1594:    return (int)(x.val) - (x.val &gt; med);
1595:}
1596:
1597:
1598:
1599:WITHIN_KERNEL INLINE void _module2_forward_i32(
1600:        _module3_* r_out,
1601:        int* r_in,
1602:        LOCAL_MEM_ARG _module3_* temp,
1603:        _module2_CDATA_QUALIFIER _module3_* cdata,
1604:        unsigned int thread_in_xform)
1605:{
1606:    r_out[0] = _module2_i32_to_elem(r_in[0]);
1607:    r_out[1] = _module2_i32_to_elem(r_in[1]);
1608:    r_out[2] = _module2_i32_to_elem(r_in[2]);
1609:    r_out[3] = _module2_i32_to_elem(r_in[3]);
1610:    r_out[4] = _module2_i32_to_elem(r_in[4]);
1611:    r_out[5] = _module2_i32_to_elem(r_in[5]);
1612:    r_out[6] = _module2_i32_to_elem(r_in[6]);
1613:    r_out[7] = _module2_i32_to_elem(r_in[7]);
1614:    _module2_forward(r_out, r_out, temp, cdata, thread_in_xform);
1615:}
1616:
1617:
1618:WITHIN_KERNEL INLINE void _module2_inverse_i32(
1619:        int* r_out,
1620:        _module3_* r_in,
1621:        LOCAL_MEM_ARG _module3_* temp,
1622:        _module2_CDATA_QUALIFIER _module3_* cdata,
1623:        unsigned int thread_in_xform)
1624:{
1625:    _module2_inverse(r_in, r_in, temp, cdata, thread_in_xform);
1626:    r_out[0] = _module2_ff_to_i32(r_in[0]);
1627:    r_out[1] = _module2_ff_to_i32(r_in[1]);
1628:    r_out[2] = _module2_ff_to_i32(r_in[2]);
1629:    r_out[3] = _module2_ff_to_i32(r_in[3]);
1630:    r_out[4] = _module2_ff_to_i32(r_in[4]);
1631:    r_out[5] = _module2_ff_to_i32(r_in[5]);
1632:    r_out[6] = _module2_ff_to_i32(r_in[6]);
1633:    r_out[7] = _module2_ff_to_i32(r_in[7]);
1634:}
1635:
1636:
1637:WITHIN_KERNEL INLINE void _module2_noop()
1638:{
1639:    LOCAL_BARRIER;
1640:    LOCAL_BARRIER;
1641:    LOCAL_BARRIER;
1642:}
1643:
1644:
1645:WITHIN_KERNEL INLINE void _module2_forward_i32_shared(
1646:        LOCAL_MEM_ARG _module3_* in_out,
1647:        LOCAL_MEM_ARG _module3_* temp,
1648:        _module2_CDATA_QUALIFIER _module3_* cdata,
1649:        unsigned int thread_in_xform)
1650:{
1651:    _module3_ r[8];
1652:    r[0] = in_out[0 + thread_in_xform];
1653:    r[1] = in_out[128 + thread_in_xform];
1654:    r[2] = in_out[256 + thread_in_xform];
1655:    r[3] = in_out[384 + thread_in_xform];
1656:    r[4] = in_out[512 + thread_in_xform];
1657:    r[5] = in_out[640 + thread_in_xform];
1658:    r[6] = in_out[768 + thread_in_xform];
1659:    r[7] = in_out[896 + thread_in_xform];
1660:    LOCAL_BARRIER;
1661:    _module2_forward(r, r, temp, cdata, thread_in_xform);
1662:    LOCAL_BARRIER;
1663:    in_out[0 + thread_in_xform] = r[0];
1664:    in_out[128 + thread_in_xform] = r[1];
1665:    in_out[256 + thread_in_xform] = r[2];
1666:    in_out[384 + thread_in_xform] = r[3];
1667:    in_out[512 + thread_in_xform] = r[4];
1668:    in_out[640 + thread_in_xform] = r[5];
1669:    in_out[768 + thread_in_xform] = r[6];
1670:    in_out[896 + thread_in_xform] = r[7];
1671:}
1672:
1673:
1674:WITHIN_KERNEL INLINE void _module2_inverse_i32_shared_add(
1675:        LOCAL_MEM_ARG int* out,
1676:        LOCAL_MEM_ARG _module3_* in,
1677:        LOCAL_MEM_ARG _module3_* temp,
1678:        _module2_CDATA_QUALIFIER _module3_* cdata,
1679:        unsigned int thread_in_xform)
1680:{
1681:    _module3_ r[8];
1682:    r[0] = in[0 + thread_in_xform];
1683:    r[1] = in[128 + thread_in_xform];
1684:    r[2] = in[256 + thread_in_xform];
1685:    r[3] = in[384 + thread_in_xform];
1686:    r[4] = in[512 + thread_in_xform];
1687:    r[5] = in[640 + thread_in_xform];
1688:    r[6] = in[768 + thread_in_xform];
1689:    r[7] = in[896 + thread_in_xform];
1690:    LOCAL_BARRIER;
1691:    _module2_inverse(r, r, temp, cdata, thread_in_xform);
1692:    LOCAL_BARRIER;
1693:    out[0 + thread_in_xform] += _module2_ff_to_i32(r[0]);
1694:    out[128 + thread_in_xform] += _module2_ff_to_i32(r[1]);
1695:    out[256 + thread_in_xform] += _module2_ff_to_i32(r[2]);
1696:    out[384 + thread_in_xform] += _module2_ff_to_i32(r[3]);
1697:    out[512 + thread_in_xform] += _module2_ff_to_i32(r[4]);
1698:    out[640 + thread_in_xform] += _module2_ff_to_i32(r[5]);
1699:    out[768 + thread_in_xform] += _module2_ff_to_i32(r[6]);
1700:    out[896 + thread_in_xform] += _module2_ff_to_i32(r[7]);
1701:}
1702:
1703:
1704:WITHIN_KERNEL INLINE void _module2_noop_shared()
1705:{
1706:    LOCAL_BARRIER;
1707:    _module2_noop();
1708:    LOCAL_BARRIER;
1709:}
1710:
1711:
1712:
1713:
1714:
1715:    // leaf output macro for &quot;output&quot;
1716:    #define _module21_(_idx0, _idx1, _val) _leaf_output[(_idx0) * (1024) + (_idx1) * (1) + (0)] = (_val)
1717:    
1718:
1719:
1720:
1721:
1722:    
1723:    INLINE WITHIN_KERNEL void _module20_func(
1724:        GLOBAL_MEM unsigned long *_leaf_output, VSIZE_T _c_idx0, VSIZE_T _c_idx1, unsigned long _val)
1725:    {
1726:        
1727:
1728:    
1729:        
1730:        VSIZE_T _idx0 = _c_idx0 / 1;
1731:    
1732:        
1733:        VSIZE_T _idx1 = _c_idx1 / 1;
1734:    
1735:
1736:
1737:        _module21_(_idx0, _idx1, _val);
1738:    }
1739:    
1740:    #define _module20_(_c_idx0, _c_idx1, _val) _module20_func(        _leaf_output, _c_idx0, _c_idx1, _val)
1741:    
1742:
1743:
1744:
1745:
1746:
1747:
1748:
1749:KERNEL void standalone_transform(GLOBAL_MEM unsigned long *_leaf_output, GLOBAL_MEM unsigned long *_leaf_input, GLOBAL_MEM unsigned long *_leaf__value1)
1750:
1751:{
1752:    VIRTUAL_SKIP_THREADS;
1753:
1754:    LOCAL_MEM unsigned long temp[1024 * 2];
1755:
1756:    unsigned long r_in[8];
1757:    unsigned long r_out[8];
1758:
1759:    VSIZE_T batch_id = virtual_group_id(0);
1760:    VSIZE_T tid = virtual_local_id(1);
1761:    int transform_in_block = tid / 128;
1762:    int thread_in_transform = tid % 128;
1763:
1764:
1765:    #pragma unroll
1766:    for (int i = 0; i &lt; 8; i++)
1767:    {
1768:        r_in[i] = _module0_(
1769:            batch_id * 2 + transform_in_block, thread_in_transform + i * 128);
1770:    }
1771:
1772:
1773:        LOCAL_MEM_ARG unsigned long* transform_temp =
1774:            temp + 1024 * transform_in_block;
1775:                _module2_forward(
1776:            (_module3_*)r_out,
1777:            (_module3_*)r_in,
1778:            (LOCAL_MEM_ARG _module3_*)transform_temp,
1779:            (_module2_CDATA_QUALIFIER _module3_*)_leaf__value1,
1780:            thread_in_transform);
1781:
1782:
1783:    #pragma unroll
1784:    for (int i = 0; i &lt; 8; i++)
1785:    {
1786:        _module20_(
1787:            batch_id * 2 + transform_in_block, thread_in_transform + i * 128,
1788:            r_out[i]);
1789:    }
1790:}
1791:
1792:<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_ntt_performance[cuda:0:0-global_mem-4]</td>
          <td class="col-duration">0.36</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f8d55de80&gt;, transforms_per_block = 4, constant_memory = False, heavy_performance_load = False<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(&#x27;transforms_per_block&#x27;, [1, 2, 4])<br/>    @pytest.mark.parametrize(&#x27;constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_ntt_performance(thread, transforms_per_block, constant_memory, heavy_performance_load):<br/>    <br/>        if not transform_supported(thread.device_params, &#x27;NTT&#x27;):<br/>            pytest.skip()<br/>    <br/>        if transforms_per_block &gt; max_supported_transforms_per_block(thread.device_params, &#x27;NTT&#x27;):<br/>            pytest.skip()<br/>    <br/>        is_cuda = thread.api.get_id() == cuda_id()<br/>    <br/>        methods = list(itertools.product(<br/>            [&#x27;cuda_asm&#x27;, &#x27;c&#x27;], # base method<br/>            [&#x27;cuda_asm&#x27;, &#x27;c_from_asm&#x27;, &#x27;c&#x27;], # mul method<br/>            [&#x27;cuda_asm&#x27;, &#x27;c_from_asm&#x27;, &#x27;c&#x27;] # lsh method<br/>            ))<br/>    <br/>        if not is_cuda:<br/>            # filter out all usage of CUDA asm if we&#x27;re on OpenCL<br/>            methods = [ms for ms in methods if &#x27;cuda_asm&#x27; not in ms]<br/>    <br/>        batch_shape = (2**14,)<br/>        a = get_test_array(batch_shape + (1024,), &quot;ff_number&quot;)<br/>    <br/>        kernel_repetitions = 100 if heavy_performance_load else 5<br/>    <br/>&gt;       a_dev = thread.to_device(a)<br/><br/>test/test_transform/test_computation.py:173: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f8d55de80&gt;, shape = (16384, 1024), dtype = dtype(&#x27;uint64&#x27;), strides = (8192, 8), offset = 0, nbytes = 134217728<br/>allocator = &lt;Boost.Python.function object at 0x37fa4390&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_ntt_performance[cuda:0:0-constant_mem-1]</td>
          <td class="col-duration">0.36</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f8d55de80&gt;, transforms_per_block = 1, constant_memory = True, heavy_performance_load = False<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(&#x27;transforms_per_block&#x27;, [1, 2, 4])<br/>    @pytest.mark.parametrize(&#x27;constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_ntt_performance(thread, transforms_per_block, constant_memory, heavy_performance_load):<br/>    <br/>        if not transform_supported(thread.device_params, &#x27;NTT&#x27;):<br/>            pytest.skip()<br/>    <br/>        if transforms_per_block &gt; max_supported_transforms_per_block(thread.device_params, &#x27;NTT&#x27;):<br/>            pytest.skip()<br/>    <br/>        is_cuda = thread.api.get_id() == cuda_id()<br/>    <br/>        methods = list(itertools.product(<br/>            [&#x27;cuda_asm&#x27;, &#x27;c&#x27;], # base method<br/>            [&#x27;cuda_asm&#x27;, &#x27;c_from_asm&#x27;, &#x27;c&#x27;], # mul method<br/>            [&#x27;cuda_asm&#x27;, &#x27;c_from_asm&#x27;, &#x27;c&#x27;] # lsh method<br/>            ))<br/>    <br/>        if not is_cuda:<br/>            # filter out all usage of CUDA asm if we&#x27;re on OpenCL<br/>            methods = [ms for ms in methods if &#x27;cuda_asm&#x27; not in ms]<br/>    <br/>        batch_shape = (2**14,)<br/>        a = get_test_array(batch_shape + (1024,), &quot;ff_number&quot;)<br/>    <br/>        kernel_repetitions = 100 if heavy_performance_load else 5<br/>    <br/>&gt;       a_dev = thread.to_device(a)<br/><br/>test/test_transform/test_computation.py:173: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f8d55de80&gt;, shape = (16384, 1024), dtype = dtype(&#x27;uint64&#x27;), strides = (8192, 8), offset = 0, nbytes = 134217728<br/>allocator = &lt;Boost.Python.function object at 0x37fa4390&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_ntt_performance[cuda:0:0-constant_mem-2]</td>
          <td class="col-duration">0.36</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f8d55de80&gt;, transforms_per_block = 2, constant_memory = True, heavy_performance_load = False<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(&#x27;transforms_per_block&#x27;, [1, 2, 4])<br/>    @pytest.mark.parametrize(&#x27;constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_ntt_performance(thread, transforms_per_block, constant_memory, heavy_performance_load):<br/>    <br/>        if not transform_supported(thread.device_params, &#x27;NTT&#x27;):<br/>            pytest.skip()<br/>    <br/>        if transforms_per_block &gt; max_supported_transforms_per_block(thread.device_params, &#x27;NTT&#x27;):<br/>            pytest.skip()<br/>    <br/>        is_cuda = thread.api.get_id() == cuda_id()<br/>    <br/>        methods = list(itertools.product(<br/>            [&#x27;cuda_asm&#x27;, &#x27;c&#x27;], # base method<br/>            [&#x27;cuda_asm&#x27;, &#x27;c_from_asm&#x27;, &#x27;c&#x27;], # mul method<br/>            [&#x27;cuda_asm&#x27;, &#x27;c_from_asm&#x27;, &#x27;c&#x27;] # lsh method<br/>            ))<br/>    <br/>        if not is_cuda:<br/>            # filter out all usage of CUDA asm if we&#x27;re on OpenCL<br/>            methods = [ms for ms in methods if &#x27;cuda_asm&#x27; not in ms]<br/>    <br/>        batch_shape = (2**14,)<br/>        a = get_test_array(batch_shape + (1024,), &quot;ff_number&quot;)<br/>    <br/>        kernel_repetitions = 100 if heavy_performance_load else 5<br/>    <br/>&gt;       a_dev = thread.to_device(a)<br/><br/>test/test_transform/test_computation.py:173: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f8d55de80&gt;, shape = (16384, 1024), dtype = dtype(&#x27;uint64&#x27;), strides = (8192, 8), offset = 0, nbytes = 134217728<br/>allocator = &lt;Boost.Python.function object at 0x37fa4390&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_ntt_performance[cuda:0:0-constant_mem-4]</td>
          <td class="col-duration">0.36</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f8d55de80&gt;, transforms_per_block = 4, constant_memory = True, heavy_performance_load = False<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(&#x27;transforms_per_block&#x27;, [1, 2, 4])<br/>    @pytest.mark.parametrize(&#x27;constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_ntt_performance(thread, transforms_per_block, constant_memory, heavy_performance_load):<br/>    <br/>        if not transform_supported(thread.device_params, &#x27;NTT&#x27;):<br/>            pytest.skip()<br/>    <br/>        if transforms_per_block &gt; max_supported_transforms_per_block(thread.device_params, &#x27;NTT&#x27;):<br/>            pytest.skip()<br/>    <br/>        is_cuda = thread.api.get_id() == cuda_id()<br/>    <br/>        methods = list(itertools.product(<br/>            [&#x27;cuda_asm&#x27;, &#x27;c&#x27;], # base method<br/>            [&#x27;cuda_asm&#x27;, &#x27;c_from_asm&#x27;, &#x27;c&#x27;], # mul method<br/>            [&#x27;cuda_asm&#x27;, &#x27;c_from_asm&#x27;, &#x27;c&#x27;] # lsh method<br/>            ))<br/>    <br/>        if not is_cuda:<br/>            # filter out all usage of CUDA asm if we&#x27;re on OpenCL<br/>            methods = [ms for ms in methods if &#x27;cuda_asm&#x27; not in ms]<br/>    <br/>        batch_shape = (2**14,)<br/>        a = get_test_array(batch_shape + (1024,), &quot;ff_number&quot;)<br/>    <br/>        kernel_repetitions = 100 if heavy_performance_load else 5<br/>    <br/>&gt;       a_dev = thread.to_device(a)<br/><br/>test/test_transform/test_computation.py:173: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f8d55de80&gt;, shape = (16384, 1024), dtype = dtype(&#x27;uint64&#x27;), strides = (8192, 8), offset = 0, nbytes = 134217728<br/>allocator = &lt;Boost.Python.function object at 0x37fa4390&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="skipped results-table-row">
        <tr>
          <td class="col-result">Skipped</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_add_perf[c_from_asm-cuda:0:0]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">(&#x27;/home/mwang/src/nucypher/nufhe/test/test_transform/test_arithmetic.py&#x27;, 304, &#x27;Skipped: &lt;Skipped instance&gt;&#x27;)<br/></div></td></tr></tbody>
      <tbody class="skipped results-table-row">
        <tr>
          <td class="col-result">Skipped</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_sub_perf[c_from_asm-cuda:0:0]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">(&#x27;/home/mwang/src/nucypher/nufhe/test/test_transform/test_arithmetic.py&#x27;, 317, &#x27;Skipped: &lt;Skipped instance&gt;&#x27;)<br/></div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_add[c-cuda:0:0]</td>
          <td class="col-duration">0.08</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_add[cuda_asm-cuda:0:0]</td>
          <td class="col-duration">0.05</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_add[c_from_asm-cuda:0:0]</td>
          <td class="col-duration">0.05</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_mod[c-cuda:0:0]</td>
          <td class="col-duration">0.03</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_mod[cuda_asm-cuda:0:0]</td>
          <td class="col-duration">0.03</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_mod[c_from_asm-cuda:0:0]</td>
          <td class="col-duration">0.03</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_mul_prepared[c-cuda:0:0]</td>
          <td class="col-duration">0.05</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_mul_prepared[cuda_asm-cuda:0:0]</td>
          <td class="col-duration">0.05</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_mul_prepared[c_from_asm-cuda:0:0]</td>
          <td class="col-duration">0.05</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_pow[cuda:0:0]</td>
          <td class="col-duration">0.30</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_lsh[c-cuda:0:0-32]</td>
          <td class="col-duration">0.08</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_lsh[c-cuda:0:0-64]</td>
          <td class="col-duration">0.09</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_lsh[c-cuda:0:0-96]</td>
          <td class="col-duration">0.10</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_lsh[c-cuda:0:0-128]</td>
          <td class="col-duration">0.11</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_lsh[c-cuda:0:0-160]</td>
          <td class="col-duration">0.11</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_lsh[c-cuda:0:0-192]</td>
          <td class="col-duration">0.11</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_lsh[cuda_asm-cuda:0:0-32]</td>
          <td class="col-duration">0.08</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_lsh[cuda_asm-cuda:0:0-64]</td>
          <td class="col-duration">0.09</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_lsh[cuda_asm-cuda:0:0-96]</td>
          <td class="col-duration">0.10</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_lsh[cuda_asm-cuda:0:0-128]</td>
          <td class="col-duration">0.11</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_lsh[cuda_asm-cuda:0:0-160]</td>
          <td class="col-duration">0.19</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log"> ------------------------------Captured stderr call------------------------------ <br/>Exception ignored in: &lt;bound method Thread.__del__ of &lt;reikna.cluda.cuda.Thread object at 0x7fadb5bf98&gt;&gt;
Traceback (most recent call last):
  File &quot;/home/mwang/envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py&quot;, line 247, in __del__
    self.release()
  File &quot;/home/mwang/envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py&quot;, line 597, in release
    self._release_specific()
  File &quot;/home/mwang/envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py&quot;, line 242, in _release_specific
    cuda.Context.pop()
pycuda._driver.LogicError: context::pop failed: invalid device context - cannot pop non-current context
<br/></div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_transform_correctness[cuda:0:0-NTT-global_mem-no_conversion-forward]</td>
          <td class="col-duration">13.48</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_ntt_performance[cuda:0:0-global_mem-1]</td>
          <td class="col-duration">93.15</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log"> ------------------------------Captured stdout call------------------------------ <br/>
  base: cuda_asm, mul: cuda_asm, lsh: cuda_asm
  cuda, 1 per block, test --- min: 0.2459, mean: 0.2549, std: 0.0245
  base: cuda_asm, mul: cuda_asm, lsh: c_from_asm
  cuda, 1 per block, test --- min: 0.3065, mean: 0.3081, std: 0.0011
  base: cuda_asm, mul: cuda_asm, lsh: c
  cuda, 1 per block, test --- min: 0.2604, mean: 0.2619, std: 0.0011
  base: cuda_asm, mul: c_from_asm, lsh: cuda_asm
  cuda, 1 per block, test --- min: 0.2461, mean: 0.2483, std: 0.0014
  base: cuda_asm, mul: c_from_asm, lsh: c_from_asm
  cuda, 1 per block, test --- min: 0.3060, mean: 0.3082, std: 0.0012
  base: cuda_asm, mul: c_from_asm, lsh: c
  cuda, 1 per block, test --- min: 0.2597, mean: 0.2621, std: 0.0013
  base: cuda_asm, mul: c, lsh: cuda_asm
  cuda, 1 per block, test --- min: 0.2462, mean: 0.2483, std: 0.0013
  base: cuda_asm, mul: c, lsh: c_from_asm
  cuda, 1 per block, test --- min: 0.3066, mean: 0.3078, std: 0.0008
  base: cuda_asm, mul: c, lsh: c
  cuda, 1 per block, test --- min: 0.2599, mean: 0.2625, std: 0.0013
  base: c, mul: cuda_asm, lsh: cuda_asm
  cuda, 1 per block, test --- min: 0.2471, mean: 0.2482, std: 0.0012
  base: c, mul: cuda_asm, lsh: c_from_asm
  cuda, 1 per block, test --- min: 0.2713, mean: 0.2735, std: 0.0013
  base: c, mul: cuda_asm, lsh: c
  cuda, 1 per block, test --- min: 0.2469, mean: 0.2478, std: 0.0014
  base: c, mul: c_from_asm, lsh: cuda_asm
  cuda, 1 per block, test --- min: 0.2476, mean: 0.2492, std: 0.0011
  base: c, mul: c_from_asm, lsh: c_from_asm
  cuda, 1 per block, test --- min: 0.2717, mean: 0.2740, std: 0.0012
  base: c, mul: c_from_asm, lsh: c
  cuda, 1 per block, test --- min: 0.2468, mean: 0.2490, std: 0.0013
  base: c, mul: c, lsh: cuda_asm
  cuda, 1 per block, test --- min: 0.2482, mean: 0.2497, std: 0.0010
  base: c, mul: c, lsh: c_from_asm
  cuda, 1 per block, test --- min: 0.2715, mean: 0.2735, std: 0.0017
  base: c, mul: c, lsh: c
  cuda, 1 per block, test --- min: 0.2473, mean: 0.2493, std: 0.0012
Best time: 0.2459 for [base: cuda_asm, mul: cuda_asm, lsh: cuda_asm]
<br/></div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_fft_performance[cuda:0:0-global_mem-1]</td>
          <td class="col-duration">10.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log"> ------------------------------Captured stdout call------------------------------ <br/>
cuda, 1 per block, test --- min: 0.3320, mean: 0.3341, std: 0.0018
<br/></div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_fft_performance[cuda:0:0-global_mem-2]</td>
          <td class="col-duration">9.50</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log"> ------------------------------Captured stdout call------------------------------ <br/>
cuda, 2 per block, test --- min: 0.3331, mean: 0.3351, std: 0.0015
<br/></div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_fft_performance[cuda:0:0-global_mem-3]</td>
          <td class="col-duration">10.51</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log"> ------------------------------Captured stdout call------------------------------ <br/>
cuda, 3 per block, test --- min: 0.4213, mean: 0.4228, std: 0.0017
<br/></div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_fft_performance[cuda:0:0-global_mem-4]</td>
          <td class="col-duration">9.74</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log"> ------------------------------Captured stdout call------------------------------ <br/>
cuda, 4 per block, test --- min: 0.3396, mean: 0.3414, std: 0.0013
<br/></div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_fft_performance[cuda:0:0-constant_mem-1]</td>
          <td class="col-duration">9.36</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log"> ------------------------------Captured stdout call------------------------------ <br/>
cuda, 1 per block, test --- min: 0.2390, mean: 0.2411, std: 0.0019
<br/></div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_fft_performance[cuda:0:0-constant_mem-2]</td>
          <td class="col-duration">36.81</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log"> ------------------------------Captured stdout call------------------------------ <br/>
cuda, 2 per block, test --- min: 0.2431, mean: 0.2455, std: 0.0018
<br/></div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_fft_performance[cuda:0:0-constant_mem-3]</td>
          <td class="col-duration">34.43</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log"> ------------------------------Captured stdout call------------------------------ <br/>
cuda, 3 per block, test --- min: 0.3186, mean: 0.3206, std: 0.0017
<br/></div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_computation.py::test_fft_performance[cuda:0:0-constant_mem-4]</td>
          <td class="col-duration">6.46</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log"> ------------------------------Captured stdout call------------------------------ <br/>
cuda, 4 per block, test --- min: 0.2462, mean: 0.2476, std: 0.0018
<br/></div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_arithmetic.py::test_prepare_for_mul_cpu</td>
          <td class="col-duration">0.02</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_ntt_cpu.py::test_ntt</td>
          <td class="col-duration">0.05</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_ntt_cpu.py::test_find_generator</td>
          <td class="col-duration">0.01</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_ntt_cpu.py::test_repr</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_transform/test_ntt_cpu.py::test_gnum_to_i32</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody></table></body></html>