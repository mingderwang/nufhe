<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <title>Test Report</title>
    <link href="assets/style.css" rel="stylesheet" type="text/css"/></head>
  <body onLoad="init()">
    <script>/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this file,
 * You can obtain one at http://mozilla.org/MPL/2.0/. */


function toArray(iter) {
    if (iter === null) {
        return null;
    }
    return Array.prototype.slice.call(iter);
}

function find(selector, elem) {
    if (!elem) {
        elem = document;
    }
    return elem.querySelector(selector);
}

function find_all(selector, elem) {
    if (!elem) {
        elem = document;
    }
    return toArray(elem.querySelectorAll(selector));
}

function sort_column(elem) {
    toggle_sort_states(elem);
    var colIndex = toArray(elem.parentNode.childNodes).indexOf(elem);
    var key;
    if (elem.classList.contains('numeric')) {
        key = key_num;
    } else if (elem.classList.contains('result')) {
        key = key_result;
    } else {
        key = key_alpha;
    }
    sort_table(elem, key(colIndex));
}

function show_all_extras() {
    find_all('.col-result').forEach(show_extras);
}

function hide_all_extras() {
    find_all('.col-result').forEach(hide_extras);
}

function show_extras(colresult_elem) {
    var extras = colresult_elem.parentNode.nextElementSibling;
    var expandcollapse = colresult_elem.firstElementChild;
    extras.classList.remove("collapsed");
    expandcollapse.classList.remove("expander");
    expandcollapse.classList.add("collapser");
}

function hide_extras(colresult_elem) {
    var extras = colresult_elem.parentNode.nextElementSibling;
    var expandcollapse = colresult_elem.firstElementChild;
    extras.classList.add("collapsed");
    expandcollapse.classList.remove("collapser");
    expandcollapse.classList.add("expander");
}

function show_filters() {
    var filter_items = document.getElementsByClassName('filter');
    for (var i = 0; i < filter_items.length; i++)
        filter_items[i].hidden = false;
}

function add_collapse() {
    // Add links for show/hide all
    var resulttable = find('table#results-table');
    var showhideall = document.createElement("p");
    showhideall.innerHTML = '<a href="javascript:show_all_extras()">Show all details</a> / ' +
                            '<a href="javascript:hide_all_extras()">Hide all details</a>';
    resulttable.parentElement.insertBefore(showhideall, resulttable);

    // Add show/hide link to each result
    find_all('.col-result').forEach(function(elem) {
        var collapsed = get_query_parameter('collapsed') || 'Passed';
        var extras = elem.parentNode.nextElementSibling;
        var expandcollapse = document.createElement("span");
        if (extras.classList.contains("collapsed")) {
            expandcollapse.classList.add("expander")
        } else if (collapsed.includes(elem.innerHTML)) {
            extras.classList.add("collapsed");
            expandcollapse.classList.add("expander");
        } else {
            expandcollapse.classList.add("collapser");
        }
        elem.appendChild(expandcollapse);

        elem.addEventListener("click", function(event) {
            if (event.currentTarget.parentNode.nextElementSibling.classList.contains("collapsed")) {
                show_extras(event.currentTarget);
            } else {
                hide_extras(event.currentTarget);
            }
        });
    })
}

function get_query_parameter(name) {
    var match = RegExp('[?&]' + name + '=([^&]*)').exec(window.location.search);
    return match && decodeURIComponent(match[1].replace(/\+/g, ' '));
}

function init () {
    reset_sort_headers();

    add_collapse();

    show_filters();

    sort_column(find('.initial-sort'));

    find_all('.sortable').forEach(function(elem) {
        elem.addEventListener("click",
                              function(event) {
                                  sort_column(elem);
                              }, false)
    });

};

function sort_table(clicked, key_func) {
    var rows = find_all('.results-table-row');
    var reversed = !clicked.classList.contains('asc');
    var sorted_rows = sort(rows, key_func, reversed);
    /* Whole table is removed here because browsers acts much slower
     * when appending existing elements.
     */
    var thead = document.getElementById("results-table-head");
    document.getElementById('results-table').remove();
    var parent = document.createElement("table");
    parent.id = "results-table";
    parent.appendChild(thead);
    sorted_rows.forEach(function(elem) {
        parent.appendChild(elem);
    });
    document.getElementsByTagName("BODY")[0].appendChild(parent);
}

function sort(items, key_func, reversed) {
    var sort_array = items.map(function(item, i) {
        return [key_func(item), i];
    });

    sort_array.sort(function(a, b) {
        var key_a = a[0];
        var key_b = b[0];

        if (key_a == key_b) return 0;

        if (reversed) {
            return (key_a < key_b ? 1 : -1);
        } else {
            return (key_a > key_b ? 1 : -1);
        }
    });

    return sort_array.map(function(item) {
        var index = item[1];
        return items[index];
    });
}

function key_alpha(col_index) {
    return function(elem) {
        return elem.childNodes[1].childNodes[col_index].firstChild.data.toLowerCase();
    };
}

function key_num(col_index) {
    return function(elem) {
        return parseFloat(elem.childNodes[1].childNodes[col_index].firstChild.data);
    };
}

function key_result(col_index) {
    return function(elem) {
        var strings = ['Error', 'Failed', 'Rerun', 'XFailed', 'XPassed',
                       'Skipped', 'Passed'];
        return strings.indexOf(elem.childNodes[1].childNodes[col_index].firstChild.data);
    };
}

function reset_sort_headers() {
    find_all('.sort-icon').forEach(function(elem) {
        elem.parentNode.removeChild(elem);
    });
    find_all('.sortable').forEach(function(elem) {
        var icon = document.createElement("div");
        icon.className = "sort-icon";
        icon.textContent = "vvv";
        elem.insertBefore(icon, elem.firstChild);
        elem.classList.remove("desc", "active");
        elem.classList.add("asc", "inactive");
    });
}

function toggle_sort_states(elem) {
    //if active, toggle between asc and desc
    if (elem.classList.contains('active')) {
        elem.classList.toggle('asc');
        elem.classList.toggle('desc');
    }

    //if inactive, reset all other functions and add ascending active
    if (elem.classList.contains('inactive')) {
        reset_sort_headers();
        elem.classList.remove('inactive');
        elem.classList.add('active');
    }
}

function is_all_rows_hidden(value) {
  return value.hidden == false;
}

function filter_table(elem) {
    var outcome_att = "data-test-result";
    var outcome = elem.getAttribute(outcome_att);
    class_outcome = outcome + " results-table-row";
    var outcome_rows = document.getElementsByClassName(class_outcome);

    for(var i = 0; i < outcome_rows.length; i++){
        outcome_rows[i].hidden = !elem.checked;
    }

    var rows = find_all('.results-table-row').filter(is_all_rows_hidden);
    var all_rows_hidden = rows.length == 0 ? true : false;
    var not_found_message = document.getElementById("not-found-message");
    not_found_message.hidden = !all_rows_hidden;
}
</script>
    <h1>test_gates.html</h1>
    <p>Report generated on 27-Jul-2020 at 10:32:23 by <a href="https://pypi.python.org/pypi/pytest-html">pytest-html</a> v2.1.1</p>
    <h2>Environment</h2>
    <table id="environment">
      <tr>
        <td>Packages</td>
        <td>{"pluggy": "0.13.1", "py": "1.9.0", "pytest": "5.4.3"}</td></tr>
      <tr>
        <td>Platform</td>
        <td>Linux-4.9.140-tegra-aarch64-with-Ubuntu-18.04-bionic</td></tr>
      <tr>
        <td>Plugins</td>
        <td>{"benchmark": "3.2.3", "html": "2.1.1", "metadata": "1.10.0"}</td></tr>
      <tr>
        <td>Python</td>
        <td>3.6.9</td></tr></table>
    <h2>Summary</h2>
    <p>60 tests ran in 816.79 seconds. </p>
    <p class="filter" hidden="true">(Un)check the boxes to filter the results.</p><input checked="true" class="filter" data-test-result="passed" hidden="true" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="passed">18 passed</span>, <input checked="true" class="filter" data-test-result="skipped" disabled="true" hidden="true" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="skipped">0 skipped</span>, <input checked="true" class="filter" data-test-result="failed" hidden="true" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="failed">42 failed</span>, <input checked="true" class="filter" data-test-result="error" hidden="true" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="error">9 errors</span>, <input checked="true" class="filter" data-test-result="xfailed" disabled="true" hidden="true" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="xfailed">0 expected failures</span>, <input checked="true" class="filter" data-test-result="xpassed" disabled="true" hidden="true" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="xpassed">0 unexpected passes</span>
    <h2>Results</h2>
    <table id="results-table">
      <thead id="results-table-head">
        <tr>
          <th class="sortable result initial-sort" col="result">Result</th>
          <th class="sortable" col="name">Test</th>
          <th class="sortable numeric" col="duration">Duration</th>
          <th>Links</th></tr>
        <tr hidden="true" id="not-found-message">
          <th colspan="4">No results found. Try to check the filters</th></tr></thead>
      <tbody class="error results-table-row">
        <tr>
          <td class="col-result">Error</td>
          <td class="col-name">test/test_gates.py::test_single_kernel_bs[bs_loop-cuda:0:0]::setup</td>
          <td class="col-duration">0.06</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f8823d4e0&gt;<br/><br/>    @pytest.fixture(scope=&#x27;session&#x27;)<br/>    def key_pair(thread):<br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng)<br/><br/>test/conftest.py:120: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f8823d4e0&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="error results-table-row">
        <tr>
          <td class="col-result">Error</td>
          <td class="col-name">test/test_gates.py::test_single_kernel_bs_with_ks[bs_kernel-cuda:0:0]::setup</td>
          <td class="col-duration">0.06</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f8850c828&gt;<br/><br/>    @pytest.fixture(scope=&#x27;session&#x27;)<br/>    def key_pair(thread):<br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng)<br/><br/>test/conftest.py:120: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f8850c828&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="error results-table-row">
        <tr>
          <td class="col-result">Error</td>
          <td class="col-name">test/test_gates.py::test_or_gate[cuda:0:0]::setup</td>
          <td class="col-duration">4.64</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f882c1630&gt;<br/><br/>    @pytest.fixture(scope=&#x27;session&#x27;)<br/>    def key_pair(thread):<br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng)<br/><br/>test/conftest.py:120: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:249: in make_key_pair<br/>    cloud_key = NuFHECloudKey.from_rng(thr, nufhe_params, rng, secret_key)<br/>nufhe/api_low_level.py:192: in from_rng<br/>    bk = BootstrapKey.from_rng(thr, rng, secret_key.lwe_key, tgsw_key, perf_params)<br/>nufhe/bootstrap.py:70: in from_rng<br/>    tgsw_encrypt_int(thr, rng, bk, lwe_key.key, accum_params.min_noise, tgsw_key, perf_params)<br/>nufhe/tgsw.py:160: in tgsw_encrypt_int<br/>    tgsw_encrypt_zero(thr, rng, result, noise, key, perf_params)<br/>nufhe/tgsw.py:151: in tgsw_encrypt_zero<br/>    tlwe_encrypt_zero(thr, rng, result.samples, noise, key.tlwe_key, perf_params)<br/>nufhe/tlwe.py:195: in tlwe_encrypt_zero<br/>    comp = get_computation(thr, TLweEncryptZero, key.params, result.shape, noise, perf_params)<br/>nufhe/computation_cache.py:55: in get_computation<br/>    compiled_comp = comp.compile(thr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/tlwe_gpu.py:182: in _build_plan<br/>    plan.computation_call(ft_noises, noises1_tr, noises1)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:502: in computation_call<br/>    self._compiler_options, self._keep))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/transform/computation.py:91: in _build_plan<br/>    slices=(len(output.shape) - 1, 1)))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:473: in kernel_call<br/>    keep=self._keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:566: in compile_static<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:786: in __init__<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:655: in __init__<br/>    self.source, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:504: in _create_program<br/>    src, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:227: in _compile<br/>    return SourceModule(src, no_extern_c=True, options=options, keep=keep)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;pycuda.compiler.SourceModule object at 0x7f88192e10&gt;<br/>source = &#x27;\n\n\n    #define CUDA\n    // taken from pycuda._cluda\n    #define LOCAL_BARRIER __syncthreads()\n\n    #define WIT...(\n            batch_id * 1 + transform_in_block, thread_in_transform + i * 128,\n            r_out[i]);\n    }\n}\n\n&#x27;<br/>nvcc = &#x27;nvcc&#x27;, options = [], keep = False, no_extern_c = True, arch = None, code = None<br/>cache_dir = None, include_dirs = []<br/><br/>    def __init__(self, source, nvcc=&quot;nvcc&quot;, options=None, keep=False,<br/>            no_extern_c=False, arch=None, code=None, cache_dir=None,<br/>            include_dirs=[]):<br/>        self._check_arch(arch)<br/>    <br/>        cubin = compile(source, nvcc, options, keep, no_extern_c,<br/>                arch, code, cache_dir, include_dirs)<br/>    <br/>        from pycuda.driver import module_from_buffer<br/>&gt;       self.module = module_from_buffer(cubin)<br/><span class="error">E       pycuda._driver.LogicError: cuModuleLoadDataEx failed: invalid device context -</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/compiler.py:294: LogicError<br/> -----------------------------Captured stderr setup------------------------------ <br/>Exception ignored in: &lt;bound method Thread.__del__ of &lt;reikna.cluda.cuda.Thread object at 0x7f82fd2278&gt;&gt;
Traceback (most recent call last):
  File &quot;/home/mwang/envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py&quot;, line 247, in __del__
    self.release()
  File &quot;/home/mwang/envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py&quot;, line 597, in release
    self._release_specific()
  File &quot;/home/mwang/envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py&quot;, line 242, in _release_specific
    cuda.Context.pop()
pycuda._driver.LogicError: context::pop failed: invalid device context - cannot pop non-current context
<br/> -------------------------------Captured log setup------------------------------- <br/>[1m[31mERROR   [0m root:api.py:507 Failed to compile:
1:
2:
3:
4:    #define CUDA
5:    // taken from pycuda._cluda
6:    #define LOCAL_BARRIER __syncthreads()
7:
8:    #define WITHIN_KERNEL __device__
9:    #define KERNEL extern &quot;C&quot; __global__
10:    #define GLOBAL_MEM /* empty */
11:    #define GLOBAL_MEM_ARG /* empty */
12:    #define LOCAL_MEM __shared__
13:    #define LOCAL_MEM_DYNAMIC extern __shared__
14:    #define LOCAL_MEM_ARG /* empty */
15:    #define CONSTANT_MEM __constant__
16:    #define CONSTANT_MEM_ARG /* empty */
17:    #define INLINE __forceinline__
18:    #define SIZE_T int
19:    #define VSIZE_T int
20:
21:    // used to align fields in structures
22:    #define ALIGN(bytes) __align__(bytes)
23:
24:    
25:
26:    WITHIN_KERNEL SIZE_T get_local_id(unsigned int dim)
27:    {
28:        if(dim == 0) return threadIdx.x;
29:        if(dim == 1) return threadIdx.y;
30:        if(dim == 2) return threadIdx.z;
31:        return 0;
32:    }
33:
34:    WITHIN_KERNEL SIZE_T get_group_id(unsigned int dim)
35:    {
36:        if(dim == 0) return blockIdx.x;
37:        if(dim == 1) return blockIdx.y;
38:        if(dim == 2) return blockIdx.z;
39:        return 0;
40:    }
41:
42:    WITHIN_KERNEL SIZE_T get_local_size(unsigned int dim)
43:    {
44:        if(dim == 0) return blockDim.x;
45:        if(dim == 1) return blockDim.y;
46:        if(dim == 2) return blockDim.z;
47:        return 1;
48:    }
49:
50:    WITHIN_KERNEL SIZE_T get_num_groups(unsigned int dim)
51:    {
52:        if(dim == 0) return gridDim.x;
53:        if(dim == 1) return gridDim.y;
54:        if(dim == 2) return gridDim.z;
55:        return 1;
56:    }
57:
58:    WITHIN_KERNEL SIZE_T get_global_size(unsigned int dim)
59:    {
60:        return get_num_groups(dim) * get_local_size(dim);
61:    }
62:
63:    WITHIN_KERNEL SIZE_T get_global_id(unsigned int dim)
64:    {
65:        return get_local_id(dim) + get_group_id(dim) * get_local_size(dim);
66:    }
67:
68:
69:
70:
71:    #define COMPLEX_CTR(T) make_##T
72:
73:    WITHIN_KERNEL float2 operator+(float2 a, float2 b)
74:    {
75:        return COMPLEX_CTR(float2)(a.x + b.x, a.y + b.y);
76:    }
77:    WITHIN_KERNEL float2 operator-(float2 a, float2 b)
78:    {
79:        return COMPLEX_CTR(float2)(a.x - b.x, a.y - b.y);
80:    }
81:    WITHIN_KERNEL float2 operator+(float2 a) { return a; }
82:    WITHIN_KERNEL float2 operator-(float2 a) { return COMPLEX_CTR(float2)(-a.x, -a.y); }
83:    WITHIN_KERNEL double2 operator+(double2 a, double2 b)
84:    {
85:        return COMPLEX_CTR(double2)(a.x + b.x, a.y + b.y);
86:    }
87:    WITHIN_KERNEL double2 operator-(double2 a, double2 b)
88:    {
89:        return COMPLEX_CTR(double2)(a.x - b.x, a.y - b.y);
90:    }
91:    WITHIN_KERNEL double2 operator+(double2 a) { return a; }
92:    WITHIN_KERNEL double2 operator-(double2 a) { return COMPLEX_CTR(double2)(-a.x, -a.y); }
93:
94:WITHIN_KERNEL VSIZE_T virtual_local_id(unsigned int dim)
95:{
96:    if (dim == 1)
97:    {
98:
99:        SIZE_T flat_id =
100:            get_local_id(0) * 1 +
101:            0;
102:
103:        return (flat_id / 1);
104:
105:    }
106:    if (dim == 0)
107:    {
108:
109:        return 0;
110:
111:    }
112:
113:    return 0;
114:}
115:
116:WITHIN_KERNEL VSIZE_T virtual_local_size(unsigned int dim)
117:{
118:    if (dim == 1)
119:    {
120:        return 128;
121:    }
122:    if (dim == 0)
123:    {
124:        return 1;
125:    }
126:
127:    return 1;
128:}
129:
130:WITHIN_KERNEL VSIZE_T virtual_group_id(unsigned int dim)
131:{
132:    if (dim == 1)
133:    {
134:
135:        return 0;
136:
137:    }
138:    if (dim == 0)
139:    {
140:
141:        SIZE_T flat_id =
142:            get_group_id(1) * 1 +
143:            0;
144:
145:        return (flat_id / 1);
146:
147:    }
148:
149:    return 0;
150:}
151:
152:WITHIN_KERNEL VSIZE_T virtual_num_groups(unsigned int dim)
153:{
154:    if (dim == 1)
155:    {
156:        return 1;
157:    }
158:    if (dim == 0)
159:    {
160:        return 2000;
161:    }
162:
163:    return 1;
164:}
165:
166:WITHIN_KERNEL VSIZE_T virtual_global_id(unsigned int dim)
167:{
168:    return virtual_local_id(dim) + virtual_group_id(dim) * virtual_local_size(dim);
169:}
170:
171:WITHIN_KERNEL VSIZE_T virtual_global_size(unsigned int dim)
172:{
173:    if(dim == 1)
174:    {
175:        return 128;
176:    }
177:    if(dim == 0)
178:    {
179:        return 2000;
180:    }
181:
182:    return 1;
183:}
184:
185:WITHIN_KERNEL VSIZE_T virtual_global_flat_id()
186:{
187:    return
188:        virtual_global_id(1) * 1 +
189:        virtual_global_id(0) * 128 +
190:        0;
191:}
192:
193:WITHIN_KERNEL VSIZE_T virtual_global_flat_size()
194:{
195:    return
196:        virtual_global_size(1) *
197:        virtual_global_size(0) *
198:        1;
199:}
200:
201:
202:WITHIN_KERNEL bool virtual_skip_local_threads()
203:{
204:
205:    return false;
206:}
207:
208:WITHIN_KERNEL bool virtual_skip_groups()
209:{
210:
211:    return false;
212:}
213:
214:WITHIN_KERNEL bool virtual_skip_global_threads()
215:{
216:
217:    return false;
218:}
219:
220:
221:
222:#ifndef CUDA
223:#define MARK_VIRTUAL_FUNCTIONS_AS_USED (void)(virtual_num_groups(0)); (void)(virtual_global_flat_id()); (void)(virtual_global_flat_size())
224:#else
225:#define MARK_VIRTUAL_FUNCTIONS_AS_USED
226:#endif
227:
228:#define VIRTUAL_SKIP_THREADS MARK_VIRTUAL_FUNCTIONS_AS_USED; if(virtual_skip_local_threads() || virtual_skip_groups() || virtual_skip_global_threads()) return
229:
230:
231:    // leaf input macro for &quot;noises1&quot;
232:    #define _module1_(_idx0, _idx1, _idx2, _idx3, _idx4) (_leaf_noises1[(_idx0) * (4096) + (_idx1) * (2048) + (_idx2) * (1024) + (_idx3) * (1024) + (_idx4) * (1) + (0)])
233:    
234:
235:
236:
237:
238:    
239:    INLINE WITHIN_KERNEL int _module0_func(
240:        GLOBAL_MEM int *_leaf_noises1, VSIZE_T _c_idx0, VSIZE_T _c_idx1)
241:    {
242:        
243:
244:    
245:        
246:        VSIZE_T _idx0 = _c_idx0 / 4;
247:        _c_idx0 -= _idx0 * 4;
248:        
249:        VSIZE_T _idx1 = _c_idx0 / 2;
250:        _c_idx0 -= _idx1 * 2;
251:        
252:        VSIZE_T _idx2 = _c_idx0 / 1;
253:        _c_idx0 -= _idx2 * 1;
254:        
255:        VSIZE_T _idx3 = _c_idx0 / 1;
256:    
257:        
258:        VSIZE_T _idx4 = _c_idx1 / 1;
259:    
260:
261:
262:        return
263:        _module1_(_idx0, _idx1, _idx2, _idx3, _idx4);
264:    }
265:    
266:    #define _module0_(_c_idx0, _c_idx1) _module0_func(        _leaf_noises1, _c_idx0, _c_idx1)
267:    
268:
269:
270:
271:typedef struct __module3_
272:{
273:    unsigned long val;
274:} _module3_;
275:
276:
277:WITHIN_KERNEL INLINE _module3_ _module3_pack(unsigned long x)
278:{
279:    _module3_ res = {x};
280:    return res;
281:}
282:
283:WITHIN_KERNEL INLINE unsigned long _module3_unpack(_module3_ x)
284:{
285:    return x.val;
286:}
287:
288:#define _module3_zero (_module3_pack(0));
289:
290:
291:#define _module3_UNPACK(hi, lo, src) {lo = (src); hi = (src) &gt;&gt; 32;}
292:
293:#ifdef CUDA
294:#define _module3_PACK(hi, lo) (((unsigned long)(hi) &lt;&lt; 32) | (lo))
295:#else
296:#define _module3_PACK(hi, lo) upsample(hi, lo)
297:#endif
298:
299:
300:#define _module3_SUB_CC(hi, lo, x1, x2) { bool cc = (x1) &lt; (x2); lo = (x1) - (x2); if (cc) { hi -= 1; } }
301:#define _module3_ADD_CC(hi, lo, x1, x2, hi_prev) { lo = (x1) + (x2); bool cc = lo &lt; (x2); hi = hi_prev; if (cc) { hi += 1; } }
302:
303:
304:
305:
306:/** Subtraction in FF(P): val_ = a + b mod P. */
307:WITHIN_KERNEL INLINE _module3_ _module4_(_module3_ a, _module3_ b)
308:{
309:
310:    _module3_ res = {0};
311:    asm(&quot;{\n\t&quot;
312:        &quot;.reg .u32          m;\n\t&quot;
313:        &quot;.reg .u64          t;\n\t&quot;
314:        // this = a - b;
315:        &quot;sub.u64            %0, %1, %2;\n\t&quot;
316:        // this -= (uint32_t)(-(this &gt; a));
317:        &quot;set.gt.u32.u64     m, %0, %1;\n\t&quot;
318:        &quot;mov.b64            t, {m, 0};\n\t&quot;
319:        &quot;sub.u64            %0, %0, t;\n\t&quot;
320:        &quot;}&quot;
321:        : &quot;+l&quot;(res.val)
322:        : &quot;l&quot;(a.val), &quot;l&quot;(b.val));
323:    return res;
324:
325:}
326:
327:
328:
329:// Addition in FF(P): val_ = a + b mod P.
330:WITHIN_KERNEL INLINE _module3_ _module5_(_module3_ a, _module3_ b)
331:{
332:
333:    _module3_ res = {0};
334:    asm(&quot;{\n\t&quot;
335:        &quot;.reg .u32          m;\n\t&quot;
336:        &quot;.reg .u64          t;\n\t&quot;
337:        &quot;.reg .pred         p;\n\t&quot;
338:        // this = a + b;
339:        &quot;add.u64            %0, %1, %2;\n\t&quot;
340:        // this += (uint32_t)(-(this &lt; b || this &gt;= FFP_MODULUS));
341:        &quot;setp.lt.u64        p, %0, %2;\n\t&quot;
342:        &quot;set.ge.or.u32.u64  m, %0, %3, p;\n\t&quot;
343:        &quot;mov.b64            t, {m, 0};\n\t&quot;
344:        &quot;add.u64            %0, %0, t;\n\t&quot;
345:        &quot;}&quot;
346:        : &quot;+l&quot;(res.val)
347:        : &quot;l&quot;(a.val), &quot;l&quot;(b.val), &quot;l&quot;(18446744069414584321UL));
348:    return res;
349:
350:}
351:
352:
353:
354:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
355:WITHIN_KERNEL INLINE _module3_ _module7_(unsigned long a)
356:{
357:
358:    _module3_ res = {0};
359:    asm(&quot;{\n\t&quot;
360:        &quot;.reg .u32        m;\n\t&quot;
361:        &quot;.reg .u64        t;\n\t&quot;
362:        &quot;mov.u64          %0, %1;\n\t&quot;
363:        &quot;set.ge.u32.u64   m, %0, %2;\n\t&quot;
364:        &quot;mov.b64          t, {m, 0};\n\t&quot;
365:        &quot;add.u64         %0, %0, t;\n\t&quot;
366:        &quot;}&quot;
367:        : &quot;+l&quot;(res.val)
368:        : &quot;l&quot;(a), &quot;l&quot;(18446744069414584321UL));
369:    return res;
370:
371:}
372:
373:
374:
375:
376:/**
377:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
378:* @param[in] l An integer in [0, 32)
379:*/
380:WITHIN_KERNEL INLINE _module3_ _module6_(_module3_ x, unsigned int l)
381:{
382:    /*
383:    Algorithm:
384:
385:    We can decompose the shift as
386:
387:        res = x * 2^l = x * M^k * 2^j,
388:
389:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
390:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
391:
392:    After the multiplication by 2^j, the result contains 3 32-bit parts:
393:
394:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
395:
396:    Thus
397:
398:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
399:
400:    Taking the modulus P = M^2 - M + 1, we get
401:
402:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
403:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
404:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
405:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
406:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
407:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
408:
409:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
410:
411:    The processing for the things inside the parentheses is simpler:
412:
413:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
414:    - (x + y) = PACK(s &lt; y, s), where s = x + y
415:      (that is, check for overflow and add 1 in the high half)
416:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
417:      (that is, check for overflow and add (M-1) in the low half)
418:    */
419:
420:
421:
422:
423:        asm(&quot;{\n\t&quot;
424:            &quot;.reg .u32      r0, r1;\n\t&quot;
425:            &quot;.reg .u32      t0, t1, t2;\n\t&quot;
426:            &quot;.reg .u32      n;\n\t&quot;
427:            &quot;.reg .u64      s;\n\t&quot;
428:            // t[2] = (uint32_t)(x &gt;&gt; (64-l));
429:            // t[1] = (uint32_t)(x &gt;&gt; (32-l));
430:            // t[0] = (uint32_t)(x &lt;&lt; l);
431:            &quot;mov.b64        {r0, r1}, %0;\n\t&quot;
432:            &quot;shl.b32        t0, r0, %1;\n\t&quot;
433:            &quot;sub.u32        n, 32, %1;\n\t&quot;
434:            &quot;shr.b64        s, %0, n;\n\t&quot;
435:            &quot;mov.b64        {t1, t2}, s;\n\t&quot;
436:            // mod P
437:            &quot;add.u32        r1, t1, t2;\n\t&quot;
438:            &quot;sub.cc.u32     r0, t0, t2;\n\t&quot;
439:            &quot;subc.u32       r1, r1, 0;\n\t&quot;
440:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
441:            // ret += (uint32_t)(-(ret &lt; ((uint64_t *)t)[0]));
442:            &quot;mov.b64        s, {t0, t1};\n\t&quot;
443:            &quot;set.lt.u32.u64 t2, %0, s;\n\t&quot;
444:            &quot;add.cc.u32     r0, r0, t2;\n\t&quot;
445:            &quot;addc.u32       r1, r1, 0;\n\t&quot;
446:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
447:            &quot;}&quot;
448:            : &quot;+l&quot;(x.val)
449:            : &quot;r&quot;(l));
450:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
451:        return _module7_(x.val);
452:
453:
454:
455:}
456:
457:
458:
459:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
460:WITHIN_KERNEL INLINE _module3_ _module9_(unsigned long a)
461:{
462:
463:    _module3_ res = {0};
464:    asm(&quot;{\n\t&quot;
465:        &quot;.reg .u32        m;\n\t&quot;
466:        &quot;.reg .u64        t;\n\t&quot;
467:        &quot;mov.u64          %0, %1;\n\t&quot;
468:        &quot;set.ge.u32.u64   m, %0, %2;\n\t&quot;
469:        &quot;mov.b64          t, {m, 0};\n\t&quot;
470:        &quot;add.u64         %0, %0, t;\n\t&quot;
471:        &quot;}&quot;
472:        : &quot;+l&quot;(res.val)
473:        : &quot;l&quot;(a), &quot;l&quot;(18446744069414584321UL));
474:    return res;
475:
476:}
477:
478:
479:
480:
481:/**
482:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
483:* @param[in] l An integer in [0, 32)
484:*/
485:WITHIN_KERNEL INLINE _module3_ _module8_(_module3_ x, unsigned int l)
486:{
487:    /*
488:    Algorithm:
489:
490:    We can decompose the shift as
491:
492:        res = x * 2^l = x * M^k * 2^j,
493:
494:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
495:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
496:
497:    After the multiplication by 2^j, the result contains 3 32-bit parts:
498:
499:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
500:
501:    Thus
502:
503:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
504:
505:    Taking the modulus P = M^2 - M + 1, we get
506:
507:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
508:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
509:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
510:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
511:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
512:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
513:
514:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
515:
516:    The processing for the things inside the parentheses is simpler:
517:
518:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
519:    - (x + y) = PACK(s &lt; y, s), where s = x + y
520:      (that is, check for overflow and add 1 in the high half)
521:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
522:      (that is, check for overflow and add (M-1) in the low half)
523:    */
524:
525:
526:
527:
528:        asm(&quot;{\n\t&quot;
529:            &quot;.reg .u32          r0, r1;\n\t&quot;
530:            &quot;.reg .u32          t0, t1, t2;\n\t&quot;
531:            &quot;.reg .u32          n;\n\t&quot;
532:            &quot;.reg .u64          s;\n\t&quot;
533:            &quot;.reg .pred         p, q;\n\t&quot;
534:            // t[2] = (uint32_t)(x &gt;&gt; (96-l));
535:            // t[1] = (uint32_t)(x &gt;&gt; (64-l));
536:            // t[0] = (uint32_t)(x &lt;&lt; (l-32));
537:            &quot;mov.b64            {r0, r1}, %0;\n\t&quot;
538:            &quot;sub.u32            n, %1, 32;\n\t&quot;
539:            &quot;shl.b32            t0, r0, n;\n\t&quot;
540:            &quot;sub.u32            n, 32, n;\n\t&quot;
541:            &quot;shr.b64            s, %0, n;\n\t&quot;
542:            &quot;mov.b64            {t1, t2}, s;\n\t&quot;
543:            // mod P
544:            &quot;add.u32            r1, t0, t1;\n\t&quot;
545:            &quot;sub.cc.u32         r0, 0, t1;\n\t&quot;
546:            &quot;subc.u32           r1, r1, 0;\n\t&quot;
547:            &quot;sub.cc.u32         r0, r0, t2;\n\t&quot;
548:            &quot;subc.u32           r1, r1, 0;\n\t&quot;
549:            &quot;mov.b64            %0, {r0, r1};\n\t&quot;
550:            // ret -= (uint32_t)(-(ret &gt; ((uint64_t)t[0] &lt;&lt; 32) &amp;&amp; t[1] == 0));
551:            &quot;setp.eq.u32        p|q, t1, 0;\n\t&quot;
552:            &quot;mov.b64            s, {0, t0};\n\t&quot;
553:            &quot;set.gt.and.u32.u64 t2, %0, s, p;\n\t&quot;
554:            &quot;sub.cc.u32         r0, r0, t2;\n\t&quot;
555:            &quot;subc.u32           r1, r1, 0;\n\t&quot;
556:            &quot;mov.b64            %0, {r0, r1};\n\t&quot;
557:            // ret += (uint32_t)(-(ret &lt; ((uint64_t)t[0] &lt;&lt; 32) &amp;&amp; t[1] != 0));
558:            &quot;set.lt.and.u32.u64 t2, %0, s, q;\n\t&quot;
559:            &quot;add.cc.u32         r0, r0, t2;\n\t&quot;
560:            &quot;addc.u32           r1, r1, 0;\n\t&quot;
561:            &quot;mov.b64            %0, {r0, r1};\n\t&quot;
562:            &quot;}&quot;
563:            : &quot;+l&quot;(x.val)
564:            : &quot;r&quot;(l));
565:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
566:        return _module9_(x.val);
567:
568:
569:
570:}
571:
572:
573:
574:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
575:WITHIN_KERNEL INLINE _module3_ _module11_(unsigned long a)
576:{
577:
578:    _module3_ res = {0};
579:    asm(&quot;{\n\t&quot;
580:        &quot;.reg .u32        m;\n\t&quot;
581:        &quot;.reg .u64        t;\n\t&quot;
582:        &quot;mov.u64          %0, %1;\n\t&quot;
583:        &quot;set.ge.u32.u64   m, %0, %2;\n\t&quot;
584:        &quot;mov.b64          t, {m, 0};\n\t&quot;
585:        &quot;add.u64         %0, %0, t;\n\t&quot;
586:        &quot;}&quot;
587:        : &quot;+l&quot;(res.val)
588:        : &quot;l&quot;(a), &quot;l&quot;(18446744069414584321UL));
589:    return res;
590:
591:}
592:
593:
594:
595:
596:/**
597:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
598:* @param[in] l An integer in [0, 32)
599:*/
600:WITHIN_KERNEL INLINE _module3_ _module10_(_module3_ x, unsigned int l)
601:{
602:    /*
603:    Algorithm:
604:
605:    We can decompose the shift as
606:
607:        res = x * 2^l = x * M^k * 2^j,
608:
609:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
610:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
611:
612:    After the multiplication by 2^j, the result contains 3 32-bit parts:
613:
614:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
615:
616:    Thus
617:
618:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
619:
620:    Taking the modulus P = M^2 - M + 1, we get
621:
622:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
623:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
624:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
625:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
626:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
627:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
628:
629:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
630:
631:    The processing for the things inside the parentheses is simpler:
632:
633:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
634:    - (x + y) = PACK(s &lt; y, s), where s = x + y
635:      (that is, check for overflow and add 1 in the high half)
636:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
637:      (that is, check for overflow and add (M-1) in the low half)
638:    */
639:
640:
641:
642:
643:        asm(&quot;{\n\t&quot;
644:            &quot;.reg .u32      r0, r1;\n\t&quot;
645:            &quot;.reg .u32      t0, t1, t2;\n\t&quot;
646:            &quot;.reg .u32      n;\n\t&quot;
647:            &quot;.reg .u64      s;\n\t&quot;
648:            // t[2] = (uint32_t)(x &gt;&gt; (128-l));
649:            // t[1] = (uint32_t)(x &gt;&gt; (96-l));
650:            // t[0] = (uint32_t)(x &lt;&lt; (l-64));
651:            &quot;mov.b64        {r0, r1}, %0;\n\t&quot;
652:            &quot;sub.u32        n, %1, 64;\n\t&quot;
653:            &quot;shl.b32        t0, r0, n;\n\t&quot;
654:            &quot;sub.u32        n, 32, n;\n\t&quot;
655:            &quot;shr.b64        s, %0, n;\n\t&quot;
656:            &quot;mov.b64        {t1, t2}, s;\n\t&quot;
657:            // mod P
658:            &quot;add.cc.u32     r0, t1, t0;\n\t&quot;
659:            &quot;addc.u32       r1, t2, 0;\n\t&quot;
660:            &quot;sub.u32        r1, r1, t0;\n\t&quot;
661:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
662:            // ret -= (uint32_t)(-(ret &gt; ((uint64_t *)t)[1]));
663:            &quot;mov.b64        s, {t1, t2};\n\t&quot;
664:            &quot;set.gt.u32.u64 t2, %0, s;\n\t&quot;
665:            &quot;sub.cc.u32     r0, r0, t2;\n\t&quot;
666:            &quot;subc.u32       r1, r1, 0;\n\t&quot;
667:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
668:            &quot;}&quot;
669:            : &quot;+l&quot;(x.val)
670:            : &quot;r&quot;(l));
671:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
672:        x = _module11_(x.val);
673:        x.val = 18446744069414584321UL - x.val;
674:        return x;
675:
676:
677:
678:}
679:
680:
681:
682:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
683:WITHIN_KERNEL INLINE _module3_ _module13_(unsigned long a)
684:{
685:
686:    _module3_ res = {0};
687:    asm(&quot;{\n\t&quot;
688:        &quot;.reg .u32        m;\n\t&quot;
689:        &quot;.reg .u64        t;\n\t&quot;
690:        &quot;mov.u64          %0, %1;\n\t&quot;
691:        &quot;set.ge.u32.u64   m, %0, %2;\n\t&quot;
692:        &quot;mov.b64          t, {m, 0};\n\t&quot;
693:        &quot;add.u64         %0, %0, t;\n\t&quot;
694:        &quot;}&quot;
695:        : &quot;+l&quot;(res.val)
696:        : &quot;l&quot;(a), &quot;l&quot;(18446744069414584321UL));
697:    return res;
698:
699:}
700:
701:
702:
703:
704:/**
705:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
706:* @param[in] l An integer in [0, 32)
707:*/
708:WITHIN_KERNEL INLINE _module3_ _module12_(_module3_ x, unsigned int l)
709:{
710:    /*
711:    Algorithm:
712:
713:    We can decompose the shift as
714:
715:        res = x * 2^l = x * M^k * 2^j,
716:
717:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
718:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
719:
720:    After the multiplication by 2^j, the result contains 3 32-bit parts:
721:
722:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
723:
724:    Thus
725:
726:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
727:
728:    Taking the modulus P = M^2 - M + 1, we get
729:
730:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
731:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
732:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
733:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
734:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
735:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
736:
737:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
738:
739:    The processing for the things inside the parentheses is simpler:
740:
741:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
742:    - (x + y) = PACK(s &lt; y, s), where s = x + y
743:      (that is, check for overflow and add 1 in the high half)
744:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
745:      (that is, check for overflow and add (M-1) in the low half)
746:    */
747:
748:
749:
750:
751:        asm(&quot;{\n\t&quot;
752:            &quot;.reg .u32      r0, r1;\n\t&quot;
753:            &quot;.reg .u32      t0, t1, t2;\n\t&quot;
754:            &quot;.reg .u32      n;\n\t&quot;
755:            &quot;.reg .u64      s;\n\t&quot;
756:            // t[2] = (uint32_t)(x &lt;&lt; (l-160));
757:            // t[1] = (uint32_t)(x &gt;&gt; (224-l));
758:            // t[0] = (uint32_t)(x &gt;&gt; (192-l));
759:            &quot;mov.b64        {r0, r1}, %0;\n\t&quot;
760:            &quot;sub.u32        n, %1, 160;\n\t&quot;
761:            &quot;shl.b32        t2, r0, n;\n\t&quot;
762:            &quot;sub.u32        n, 32, n;\n\t&quot;
763:            &quot;shr.b64        s, %0, n;\n\t&quot;
764:            &quot;mov.b64        {t0, t1}, s;\n\t&quot;
765:            // mod P
766:            &quot;add.cc.u32     r0, t0, t2;\n\t&quot;
767:            &quot;addc.u32       r1, t1, 0;\n\t&quot;
768:            &quot;sub.u32        r1, r1, t2;\n\t&quot;
769:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
770:            // ret += (uint32_t)(-(ret &gt; ((uint64_t *)t)[0]));
771:            &quot;mov.b64        s, {t0, t1};\n\t&quot;
772:            &quot;set.gt.u32.u64 t2, %0, s;\n\t&quot;
773:            &quot;sub.cc.u32     r0, r0, t2;\n\t&quot;
774:            &quot;subc.u32       r1, r1, 0;\n\t&quot;
775:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
776:            &quot;}&quot;
777:            : &quot;+l&quot;(x.val)
778:            : &quot;r&quot;(l));
779:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
780:        return _module13_(x.val);
781:
782:
783:
784:}
785:
786:
787:
788:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
789:WITHIN_KERNEL INLINE _module3_ _module15_(unsigned long a)
790:{
791:
792:    _module3_ res = {0};
793:    asm(&quot;{\n\t&quot;
794:        &quot;.reg .u32        m;\n\t&quot;
795:        &quot;.reg .u64        t;\n\t&quot;
796:        &quot;mov.u64          %0, %1;\n\t&quot;
797:        &quot;set.ge.u32.u64   m, %0, %2;\n\t&quot;
798:        &quot;mov.b64          t, {m, 0};\n\t&quot;
799:        &quot;add.u64         %0, %0, t;\n\t&quot;
800:        &quot;}&quot;
801:        : &quot;+l&quot;(res.val)
802:        : &quot;l&quot;(a), &quot;l&quot;(18446744069414584321UL));
803:    return res;
804:
805:}
806:
807:
808:
809:
810:/**
811:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
812:* @param[in] l An integer in [0, 32)
813:*/
814:WITHIN_KERNEL INLINE _module3_ _module14_(_module3_ x, unsigned int l)
815:{
816:    /*
817:    Algorithm:
818:
819:    We can decompose the shift as
820:
821:        res = x * 2^l = x * M^k * 2^j,
822:
823:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
824:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
825:
826:    After the multiplication by 2^j, the result contains 3 32-bit parts:
827:
828:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
829:
830:    Thus
831:
832:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
833:
834:    Taking the modulus P = M^2 - M + 1, we get
835:
836:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
837:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
838:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
839:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
840:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
841:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
842:
843:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
844:
845:    The processing for the things inside the parentheses is simpler:
846:
847:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
848:    - (x + y) = PACK(s &lt; y, s), where s = x + y
849:      (that is, check for overflow and add 1 in the high half)
850:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
851:      (that is, check for overflow and add (M-1) in the low half)
852:    */
853:
854:
855:
856:
857:        asm(&quot;{\n\t&quot;
858:            &quot;.reg .u32          r0, r1;\n\t&quot;
859:            &quot;.reg .u32          t0, t1, t2;\n\t&quot;
860:            &quot;.reg .u32          n;\n\t&quot;
861:            &quot;.reg .u64          s;\n\t&quot;
862:            &quot;.reg .pred         p, q;\n\t&quot;
863:            // t[2] = (uint32_t)(x &gt;&gt; (192-l));
864:            // t[1] = (uint32_t)(x &gt;&gt; (160-l));
865:            // t[0] = (uint32_t)(x &lt;&lt; (l-128));
866:            &quot;mov.b64            {r0, r1}, %0;\n\t&quot;
867:            &quot;sub.u32            n, %1, 128;\n\t&quot;
868:            &quot;shl.b32            t0, r0, n;\n\t&quot;
869:            &quot;sub.u32            n, 32, n;\n\t&quot;
870:            &quot;shr.b64            s, %0, n;\n\t&quot;
871:            &quot;mov.b64            {t1, t2}, s;\n\t&quot;
872:            // mod P
873:            &quot;add.u32            r1, t0, t1;\n\t&quot;
874:            &quot;sub.cc.u32         r0, 0, t1;\n\t&quot;
875:            &quot;subc.u32           r1, r1, 0;\n\t&quot;
876:            &quot;sub.cc.u32         r0, r0, t2;\n\t&quot;
877:            &quot;subc.u32           r1, r1, 0;\n\t&quot;
878:            &quot;mov.b64            %0, {r0, r1};\n\t&quot;
879:            // ret -= (uint32_t)(-(ret &gt; ((uint64_t)t[0] &lt;&lt; 32) &amp;&amp; t[1] == 0));
880:            &quot;setp.eq.u32        p|q, t1, 0;\n\t&quot;
881:            &quot;mov.b64            s, {0, t0};\n\t&quot;
882:            &quot;set.gt.and.u32.u64 t2, %0, s, p;\n\t&quot;
883:            &quot;sub.cc.u32         r0, r0, t2;\n\t&quot;
884:            &quot;subc.u32           r1, r1, 0;\n\t&quot;
885:            &quot;mov.b64            %0, {r0, r1};\n\t&quot;
886:            // ret += (uint32_t)(-(ret &lt; ((uint64_t)t[0] &lt;&lt; 32) &amp;&amp; t[1] != 0));
887:            &quot;set.lt.and.u32.u64 t2, %0, s, q;\n\t&quot;
888:            &quot;add.cc.u32         r0, r0, t2;\n\t&quot;
889:            &quot;addc.u32           r1, r1, 0;\n\t&quot;
890:            &quot;mov.b64            %0, {r0, r1};\n\t&quot;
891:            &quot;}&quot;
892:            : &quot;+l&quot;(x.val)
893:            : &quot;r&quot;(l));
894:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
895:        x = _module15_(x.val);
896:        x.val = 18446744069414584321UL - x.val;
897:        return x;
898:
899:
900:
901:}
902:
903:
904:
905:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
906:WITHIN_KERNEL INLINE _module3_ _module17_(unsigned long a)
907:{
908:
909:    _module3_ res = {0};
910:    asm(&quot;{\n\t&quot;
911:        &quot;.reg .u32        m;\n\t&quot;
912:        &quot;.reg .u64        t;\n\t&quot;
913:        &quot;mov.u64          %0, %1;\n\t&quot;
914:        &quot;set.ge.u32.u64   m, %0, %2;\n\t&quot;
915:        &quot;mov.b64          t, {m, 0};\n\t&quot;
916:        &quot;add.u64         %0, %0, t;\n\t&quot;
917:        &quot;}&quot;
918:        : &quot;+l&quot;(res.val)
919:        : &quot;l&quot;(a), &quot;l&quot;(18446744069414584321UL));
920:    return res;
921:
922:}
923:
924:
925:
926:
927:/**
928:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
929:* @param[in] l An integer in [0, 32)
930:*/
931:WITHIN_KERNEL INLINE _module3_ _module16_(_module3_ x, unsigned int l)
932:{
933:    /*
934:    Algorithm:
935:
936:    We can decompose the shift as
937:
938:        res = x * 2^l = x * M^k * 2^j,
939:
940:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
941:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
942:
943:    After the multiplication by 2^j, the result contains 3 32-bit parts:
944:
945:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
946:
947:    Thus
948:
949:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
950:
951:    Taking the modulus P = M^2 - M + 1, we get
952:
953:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
954:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
955:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
956:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
957:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
958:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
959:
960:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
961:
962:    The processing for the things inside the parentheses is simpler:
963:
964:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
965:    - (x + y) = PACK(s &lt; y, s), where s = x + y
966:      (that is, check for overflow and add 1 in the high half)
967:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
968:      (that is, check for overflow and add (M-1) in the low half)
969:    */
970:
971:
972:
973:
974:        asm(&quot;{\n\t&quot;
975:            &quot;.reg .u32      r0, r1;\n\t&quot;
976:            &quot;.reg .u32      t0, t1, t2;\n\t&quot;
977:            &quot;.reg .u32      n;\n\t&quot;
978:            &quot;.reg .u64      s;\n\t&quot;
979:            // t[2] = (uint32_t)(x &gt;&gt; (160-l));
980:            // t[1] = (uint32_t)(x &gt;&gt; (128-l));
981:            // t[0] = (uint32_t)(x &lt;&lt; (l-96));
982:            &quot;mov.b64        {r0, r1}, %0;\n\t&quot;
983:            &quot;sub.u32        n, %1, 96;\n\t&quot;
984:            &quot;shl.b32        t0, r0, n;\n\t&quot;
985:            &quot;sub.u32        n, 32, n;\n\t&quot;
986:            &quot;shr.b64        s, %0, n;\n\t&quot;
987:            &quot;mov.b64        {t1, t2}, s;\n\t&quot;
988:            // mod P
989:            &quot;add.u32        r1, t1, t2;\n\t&quot;
990:            &quot;sub.cc.u32     r0, t0, t2;\n\t&quot;
991:            &quot;subc.u32       r1, r1, 0;\n\t&quot;
992:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
993:            // ret += (uint32_t)(-(ret &lt; ((uint64_t *)t)[0]));
994:            &quot;mov.b64        s, {t0, t1};\n\t&quot;
995:            &quot;set.lt.u32.u64 t2, %0, s;\n\t&quot;
996:            &quot;add.cc.u32     r0, r0, t2;\n\t&quot;
997:            &quot;addc.u32       r1, r1, 0;\n\t&quot;
998:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
999:            &quot;}&quot;
1000:            : &quot;+l&quot;(x.val)
1001:            : &quot;r&quot;(l));
1002:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
1003:        x = _module17_(x.val);
1004:        x.val = 18446744069414584321UL - x.val;
1005:        return x;
1006:
1007:
1008:
1009:}
1010:
1011:
1012:
1013:/** Subtraction in FF(P): val_ = a + b mod P. */
1014:WITHIN_KERNEL INLINE _module3_ _module19_(_module3_ a, _module3_ b)
1015:{
1016:
1017:    _module3_ res = {0};
1018:    asm(&quot;{\n\t&quot;
1019:        &quot;.reg .u32          m;\n\t&quot;
1020:        &quot;.reg .u64          t;\n\t&quot;
1021:        // this = a - b;
1022:        &quot;sub.u64            %0, %1, %2;\n\t&quot;
1023:        // this -= (uint32_t)(-(this &gt; a));
1024:        &quot;set.gt.u32.u64     m, %0, %1;\n\t&quot;
1025:        &quot;mov.b64            t, {m, 0};\n\t&quot;
1026:        &quot;sub.u64            %0, %0, t;\n\t&quot;
1027:        &quot;}&quot;
1028:        : &quot;+l&quot;(res.val)
1029:        : &quot;l&quot;(a.val), &quot;l&quot;(b.val));
1030:    return res;
1031:
1032:}
1033:
1034:
1035:
1036:WITHIN_KERNEL INLINE _module3_ _module18_(_module3_ a, _module3_ b)
1037:{
1038:    /*
1039:    This function performs Montgomery multiplication with the fixed modulus (M=2**64-2**32+1)
1040:    and fixed word size (R=2**64), which helps simplify the algorithm.
1041:    The result is `a * b * R**(-1) mod M`.
1042:
1043:    Note that if you multiply two numbers `a` and `b` in Montgomery representation
1044:    (a&#x27; = a * R mod M, b&#x27; = b * R mod M), the result is the Montgomery representation of their
1045:    product (a&#x27; * b&#x27; * R**(-1) mod M = a * b * R mod M).
1046:    But if one of the numbers is in Montgomery representation and the other is not, you
1047:    get the normal product back (a * b&#x27; * R**(-1) mod M = a * b mod M).
1048:
1049:    This way if one of the factors is precalculated, you can convert it
1050:    into Montgomery representation, and use this function instead of the general
1051:    multiplication function (which is slower).
1052:    */
1053:
1054:
1055:    #ifdef CUDA
1056:    unsigned long hi = __umul64hi(a.val, b.val);
1057:    #else
1058:    unsigned long hi = mul_hi(a.val, b.val);
1059:    #endif
1060:    unsigned long lo = a.val * b.val;
1061:
1062:    unsigned long u = (lo &lt;&lt; 32) + lo;
1063:    unsigned long p2 = u - (u &gt;&gt; 32);
1064:    unsigned long uu = u &lt;&lt; 32;
1065:    if (uu &gt; u)
1066:    {
1067:        p2 -= 1;
1068:    }
1069:
1070:
1071:    _module3_ hi_ff = { hi };
1072:    _module3_ p2_ff = { p2 };
1073:
1074:    return _module19_(hi_ff, p2_ff);
1075:}
1076:
1077:
1078:
1079:
1080:
1081:#define _module2_CDATA_QUALIFIER GLOBAL_MEM_ARG
1082:
1083:
1084:WITHIN_KERNEL INLINE void _module2_swap(_module3_ *a, _module3_ *b)
1085:{
1086:    _module3_ t = *b;
1087:    *b = *a;
1088:    *a = t;
1089:}
1090:
1091:
1092:WITHIN_KERNEL INLINE void _module2_NTT2(_module3_* r0, _module3_* r1)
1093:{
1094:    _module3_ t = _module4_(*r0, *r1);
1095:    *r0 = _module5_(*r0, *r1);
1096:    *r1 = t;
1097:}
1098:
1099:
1100:WITHIN_KERNEL INLINE void _module2_NTT2_pair(_module3_* r)
1101:{
1102:    _module2_NTT2(&amp;r[0], &amp;r[1]);
1103:}
1104:
1105:
1106:WITHIN_KERNEL INLINE void _module2_NTTInv2(_module3_* r0, _module3_* r1)
1107:{
1108:    _module2_NTT2(r0, r1);
1109:}
1110:
1111:
1112:WITHIN_KERNEL INLINE void _module2_NTTInv2_pair(_module3_* r)
1113:{
1114:    _module2_NTT2(&amp;r[0], &amp;r[1]);
1115:}
1116:
1117:
1118:WITHIN_KERNEL INLINE void _module2_NTT8(_module3_* r)
1119:{
1120:    _module2_NTT2(&amp;r[0], &amp;r[4]);
1121:    _module2_NTT2(&amp;r[1], &amp;r[5]);
1122:    _module2_NTT2(&amp;r[2], &amp;r[6]);
1123:    _module2_NTT2(&amp;r[3], &amp;r[7]);
1124:    r[5] = _module6_(r[5], 24);
1125:    r[6] = _module8_(r[6], 48);
1126:    r[7] = _module10_(r[7], 72);
1127:    // instead of calling NTT4 ...
1128:    _module2_NTT2(&amp;r[0], &amp;r[2]);
1129:    _module2_NTT2(&amp;r[1], &amp;r[3]);
1130:    r[3] = _module8_(r[3], 48);
1131:    _module2_NTT2(&amp;r[4], &amp;r[6]);
1132:    _module2_NTT2(&amp;r[5], &amp;r[7]);
1133:    r[7] = _module8_(r[7], 48);
1134:    _module2_NTT2_pair(&amp;r[0]);
1135:    _module2_NTT2_pair(&amp;r[2]);
1136:    _module2_NTT2_pair(&amp;r[4]);
1137:    _module2_NTT2_pair(&amp;r[6]);
1138:    // ... we save 2 swaps (otherwise 4) here
1139:    _module2_swap(&amp;r[1], &amp;r[4]);
1140:    _module2_swap(&amp;r[3], &amp;r[6]);
1141:}
1142:
1143:
1144:WITHIN_KERNEL INLINE void _module2_NTTInv8(_module3_* r)
1145:{
1146:    _module2_NTTInv2(&amp;r[0], &amp;r[4]);
1147:    _module2_NTTInv2(&amp;r[1], &amp;r[5]);
1148:    _module2_NTTInv2(&amp;r[2], &amp;r[6]);
1149:    _module2_NTTInv2(&amp;r[3], &amp;r[7]);
1150:    r[5] = _module12_(r[5], 168);
1151:    r[6] = _module14_(r[6], 144);
1152:    r[7] = _module16_(r[7], 120);
1153:    // instead of calling NTT4 ...
1154:    _module2_NTTInv2(&amp;r[0], &amp;r[2]);
1155:    _module2_NTTInv2(&amp;r[1], &amp;r[3]);
1156:    r[3] = _module14_(r[3], 144);
1157:    _module2_NTTInv2(&amp;r[4], &amp;r[6]);
1158:    _module2_NTTInv2(&amp;r[5], &amp;r[7]);
1159:    r[7] = _module14_(r[7], 144);
1160:    _module2_NTTInv2_pair(&amp;r[0]);
1161:    _module2_NTTInv2_pair(&amp;r[2]);
1162:    _module2_NTTInv2_pair(&amp;r[4]);
1163:    _module2_NTTInv2_pair(&amp;r[6]);
1164:    // ... we save 2 swaps (otherwise 4) here
1165:    _module2_swap(&amp;r[1], &amp;r[4]);
1166:    _module2_swap(&amp;r[3], &amp;r[6]);
1167:}
1168:
1169:
1170:WITHIN_KERNEL INLINE void _module2_NTT8x2Lsh_1(_module3_* s)
1171:{
1172:    s[1] = _module6_(s[1], 12);
1173:    s[2] = _module6_(s[2], 24);
1174:    s[3] = _module8_(s[3], 36);
1175:    s[4] = _module8_(s[4], 48);
1176:    s[5] = _module8_(s[5], 60);
1177:    s[6] = _module10_(s[6], 72);
1178:    s[7] = _module10_(s[7], 84);
1179:}
1180:
1181:
1182:WITHIN_KERNEL INLINE void _module2_NTT8x2Lsh(_module3_* s, unsigned int col)
1183:{
1184:    if (1 == col)
1185:        _module2_NTT8x2Lsh_1(s);
1186:}
1187:
1188:
1189:WITHIN_KERNEL INLINE void _module2_NTTInv8x2Lsh_1(_module3_* s)
1190:{
1191:    s[1] = _module12_(s[1], 180);
1192:    s[2] = _module12_(s[2], 168);
1193:    s[3] = _module14_(s[3], 156);
1194:    s[4] = _module14_(s[4], 144);
1195:    s[5] = _module14_(s[5], 132);
1196:    s[6] = _module16_(s[6], 120);
1197:    s[7] = _module16_(s[7], 108);
1198:}
1199:
1200:
1201:WITHIN_KERNEL INLINE void _module2_NTTInv8x2Lsh(_module3_* s, unsigned int col)
1202:{
1203:    if (1 == col)
1204:        _module2_NTTInv8x2Lsh_1(s);
1205:}
1206:
1207:
1208:WITHIN_KERNEL INLINE void _module2_NTT8x8Lsh_1(_module3_* s)
1209:{
1210:    s[1] = _module6_(s[1], 3);
1211:    s[2] = _module6_(s[2], 6);
1212:    s[3] = _module6_(s[3], 9);
1213:    s[4] = _module6_(s[4], 12);
1214:    s[5] = _module6_(s[5], 15);
1215:    s[6] = _module6_(s[6], 18);
1216:    s[7] = _module6_(s[7], 21);
1217:}
1218:WITHIN_KERNEL INLINE void _module2_NTT8x8Lsh_2(_module3_* s)
1219:{
1220:    s[1] = _module6_(s[1], 6);
1221:    s[2] = _module6_(s[2], 12);
1222:    s[3] = _module6_(s[3], 18);
1223:    s[4] = _module6_(s[4], 24);
1224:    s[5] = _module6_(s[5], 30);
1225:    s[6] = _module8_(s[6], 36);
1226:    s[7] = _module8_(s[7], 42);
1227:}
1228:WITHIN_KERNEL INLINE void _module2_NTT8x8Lsh_3(_module3_* s)
1229:{
1230:    s[1] = _module6_(s[1], 9);
1231:    s[2] = _module6_(s[2], 18);
1232:    s[3] = _module6_(s[3], 27);
1233:    s[4] = _module8_(s[4], 36);
1234:    s[5] = _module8_(s[5], 45);
1235:    s[6] = _module8_(s[6], 54);
1236:    s[7] = _module8_(s[7], 63);
1237:}
1238:WITHIN_KERNEL INLINE void _module2_NTT8x8Lsh_4(_module3_* s)
1239:{
1240:    s[1] = _module6_(s[1], 12);
1241:    s[2] = _module6_(s[2], 24);
1242:    s[3] = _module8_(s[3], 36);
1243:    s[4] = _module8_(s[4], 48);
1244:    s[5] = _module8_(s[5], 60);
1245:    s[6] = _module10_(s[6], 72);
1246:    s[7] = _module10_(s[7], 84);
1247:}
1248:WITHIN_KERNEL INLINE void _module2_NTT8x8Lsh_5(_module3_* s)
1249:{
1250:    s[1] = _module6_(s[1], 15);
1251:    s[2] = _module6_(s[2], 30);
1252:    s[3] = _module8_(s[3], 45);
1253:    s[4] = _module8_(s[4], 60);
1254:    s[5] = _module10_(s[5], 75);
1255:    s[6] = _module10_(s[6], 90);
1256:    s[7] = _module16_(s[7], 105);
1257:}
1258:WITHIN_KERNEL INLINE void _module2_NTT8x8Lsh_6(_module3_* s)
1259:{
1260:    s[1] = _module6_(s[1], 18);
1261:    s[2] = _module8_(s[2], 36);
1262:    s[3] = _module8_(s[3], 54);
1263:    s[4] = _module10_(s[4], 72);
1264:    s[5] = _module10_(s[5], 90);
1265:    s[6] = _module16_(s[6], 108);
1266:    s[7] = _module16_(s[7], 126);
1267:}
1268:WITHIN_KERNEL INLINE void _module2_NTT8x8Lsh_7(_module3_* s)
1269:{
1270:    s[1] = _module6_(s[1], 21);
1271:    s[2] = _module8_(s[2], 42);
1272:    s[3] = _module8_(s[3], 63);
1273:    s[4] = _module10_(s[4], 84);
1274:    s[5] = _module16_(s[5], 105);
1275:    s[6] = _module16_(s[6], 126);
1276:    s[7] = _module14_(s[7], 147);
1277:}
1278:
1279:
1280:WITHIN_KERNEL INLINE void _module2_NTT8x8Lsh(_module3_* s, unsigned int col)
1281:{
1282:    if (1 == col)
1283:        _module2_NTT8x8Lsh_1(s);
1284:    else if (2 == col)
1285:        _module2_NTT8x8Lsh_2(s);
1286:    else if (3 == col)
1287:        _module2_NTT8x8Lsh_3(s);
1288:    else if (4 == col)
1289:        _module2_NTT8x8Lsh_4(s);
1290:    else if (5 == col)
1291:        _module2_NTT8x8Lsh_5(s);
1292:    else if (6 == col)
1293:        _module2_NTT8x8Lsh_6(s);
1294:    else if (7 == col)
1295:        _module2_NTT8x8Lsh_7(s);
1296:}
1297:
1298:
1299:WITHIN_KERNEL INLINE void _module2_NTTInv8x8Lsh_1(_module3_* s)
1300:{
1301:    s[1] = _module12_(s[1], 189);
1302:    s[2] = _module12_(s[2], 186);
1303:    s[3] = _module12_(s[3], 183);
1304:    s[4] = _module12_(s[4], 180);
1305:    s[5] = _module12_(s[5], 177);
1306:    s[6] = _module12_(s[6], 174);
1307:    s[7] = _module12_(s[7], 171);
1308:}
1309:WITHIN_KERNEL INLINE void _module2_NTTInv8x8Lsh_2(_module3_* s)
1310:{
1311:    s[1] = _module12_(s[1], 186);
1312:    s[2] = _module12_(s[2], 180);
1313:    s[3] = _module12_(s[3], 174);
1314:    s[4] = _module12_(s[4], 168);
1315:    s[5] = _module12_(s[5], 162);
1316:    s[6] = _module14_(s[6], 156);
1317:    s[7] = _module14_(s[7], 150);
1318:}
1319:WITHIN_KERNEL INLINE void _module2_NTTInv8x8Lsh_3(_module3_* s)
1320:{
1321:    s[1] = _module12_(s[1], 183);
1322:    s[2] = _module12_(s[2], 174);
1323:    s[3] = _module12_(s[3], 165);
1324:    s[4] = _module14_(s[4], 156);
1325:    s[5] = _module14_(s[5], 147);
1326:    s[6] = _module14_(s[6], 138);
1327:    s[7] = _module14_(s[7], 129);
1328:}
1329:WITHIN_KERNEL INLINE void _module2_NTTInv8x8Lsh_4(_module3_* s)
1330:{
1331:    s[1] = _module12_(s[1], 180);
1332:    s[2] = _module12_(s[2], 168);
1333:    s[3] = _module14_(s[3], 156);
1334:    s[4] = _module14_(s[4], 144);
1335:    s[5] = _module14_(s[5], 132);
1336:    s[6] = _module16_(s[6], 120);
1337:    s[7] = _module16_(s[7], 108);
1338:}
1339:WITHIN_KERNEL INLINE void _module2_NTTInv8x8Lsh_5(_module3_* s)
1340:{
1341:    s[1] = _module12_(s[1], 177);
1342:    s[2] = _module12_(s[2], 162);
1343:    s[3] = _module14_(s[3], 147);
1344:    s[4] = _module14_(s[4], 132);
1345:    s[5] = _module16_(s[5], 117);
1346:    s[6] = _module16_(s[6], 102);
1347:    s[7] = _module10_(s[7], 87);
1348:}
1349:WITHIN_KERNEL INLINE void _module2_NTTInv8x8Lsh_6(_module3_* s)
1350:{
1351:    s[1] = _module12_(s[1], 174);
1352:    s[2] = _module14_(s[2], 156);
1353:    s[3] = _module14_(s[3], 138);
1354:    s[4] = _module16_(s[4], 120);
1355:    s[5] = _module16_(s[5], 102);
1356:    s[6] = _module10_(s[6], 84);
1357:    s[7] = _module10_(s[7], 66);
1358:}
1359:WITHIN_KERNEL INLINE void _module2_NTTInv8x8Lsh_7(_module3_* s)
1360:{
1361:    s[1] = _module12_(s[1], 171);
1362:    s[2] = _module14_(s[2], 150);
1363:    s[3] = _module14_(s[3], 129);
1364:    s[4] = _module16_(s[4], 108);
1365:    s[5] = _module10_(s[5], 87);
1366:    s[6] = _module10_(s[6], 66);
1367:    s[7] = _module8_(s[7], 45);
1368:}
1369:
1370:
1371:WITHIN_KERNEL INLINE void _module2_NTTInv8x8Lsh(_module3_* s, unsigned int col)
1372:{
1373:    if (1 == col)
1374:        _module2_NTTInv8x8Lsh_1(s);
1375:    else if (2 == col)
1376:        _module2_NTTInv8x8Lsh_2(s);
1377:    else if (3 == col)
1378:        _module2_NTTInv8x8Lsh_3(s);
1379:    else if (4 == col)
1380:        _module2_NTTInv8x8Lsh_4(s);
1381:    else if (5 == col)
1382:        _module2_NTTInv8x8Lsh_5(s);
1383:    else if (6 == col)
1384:        _module2_NTTInv8x8Lsh_6(s);
1385:    else if (7 == col)
1386:        _module2_NTTInv8x8Lsh_7(s);
1387:}
1388:
1389:
1390:WITHIN_KERNEL INLINE void _module2_Index3DFrom1D(uint3 *t3d, unsigned int t1d, unsigned int dim_x, unsigned int dim_y, unsigned int dim_z)
1391:{
1392:    t3d-&gt;x = t1d % dim_x;
1393:    t1d /= dim_x;
1394:    t3d-&gt;y = t1d % dim_y;
1395:    t3d-&gt;z = t1d / dim_y;
1396:}
1397:
1398:
1399:WITHIN_KERNEL INLINE void _module2__forward(
1400:        _module3_* r,
1401:        LOCAL_MEM_ARG _module3_* s,
1402:        _module2_CDATA_QUALIFIER _module3_* twd,
1403:        const unsigned int t1d)
1404:{
1405:    uint3 t3d;
1406:    _module2_Index3DFrom1D(&amp;t3d, t1d, 8, 8, 2);
1407:
1408:    LOCAL_MEM_ARG _module3_* ptr;
1409:
1410:    _module2_NTT8(r);
1411:    _module2_NTT8x2Lsh(r, t3d.z);
1412:    ptr = &amp;s[(t3d.y &lt;&lt; 7) | (t3d.z &lt;&lt; 6) | (t3d.x &lt;&lt; 2)];
1413:    #pragma unroll
1414:    for (unsigned int i = 0; i &lt; 8; i ++)
1415:        ptr[(i &gt;&gt; 2 &lt;&lt; 5) | (i &amp; 0x3)] = r[i];
1416:    LOCAL_BARRIER;
1417:
1418:    ptr = &amp;s[(t3d.z &lt;&lt; 9) | (t3d.y &lt;&lt; 3) | t3d.x];
1419:    #pragma unroll
1420:    for (unsigned int i = 0; i &lt; 8; i ++)
1421:        r[i] = ptr[i &lt;&lt; 6];
1422:    _module2_NTT2_pair(r);
1423:    _module2_NTT2_pair(r + 2);
1424:    _module2_NTT2_pair(r + 4);
1425:    _module2_NTT2_pair(r + 6);
1426:    #pragma unroll
1427:    for (unsigned int i = 0; i &lt; 8; i ++)
1428:        ptr[i &lt;&lt; 6] = r[i];
1429:    LOCAL_BARRIER;
1430:
1431:    ptr = &amp;s[t1d];
1432:    #pragma unroll
1433:    for (unsigned int i = 0; i &lt; 8; i ++)
1434:        r[i] = _module18_(ptr[i &lt;&lt; 7], twd[i &lt;&lt; 7 | t1d]); // mult twiddle
1435:    _module2_NTT8(r);
1436:    #pragma unroll
1437:    for (unsigned int i = 0; i &lt; 8; i ++)
1438:        ptr[i &lt;&lt; 7] = r[i];
1439:    LOCAL_BARRIER;
1440:
1441:    ptr = &amp;s[(t1d &gt;&gt; 2 &lt;&lt; 5) | (t3d.x &amp; 0x3)];
1442:    #pragma unroll
1443:    for (unsigned int i = 0; i &lt; 8; i ++)
1444:        r[i] = ptr[i &lt;&lt; 2];
1445:    _module2_NTT8x8Lsh(r, t1d &gt;&gt; 4); // less divergence if put here!
1446:    _module2_NTT8(r);
1447:}
1448:
1449:
1450:WITHIN_KERNEL INLINE void _module2__inverse(
1451:        _module3_* r,
1452:        LOCAL_MEM_ARG _module3_* s,
1453:        _module2_CDATA_QUALIFIER _module3_* twd,
1454:        const unsigned int t1d)
1455:{
1456:    uint3 t3d;
1457:    _module2_Index3DFrom1D(&amp;t3d, t1d, 8, 8, 2);
1458:
1459:    LOCAL_MEM_ARG _module3_* ptr;
1460:
1461:    _module2_NTTInv8(r);
1462:    _module2_NTTInv8x2Lsh(r, t3d.z);
1463:    ptr = &amp;s[(t3d.y &lt;&lt; 7) | (t3d.z &lt;&lt; 6) | (t3d.x &lt;&lt; 2)];
1464:    #pragma unroll
1465:    for (unsigned int i = 0; i &lt; 8; i ++)
1466:        ptr[(i &gt;&gt; 2 &lt;&lt; 5) | (i &amp; 0x3)] = r[i];
1467:    LOCAL_BARRIER;
1468:
1469:    ptr = &amp;s[(t3d.z &lt;&lt; 9) | (t3d.y &lt;&lt; 3) | t3d.x];
1470:    #pragma unroll
1471:    for (unsigned int i = 0; i &lt; 8; i ++)
1472:        r[i] = ptr[i &lt;&lt; 6];
1473:    _module2_NTT2_pair(r);
1474:    _module2_NTT2_pair(r + 2);
1475:    _module2_NTT2_pair(r + 4);
1476:    _module2_NTT2_pair(r + 6);
1477:    #pragma unroll
1478:    for (unsigned int i = 0; i &lt; 8; i ++)
1479:        ptr[i &lt;&lt; 6] = r[i];
1480:    LOCAL_BARRIER;
1481:
1482:    ptr = &amp;s[t1d];
1483:    #pragma unroll
1484:    for (unsigned int i = 0; i &lt; 8; i ++)
1485:        r[i] = _module18_(ptr[i &lt;&lt; 7], twd[i &lt;&lt; 7 | t1d]); // mult twiddle
1486:    _module2_NTTInv8(r);
1487:    #pragma unroll
1488:    for (unsigned int i = 0; i &lt; 8; i ++)
1489:        ptr[i &lt;&lt; 7] = r[i];
1490:    LOCAL_BARRIER;
1491:
1492:    ptr = &amp;s[(t1d &gt;&gt; 2 &lt;&lt; 5) | (t3d.x &amp; 0x3)];
1493:    #pragma unroll
1494:        for (unsigned int i = 0; i &lt; 8; i ++)
1495:    r[i] = ptr[i &lt;&lt; 2];
1496:    _module2_NTTInv8x8Lsh(r, t1d &gt;&gt; 4); // less divergence if put here!
1497:    _module2_NTTInv8(r);
1498:}
1499:
1500:
1501:WITHIN_KERNEL INLINE void _module2_forward(
1502:        _module3_* r_out,
1503:        _module3_* r_in,
1504:        LOCAL_MEM_ARG _module3_* temp,
1505:        _module2_CDATA_QUALIFIER _module3_* cdata,
1506:        unsigned int thread_in_xform)
1507:{
1508:    // Preprocess
1509:    r_out[0] = _module18_(
1510:        r_in[0],
1511:        cdata[1024 + 0 + thread_in_xform]
1512:        );
1513:    r_out[1] = _module18_(
1514:        r_in[1],
1515:        cdata[1024 + 128 + thread_in_xform]
1516:        );
1517:    r_out[2] = _module18_(
1518:        r_in[2],
1519:        cdata[1024 + 256 + thread_in_xform]
1520:        );
1521:    r_out[3] = _module18_(
1522:        r_in[3],
1523:        cdata[1024 + 384 + thread_in_xform]
1524:        );
1525:    r_out[4] = _module18_(
1526:        r_in[4],
1527:        cdata[1024 + 512 + thread_in_xform]
1528:        );
1529:    r_out[5] = _module18_(
1530:        r_in[5],
1531:        cdata[1024 + 640 + thread_in_xform]
1532:        );
1533:    r_out[6] = _module18_(
1534:        r_in[6],
1535:        cdata[1024 + 768 + thread_in_xform]
1536:        );
1537:    r_out[7] = _module18_(
1538:        r_in[7],
1539:        cdata[1024 + 896 + thread_in_xform]
1540:        );
1541:
1542:    _module2__forward(r_out, temp, cdata, thread_in_xform);
1543:}
1544:
1545:
1546:WITHIN_KERNEL INLINE void _module2_inverse(
1547:        _module3_* r_out,
1548:        _module3_* r_in,
1549:        LOCAL_MEM_ARG _module3_* temp,
1550:        _module2_CDATA_QUALIFIER _module3_* cdata,
1551:        unsigned int thread_in_xform)
1552:{
1553:    _module2__inverse(r_in, temp, cdata, thread_in_xform);
1554:
1555:    // Postprocess
1556:    r_out[0] = _module18_(
1557:        r_in[0],
1558:        cdata[1024 + 0 + thread_in_xform]
1559:        );
1560:    r_out[1] = _module18_(
1561:        r_in[1],
1562:        cdata[1024 + 128 + thread_in_xform]
1563:        );
1564:    r_out[2] = _module18_(
1565:        r_in[2],
1566:        cdata[1024 + 256 + thread_in_xform]
1567:        );
1568:    r_out[3] = _module18_(
1569:        r_in[3],
1570:        cdata[1024 + 384 + thread_in_xform]
1571:        );
1572:    r_out[4] = _module18_(
1573:        r_in[4],
1574:        cdata[1024 + 512 + thread_in_xform]
1575:        );
1576:    r_out[5] = _module18_(
1577:        r_in[5],
1578:        cdata[1024 + 640 + thread_in_xform]
1579:        );
1580:    r_out[6] = _module18_(
1581:        r_in[6],
1582:        cdata[1024 + 768 + thread_in_xform]
1583:        );
1584:    r_out[7] = _module18_(
1585:        r_in[7],
1586:        cdata[1024 + 896 + thread_in_xform]
1587:        );
1588:}
1589:
1590:
1591:WITHIN_KERNEL INLINE _module3_ _module2_i32_to_elem(int x)
1592:{
1593:    _module3_ res = { (unsigned long)x - (unsigned int)(-(x &lt; 0)) };
1594:    return res;
1595:}
1596:
1597:
1598:WITHIN_KERNEL INLINE int _module2_ff_to_i32(_module3_ x)
1599:{
1600:    // Interpreting anything &gt; P/2 as a negative integer,
1601:    // then taking modulo 2^31
1602:    const unsigned long med = 18446744069414584321UL / 2;
1603:    return (int)(x.val) - (x.val &gt; med);
1604:}
1605:
1606:
1607:
1608:WITHIN_KERNEL INLINE void _module2_forward_i32(
1609:        _module3_* r_out,
1610:        int* r_in,
1611:        LOCAL_MEM_ARG _module3_* temp,
1612:        _module2_CDATA_QUALIFIER _module3_* cdata,
1613:        unsigned int thread_in_xform)
1614:{
1615:    r_out[0] = _module2_i32_to_elem(r_in[0]);
1616:    r_out[1] = _module2_i32_to_elem(r_in[1]);
1617:    r_out[2] = _module2_i32_to_elem(r_in[2]);
1618:    r_out[3] = _module2_i32_to_elem(r_in[3]);
1619:    r_out[4] = _module2_i32_to_elem(r_in[4]);
1620:    r_out[5] = _module2_i32_to_elem(r_in[5]);
1621:    r_out[6] = _module2_i32_to_elem(r_in[6]);
1622:    r_out[7] = _module2_i32_to_elem(r_in[7]);
1623:    _module2_forward(r_out, r_out, temp, cdata, thread_in_xform);
1624:}
1625:
1626:
1627:WITHIN_KERNEL INLINE void _module2_inverse_i32(
1628:        int* r_out,
1629:        _module3_* r_in,
1630:        LOCAL_MEM_ARG _module3_* temp,
1631:        _module2_CDATA_QUALIFIER _module3_* cdata,
1632:        unsigned int thread_in_xform)
1633:{
1634:    _module2_inverse(r_in, r_in, temp, cdata, thread_in_xform);
1635:    r_out[0] = _module2_ff_to_i32(r_in[0]);
1636:    r_out[1] = _module2_ff_to_i32(r_in[1]);
1637:    r_out[2] = _module2_ff_to_i32(r_in[2]);
1638:    r_out[3] = _module2_ff_to_i32(r_in[3]);
1639:    r_out[4] = _module2_ff_to_i32(r_in[4]);
1640:    r_out[5] = _module2_ff_to_i32(r_in[5]);
1641:    r_out[6] = _module2_ff_to_i32(r_in[6]);
1642:    r_out[7] = _module2_ff_to_i32(r_in[7]);
1643:}
1644:
1645:
1646:WITHIN_KERNEL INLINE void _module2_noop()
1647:{
1648:    LOCAL_BARRIER;
1649:    LOCAL_BARRIER;
1650:    LOCAL_BARRIER;
1651:}
1652:
1653:
1654:WITHIN_KERNEL INLINE void _module2_forward_i32_shared(
1655:        LOCAL_MEM_ARG _module3_* in_out,
1656:        LOCAL_MEM_ARG _module3_* temp,
1657:        _module2_CDATA_QUALIFIER _module3_* cdata,
1658:        unsigned int thread_in_xform)
1659:{
1660:    _module3_ r[8];
1661:    r[0] = in_out[0 + thread_in_xform];
1662:    r[1] = in_out[128 + thread_in_xform];
1663:    r[2] = in_out[256 + thread_in_xform];
1664:    r[3] = in_out[384 + thread_in_xform];
1665:    r[4] = in_out[512 + thread_in_xform];
1666:    r[5] = in_out[640 + thread_in_xform];
1667:    r[6] = in_out[768 + thread_in_xform];
1668:    r[7] = in_out[896 + thread_in_xform];
1669:    LOCAL_BARRIER;
1670:    _module2_forward(r, r, temp, cdata, thread_in_xform);
1671:    LOCAL_BARRIER;
1672:    in_out[0 + thread_in_xform] = r[0];
1673:    in_out[128 + thread_in_xform] = r[1];
1674:    in_out[256 + thread_in_xform] = r[2];
1675:    in_out[384 + thread_in_xform] = r[3];
1676:    in_out[512 + thread_in_xform] = r[4];
1677:    in_out[640 + thread_in_xform] = r[5];
1678:    in_out[768 + thread_in_xform] = r[6];
1679:    in_out[896 + thread_in_xform] = r[7];
1680:}
1681:
1682:
1683:WITHIN_KERNEL INLINE void _module2_inverse_i32_shared_add(
1684:        LOCAL_MEM_ARG int* out,
1685:        LOCAL_MEM_ARG _module3_* in,
1686:        LOCAL_MEM_ARG _module3_* temp,
1687:        _module2_CDATA_QUALIFIER _module3_* cdata,
1688:        unsigned int thread_in_xform)
1689:{
1690:    _module3_ r[8];
1691:    r[0] = in[0 + thread_in_xform];
1692:    r[1] = in[128 + thread_in_xform];
1693:    r[2] = in[256 + thread_in_xform];
1694:    r[3] = in[384 + thread_in_xform];
1695:    r[4] = in[512 + thread_in_xform];
1696:    r[5] = in[640 + thread_in_xform];
1697:    r[6] = in[768 + thread_in_xform];
1698:    r[7] = in[896 + thread_in_xform];
1699:    LOCAL_BARRIER;
1700:    _module2_inverse(r, r, temp, cdata, thread_in_xform);
1701:    LOCAL_BARRIER;
1702:    out[0 + thread_in_xform] += _module2_ff_to_i32(r[0]);
1703:    out[128 + thread_in_xform] += _module2_ff_to_i32(r[1]);
1704:    out[256 + thread_in_xform] += _module2_ff_to_i32(r[2]);
1705:    out[384 + thread_in_xform] += _module2_ff_to_i32(r[3]);
1706:    out[512 + thread_in_xform] += _module2_ff_to_i32(r[4]);
1707:    out[640 + thread_in_xform] += _module2_ff_to_i32(r[5]);
1708:    out[768 + thread_in_xform] += _module2_ff_to_i32(r[6]);
1709:    out[896 + thread_in_xform] += _module2_ff_to_i32(r[7]);
1710:}
1711:
1712:
1713:WITHIN_KERNEL INLINE void _module2_noop_shared()
1714:{
1715:    LOCAL_BARRIER;
1716:    _module2_noop();
1717:    LOCAL_BARRIER;
1718:}
1719:
1720:
1721:
1722:
1723:
1724:    // leaf output macro for &quot;_temp2&quot;
1725:    #define _module21_(_idx0, _idx1, _idx2, _idx3, _idx4, _val) _leaf__temp2[(_idx0) * (4096) + (_idx1) * (2048) + (_idx2) * (1024) + (_idx3) * (1024) + (_idx4) * (1) + (0)] = (_val)
1726:    
1727:
1728:
1729:
1730:
1731:    
1732:    INLINE WITHIN_KERNEL void _module20_func(
1733:        GLOBAL_MEM unsigned long *_leaf__temp2, VSIZE_T _c_idx0, VSIZE_T _c_idx1, unsigned long _val)
1734:    {
1735:        
1736:
1737:    
1738:        
1739:        VSIZE_T _idx0 = _c_idx0 / 4;
1740:        _c_idx0 -= _idx0 * 4;
1741:        
1742:        VSIZE_T _idx1 = _c_idx0 / 2;
1743:        _c_idx0 -= _idx1 * 2;
1744:        
1745:        VSIZE_T _idx2 = _c_idx0 / 1;
1746:        _c_idx0 -= _idx2 * 1;
1747:        
1748:        VSIZE_T _idx3 = _c_idx0 / 1;
1749:    
1750:        
1751:        VSIZE_T _idx4 = _c_idx1 / 1;
1752:    
1753:
1754:
1755:        _module21_(_idx0, _idx1, _idx2, _idx3, _idx4, _val);
1756:    }
1757:    
1758:    #define _module20_(_c_idx0, _c_idx1, _val) _module20_func(        _leaf__temp2, _c_idx0, _c_idx1, _val)
1759:    
1760:
1761:
1762:
1763:
1764:
1765:
1766:
1767:KERNEL void standalone_transform(GLOBAL_MEM unsigned long *_leaf__temp2, GLOBAL_MEM int *_leaf_noises1, GLOBAL_MEM unsigned long *_leaf__nested2__value1)
1768:
1769:{
1770:    VIRTUAL_SKIP_THREADS;
1771:
1772:    LOCAL_MEM unsigned long temp[1024 * 1];
1773:
1774:    int r_in[8];
1775:    unsigned long r_out[8];
1776:
1777:    VSIZE_T batch_id = virtual_group_id(0);
1778:    VSIZE_T tid = virtual_local_id(1);
1779:    int transform_in_block = tid / 128;
1780:    int thread_in_transform = tid % 128;
1781:
1782:
1783:    #pragma unroll
1784:    for (int i = 0; i &lt; 8; i++)
1785:    {
1786:        r_in[i] = _module0_(
1787:            batch_id * 1 + transform_in_block, thread_in_transform + i * 128);
1788:    }
1789:
1790:
1791:        LOCAL_MEM_ARG unsigned long* transform_temp =
1792:            temp + 1024 * transform_in_block;
1793:                _module2_forward_i32(
1794:            (_module3_*)r_out,
1795:            (int*)r_in,
1796:            (LOCAL_MEM_ARG _module3_*)transform_temp,
1797:            (_module2_CDATA_QUALIFIER _module3_*)_leaf__nested2__value1,
1798:            thread_in_transform);
1799:
1800:
1801:    #pragma unroll
1802:    for (int i = 0; i &lt; 8; i++)
1803:    {
1804:        _module20_(
1805:            batch_id * 1 + transform_in_block, thread_in_transform + i * 128,
1806:            r_out[i]);
1807:    }
1808:}
1809:
1810:<br/></div></td></tr></tbody>
      <tbody class="error results-table-row">
        <tr>
          <td class="col-result">Error</td>
          <td class="col-name">test/test_gates.py::test_xor_gate[cuda:0:0]::setup</td>
          <td class="col-duration">0.08</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f882724e0&gt;<br/><br/>    @pytest.fixture(scope=&#x27;session&#x27;)<br/>    def key_pair(thread):<br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng)<br/><br/>test/conftest.py:120: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f882724e0&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="error results-table-row">
        <tr>
          <td class="col-result">Error</td>
          <td class="col-name">test/test_gates.py::test_not_gate[cuda:0:0]::setup</td>
          <td class="col-duration">0.07</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f881e7780&gt;<br/><br/>    @pytest.fixture(scope=&#x27;session&#x27;)<br/>    def key_pair(thread):<br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng)<br/><br/>test/conftest.py:120: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f881e7780&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="error results-table-row">
        <tr>
          <td class="col-result">Error</td>
          <td class="col-name">test/test_gates.py::test_nor_gate[cuda:0:0]::setup</td>
          <td class="col-duration">0.06</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f884614e0&gt;<br/><br/>    @pytest.fixture(scope=&#x27;session&#x27;)<br/>    def key_pair(thread):<br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng)<br/><br/>test/conftest.py:120: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f884614e0&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="error results-table-row">
        <tr>
          <td class="col-result">Error</td>
          <td class="col-name">test/test_gates.py::test_andyn_gate[cuda:0:0]::setup</td>
          <td class="col-duration">0.08</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f82fa3ba8&gt;<br/><br/>    @pytest.fixture(scope=&#x27;session&#x27;)<br/>    def key_pair(thread):<br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng)<br/><br/>test/conftest.py:120: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f82fa3ba8&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="error results-table-row">
        <tr>
          <td class="col-result">Error</td>
          <td class="col-name">test/test_gates.py::test_oryn_gate[cuda:0:0]::setup</td>
          <td class="col-duration">0.08</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f82d9a860&gt;<br/><br/>    @pytest.fixture(scope=&#x27;session&#x27;)<br/>    def key_pair(thread):<br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng)<br/><br/>test/conftest.py:120: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f82d9a860&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="error results-table-row">
        <tr>
          <td class="col-result">Error</td>
          <td class="col-name">test/test_gates.py::test_constant_gate[cuda:0:0]::setup</td>
          <td class="col-duration">0.08</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f82f130b8&gt;<br/><br/>    @pytest.fixture(scope=&#x27;session&#x27;)<br/>    def key_pair(thread):<br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng)<br/><br/>test/conftest.py:120: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f82f130b8&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_tlwe_mask_size[cuda:0:0-mask_size=1]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f88409828&gt;, tlwe_mask_size = 1<br/><br/>    @pytest.mark.parametrize(&#x27;tlwe_mask_size&#x27;, [1, 2], ids=[&#x27;mask_size=1&#x27;, &#x27;mask_size=2&#x27;])<br/>    def test_tlwe_mask_size(thread, tlwe_mask_size):<br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, tlwe_mask_size=tlwe_mask_size)<br/><br/>test/test_gates.py:99: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f88409828&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_tlwe_mask_size[cuda:0:0-mask_size=2]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f88409828&gt;, tlwe_mask_size = 2<br/><br/>    @pytest.mark.parametrize(&#x27;tlwe_mask_size&#x27;, [1, 2], ids=[&#x27;mask_size=1&#x27;, &#x27;mask_size=2&#x27;])<br/>    def test_tlwe_mask_size(thread, tlwe_mask_size):<br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, tlwe_mask_size=tlwe_mask_size)<br/><br/>test/test_gates.py:99: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f88409828&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_single_kernel_bs_performance[bs_loop-cuda:0:0-NTT-NAND]</td>
          <td class="col-duration">7.92</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1a20&gt;, transform_type = &#x27;NTT&#x27;<br/>single_kernel_bootstrap = False, test_function_name = &#x27;NAND&#x27;, heavy_performance_load = False<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(&#x27;test_function_name&#x27;, [&#x27;NAND&#x27;, &#x27;MUX&#x27;, &#x27;uint_min&#x27;])<br/>    def test_single_kernel_bs_performance(<br/>            thread, transform_type, single_kernel_bootstrap,<br/>            test_function_name, heavy_performance_load):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        test_function = dict(<br/>            NAND=(gate_nand, nand_ref, 2),<br/>            MUX=(gate_mux, mux_ref, 3),<br/>            uint_min=(uint_min, uint_min_ref, 2),<br/>            )[test_function_name]<br/>    <br/>        if test_function_name == &#x27;uint_min&#x27;:<br/>            shape = (128, 32) if heavy_performance_load else (4, 16)<br/>        else:<br/>            shape = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>        secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/>    <br/>        # TODO: instead of creating a whole key and then checking if the parameters are supported,<br/>        # we can just create a parameter object separately.<br/>        if (single_kernel_bootstrap<br/>                and not single_kernel_bootstrap_supported(secret_key.params, thread.device_params)):<br/>            pytest.skip()<br/>    <br/>        perf_params = PerformanceParameters(<br/>            secret_key.params, single_kernel_bootstrap=single_kernel_bootstrap)<br/>        perf_params = perf_params.for_device(thread.device_params)<br/>    <br/>        results = check_performance(<br/>&gt;           thread, (secret_key, cloud_key), perf_params, shape=shape, test_function=test_function)<br/><br/>test/test_gates.py:351: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_gates.py:276: in check_performance<br/>    shape=shape1, performance_test=True, perf_params=perf_params)<br/>test/test_gates.py:65: in check_gate<br/>    nufhe_func(thread, cloud_key, answer, *ciphertexts, perf_params)<br/>nufhe/gates.py:121: in gate_nand<br/>    MU, temp_result, perf_params)<br/>nufhe/bootstrap.py:220: in bootstrap<br/>    t32_to_phase(thr, barb, x.b, 2 * N)<br/>nufhe/numeric_functions.py:35: in t32_to_phase<br/>    comp = get_computation(thr, Torus32ToPhase, messages.shape, mspace_size)<br/>nufhe/computation_cache.py:55: in get_computation<br/>    compiled_comp = comp.compile(thr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/numeric_functions_gpu.py:75: in _build_plan<br/>    result, phase)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:502: in computation_call<br/>    self._compiler_options, self._keep))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/algorithms/pureparallel.py:123: in _build_plan<br/>    snippet=self._snippet))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:473: in kernel_call<br/>    keep=self._keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:566: in compile_static<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:786: in __init__<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:655: in __init__<br/>    self.source, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:504: in _create_program<br/>    src, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:227: in _compile<br/>    return SourceModule(src, no_extern_c=True, options=options, keep=keep)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;pycuda.compiler.SourceModule object at 0x7f884883c8&gt;<br/>source = &#x27;\n\n\n    #define CUDA\n    // taken from pycuda._cluda\n    #define LOCAL_BARRIER __syncthreads()\n\n    #define WIT...e0_;\n            _module2_(((unsigned int)phase + 1048576) / 2097152);\n            \n\n            }\n            \n&#x27;<br/>nvcc = &#x27;nvcc&#x27;, options = [], keep = False, no_extern_c = True, arch = None, code = None<br/>cache_dir = None, include_dirs = []<br/><br/>    def __init__(self, source, nvcc=&quot;nvcc&quot;, options=None, keep=False,<br/>            no_extern_c=False, arch=None, code=None, cache_dir=None,<br/>            include_dirs=[]):<br/>        self._check_arch(arch)<br/>    <br/>        cubin = compile(source, nvcc, options, keep, no_extern_c,<br/>                arch, code, cache_dir, include_dirs)<br/>    <br/>        from pycuda.driver import module_from_buffer<br/>&gt;       self.module = module_from_buffer(cubin)<br/><span class="error">E       pycuda._driver.LogicError</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/compiler.py:294: LogicError<br/> -------------------------------Captured log call-------------------------------- <br/>[1m[31mERROR   [0m root:api.py:507 Failed to compile:
1:
2:
3:
4:    #define CUDA
5:    // taken from pycuda._cluda
6:    #define LOCAL_BARRIER __syncthreads()
7:
8:    #define WITHIN_KERNEL __device__
9:    #define KERNEL extern &quot;C&quot; __global__
10:    #define GLOBAL_MEM /* empty */
11:    #define GLOBAL_MEM_ARG /* empty */
12:    #define LOCAL_MEM __shared__
13:    #define LOCAL_MEM_DYNAMIC extern __shared__
14:    #define LOCAL_MEM_ARG /* empty */
15:    #define CONSTANT_MEM __constant__
16:    #define CONSTANT_MEM_ARG /* empty */
17:    #define INLINE __forceinline__
18:    #define SIZE_T int
19:    #define VSIZE_T int
20:
21:    // used to align fields in structures
22:    #define ALIGN(bytes) __align__(bytes)
23:
24:    
25:
26:    WITHIN_KERNEL SIZE_T get_local_id(unsigned int dim)
27:    {
28:        if(dim == 0) return threadIdx.x;
29:        if(dim == 1) return threadIdx.y;
30:        if(dim == 2) return threadIdx.z;
31:        return 0;
32:    }
33:
34:    WITHIN_KERNEL SIZE_T get_group_id(unsigned int dim)
35:    {
36:        if(dim == 0) return blockIdx.x;
37:        if(dim == 1) return blockIdx.y;
38:        if(dim == 2) return blockIdx.z;
39:        return 0;
40:    }
41:
42:    WITHIN_KERNEL SIZE_T get_local_size(unsigned int dim)
43:    {
44:        if(dim == 0) return blockDim.x;
45:        if(dim == 1) return blockDim.y;
46:        if(dim == 2) return blockDim.z;
47:        return 1;
48:    }
49:
50:    WITHIN_KERNEL SIZE_T get_num_groups(unsigned int dim)
51:    {
52:        if(dim == 0) return gridDim.x;
53:        if(dim == 1) return gridDim.y;
54:        if(dim == 2) return gridDim.z;
55:        return 1;
56:    }
57:
58:    WITHIN_KERNEL SIZE_T get_global_size(unsigned int dim)
59:    {
60:        return get_num_groups(dim) * get_local_size(dim);
61:    }
62:
63:    WITHIN_KERNEL SIZE_T get_global_id(unsigned int dim)
64:    {
65:        return get_local_id(dim) + get_group_id(dim) * get_local_size(dim);
66:    }
67:
68:
69:
70:
71:    #define COMPLEX_CTR(T) make_##T
72:
73:    WITHIN_KERNEL float2 operator+(float2 a, float2 b)
74:    {
75:        return COMPLEX_CTR(float2)(a.x + b.x, a.y + b.y);
76:    }
77:    WITHIN_KERNEL float2 operator-(float2 a, float2 b)
78:    {
79:        return COMPLEX_CTR(float2)(a.x - b.x, a.y - b.y);
80:    }
81:    WITHIN_KERNEL float2 operator+(float2 a) { return a; }
82:    WITHIN_KERNEL float2 operator-(float2 a) { return COMPLEX_CTR(float2)(-a.x, -a.y); }
83:    WITHIN_KERNEL double2 operator+(double2 a, double2 b)
84:    {
85:        return COMPLEX_CTR(double2)(a.x + b.x, a.y + b.y);
86:    }
87:    WITHIN_KERNEL double2 operator-(double2 a, double2 b)
88:    {
89:        return COMPLEX_CTR(double2)(a.x - b.x, a.y - b.y);
90:    }
91:    WITHIN_KERNEL double2 operator+(double2 a) { return a; }
92:    WITHIN_KERNEL double2 operator-(double2 a) { return COMPLEX_CTR(double2)(-a.x, -a.y); }
93:
94:WITHIN_KERNEL VSIZE_T virtual_local_id(unsigned int dim)
95:{
96:    if (dim == 0)
97:    {
98:
99:        SIZE_T flat_id =
100:            get_local_id(0) * 1 +
101:            0;
102:
103:        return (flat_id / 1);
104:
105:    }
106:
107:    return 0;
108:}
109:
110:WITHIN_KERNEL VSIZE_T virtual_local_size(unsigned int dim)
111:{
112:    if (dim == 0)
113:    {
114:        return 64;
115:    }
116:
117:    return 1;
118:}
119:
120:WITHIN_KERNEL VSIZE_T virtual_group_id(unsigned int dim)
121:{
122:    if (dim == 0)
123:    {
124:
125:        return 0;
126:
127:    }
128:
129:    return 0;
130:}
131:
132:WITHIN_KERNEL VSIZE_T virtual_num_groups(unsigned int dim)
133:{
134:    if (dim == 0)
135:    {
136:        return 1;
137:    }
138:
139:    return 1;
140:}
141:
142:WITHIN_KERNEL VSIZE_T virtual_global_id(unsigned int dim)
143:{
144:    return virtual_local_id(dim) + virtual_group_id(dim) * virtual_local_size(dim);
145:}
146:
147:WITHIN_KERNEL VSIZE_T virtual_global_size(unsigned int dim)
148:{
149:    if(dim == 0)
150:    {
151:        return 64;
152:    }
153:
154:    return 1;
155:}
156:
157:WITHIN_KERNEL VSIZE_T virtual_global_flat_id()
158:{
159:    return
160:        virtual_global_id(0) * 1 +
161:        0;
162:}
163:
164:WITHIN_KERNEL VSIZE_T virtual_global_flat_size()
165:{
166:    return
167:        virtual_global_size(0) *
168:        1;
169:}
170:
171:
172:WITHIN_KERNEL bool virtual_skip_local_threads()
173:{
174:
175:    return false;
176:}
177:
178:WITHIN_KERNEL bool virtual_skip_groups()
179:{
180:
181:    return false;
182:}
183:
184:WITHIN_KERNEL bool virtual_skip_global_threads()
185:{
186:
187:    return false;
188:}
189:
190:
191:
192:#ifndef CUDA
193:#define MARK_VIRTUAL_FUNCTIONS_AS_USED (void)(virtual_num_groups(0)); (void)(virtual_global_flat_id()); (void)(virtual_global_flat_size())
194:#else
195:#define MARK_VIRTUAL_FUNCTIONS_AS_USED
196:#endif
197:
198:#define VIRTUAL_SKIP_THREADS MARK_VIRTUAL_FUNCTIONS_AS_USED; if(virtual_skip_local_threads() || virtual_skip_groups() || virtual_skip_global_threads()) return
199:
200:
201:    // leaf input macro for &quot;messages&quot;
202:    #define _module1_(_idx0) (_leaf_messages[(_idx0) * (1) + (0)])
203:    
204:
205:
206:
207:
208:    // input for a transformation for &quot;messages&quot;
209:    #define _module0_ _module1_(_idx0)
210:    
211:
212:
213:
214:
215:    // leaf output macro for &quot;result&quot;
216:    #define _module3_(_idx0, _val) _leaf_result[(_idx0) * (1) + (0)] = (_val)
217:    
218:
219:
220:
221:
222:    // output for a transformation for &quot;result&quot;
223:    #define _module2_(_val) _module3_(_idx0, _val)
224:    
225:
226:
227:
228:
229:            
230:KERNEL void kernel_pure_parallel(GLOBAL_MEM int *_leaf_result, GLOBAL_MEM int *_leaf_messages)
231:
232:            {
233:                VIRTUAL_SKIP_THREADS;
234:
235:                VSIZE_T _idx0 = virtual_global_id(0);
236:
237:                
238:
239:            
240:            int phase = _module0_;
241:            _module2_(((unsigned int)phase + 1048576) / 2097152);
242:            
243:
244:            }
245:            
246:<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_single_kernel_bs_performance[bs_loop-cuda:0:0-NTT-MUX]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1a20&gt;, transform_type = &#x27;NTT&#x27;<br/>single_kernel_bootstrap = False, test_function_name = &#x27;MUX&#x27;, heavy_performance_load = False<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(&#x27;test_function_name&#x27;, [&#x27;NAND&#x27;, &#x27;MUX&#x27;, &#x27;uint_min&#x27;])<br/>    def test_single_kernel_bs_performance(<br/>            thread, transform_type, single_kernel_bootstrap,<br/>            test_function_name, heavy_performance_load):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        test_function = dict(<br/>            NAND=(gate_nand, nand_ref, 2),<br/>            MUX=(gate_mux, mux_ref, 3),<br/>            uint_min=(uint_min, uint_min_ref, 2),<br/>            )[test_function_name]<br/>    <br/>        if test_function_name == &#x27;uint_min&#x27;:<br/>            shape = (128, 32) if heavy_performance_load else (4, 16)<br/>        else:<br/>            shape = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/><br/>test/test_gates.py:338: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1a20&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_single_kernel_bs_performance[bs_loop-cuda:0:0-NTT-uint_min]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1a20&gt;, transform_type = &#x27;NTT&#x27;<br/>single_kernel_bootstrap = False, test_function_name = &#x27;uint_min&#x27;<br/>heavy_performance_load = False<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(&#x27;test_function_name&#x27;, [&#x27;NAND&#x27;, &#x27;MUX&#x27;, &#x27;uint_min&#x27;])<br/>    def test_single_kernel_bs_performance(<br/>            thread, transform_type, single_kernel_bootstrap,<br/>            test_function_name, heavy_performance_load):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        test_function = dict(<br/>            NAND=(gate_nand, nand_ref, 2),<br/>            MUX=(gate_mux, mux_ref, 3),<br/>            uint_min=(uint_min, uint_min_ref, 2),<br/>            )[test_function_name]<br/>    <br/>        if test_function_name == &#x27;uint_min&#x27;:<br/>            shape = (128, 32) if heavy_performance_load else (4, 16)<br/>        else:<br/>            shape = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/><br/>test/test_gates.py:338: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1a20&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_single_kernel_bs_performance[bs_loop-cuda:0:0-FFT-NAND]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1a20&gt;, transform_type = &#x27;FFT&#x27;<br/>single_kernel_bootstrap = False, test_function_name = &#x27;NAND&#x27;, heavy_performance_load = False<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(&#x27;test_function_name&#x27;, [&#x27;NAND&#x27;, &#x27;MUX&#x27;, &#x27;uint_min&#x27;])<br/>    def test_single_kernel_bs_performance(<br/>            thread, transform_type, single_kernel_bootstrap,<br/>            test_function_name, heavy_performance_load):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        test_function = dict(<br/>            NAND=(gate_nand, nand_ref, 2),<br/>            MUX=(gate_mux, mux_ref, 3),<br/>            uint_min=(uint_min, uint_min_ref, 2),<br/>            )[test_function_name]<br/>    <br/>        if test_function_name == &#x27;uint_min&#x27;:<br/>            shape = (128, 32) if heavy_performance_load else (4, 16)<br/>        else:<br/>            shape = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/><br/>test/test_gates.py:338: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1a20&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_single_kernel_bs_performance[bs_loop-cuda:0:0-FFT-MUX]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1a20&gt;, transform_type = &#x27;FFT&#x27;<br/>single_kernel_bootstrap = False, test_function_name = &#x27;MUX&#x27;, heavy_performance_load = False<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(&#x27;test_function_name&#x27;, [&#x27;NAND&#x27;, &#x27;MUX&#x27;, &#x27;uint_min&#x27;])<br/>    def test_single_kernel_bs_performance(<br/>            thread, transform_type, single_kernel_bootstrap,<br/>            test_function_name, heavy_performance_load):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        test_function = dict(<br/>            NAND=(gate_nand, nand_ref, 2),<br/>            MUX=(gate_mux, mux_ref, 3),<br/>            uint_min=(uint_min, uint_min_ref, 2),<br/>            )[test_function_name]<br/>    <br/>        if test_function_name == &#x27;uint_min&#x27;:<br/>            shape = (128, 32) if heavy_performance_load else (4, 16)<br/>        else:<br/>            shape = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/><br/>test/test_gates.py:338: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1a20&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_single_kernel_bs_performance[bs_loop-cuda:0:0-FFT-uint_min]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1a20&gt;, transform_type = &#x27;FFT&#x27;<br/>single_kernel_bootstrap = False, test_function_name = &#x27;uint_min&#x27;<br/>heavy_performance_load = False<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(&#x27;test_function_name&#x27;, [&#x27;NAND&#x27;, &#x27;MUX&#x27;, &#x27;uint_min&#x27;])<br/>    def test_single_kernel_bs_performance(<br/>            thread, transform_type, single_kernel_bootstrap,<br/>            test_function_name, heavy_performance_load):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        test_function = dict(<br/>            NAND=(gate_nand, nand_ref, 2),<br/>            MUX=(gate_mux, mux_ref, 3),<br/>            uint_min=(uint_min, uint_min_ref, 2),<br/>            )[test_function_name]<br/>    <br/>        if test_function_name == &#x27;uint_min&#x27;:<br/>            shape = (128, 32) if heavy_performance_load else (4, 16)<br/>        else:<br/>            shape = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/><br/>test/test_gates.py:338: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1a20&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_constant_mem_performance[bs_loop-cuda:0:0-FFT-global_mem]</td>
          <td class="col-duration">39.91</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f88deeba8&gt;, transform_type = &#x27;FFT&#x27;<br/>single_kernel_bootstrap = False, heavy_performance_load = False, use_constant_memory = False<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(&#x27;use_constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_constant_mem_performance(<br/>            thread, transform_type, single_kernel_bootstrap, heavy_performance_load,<br/>            use_constant_memory):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        # We want to test the effect of using constant memory on the bootstrap calculation.<br/>        # A single-kernel bootstrap uses the `use_constant_memory_multi_iter` option,<br/>        # and a multi-kernel bootstrap uses the `use_constant_memory_single_iter` option.<br/>        kwds = dict(single_kernel_bootstrap=single_kernel_bootstrap)<br/>        if single_kernel_bootstrap:<br/>            kwds.update(dict(use_constant_memory_multi_iter=use_constant_memory))<br/>        else:<br/>            kwds.update(dict(use_constant_memory_single_iter=use_constant_memory))<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>        secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/>    <br/>        # TODO: instead of creating a whole key and then checking if the parameters are supported,<br/>        # we can just create a parameter object separately.<br/>        if (single_kernel_bootstrap<br/>                and not single_kernel_bootstrap_supported(secret_key.params, thread.device_params)):<br/>            pytest.skip()<br/>    <br/>        perf_params = PerformanceParameters(secret_key.params, **kwds).for_device(thread.device_params)<br/>    <br/>&gt;       results = check_performance(thread, (secret_key, cloud_key), perf_params, shape=size)<br/><br/>test/test_gates.py:387: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_gates.py:279: in check_performance<br/>    shape=shape2, performance_test=True, perf_params=perf_params)<br/>test/test_gates.py:65: in check_gate<br/>    nufhe_func(thread, cloud_key, answer, *ciphertexts, perf_params)<br/>nufhe/gates.py:121: in gate_nand<br/>    MU, temp_result, perf_params)<br/>nufhe/bootstrap.py:229: in bootstrap<br/>    no_keyswitch=no_keyswitch)<br/>nufhe/bootstrap.py:190: in blind_rotate_and_extract<br/>    thr, acc, bk, bara, bk.in_out_params.size, bk_params, perf_params)<br/>nufhe/bootstrap.py:135: in blind_rotate<br/>    mux_rotate(thr, temp2, temp3, bk.tgsw, i, bara, bk_params, perf_params)<br/>nufhe/bootstrap.py:106: in mux_rotate<br/>    tgsw_transformed_external_mul(thr, result, bki, bk_idx, perf_params)<br/>nufhe/tgsw.py:171: in tgsw_transformed_external_mul<br/>    bootstrap_key.params, result.shape, bootstrap_key.shape[0], perf_params)<br/>nufhe/computation_cache.py:55: in get_computation<br/>    compiled_comp = comp.compile(thr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/tgsw_gpu.py:166: in _build_plan<br/>    plan.computation_call(decomp_and_ftr, tr_sample, accum)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:502: in computation_call<br/>    self._compiler_options, self._keep))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/transform/computation.py:67: in _build_plan<br/>    cdata = plan.persistent_array(cdata_arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:304: in persistent_array<br/>    self._persistent_values[name] = self._thread.to_device(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f88deeba8&gt;, shape = (1024,)<br/>dtype = dtype(&#x27;complex128&#x27;), strides = (16,), offset = 0, nbytes = 16384<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_constant_mem_performance[bs_loop-cuda:0:0-FFT-constant_mem]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f88deeba8&gt;, transform_type = &#x27;FFT&#x27;<br/>single_kernel_bootstrap = False, heavy_performance_load = False, use_constant_memory = True<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(&#x27;use_constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_constant_mem_performance(<br/>            thread, transform_type, single_kernel_bootstrap, heavy_performance_load,<br/>            use_constant_memory):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        # We want to test the effect of using constant memory on the bootstrap calculation.<br/>        # A single-kernel bootstrap uses the `use_constant_memory_multi_iter` option,<br/>        # and a multi-kernel bootstrap uses the `use_constant_memory_single_iter` option.<br/>        kwds = dict(single_kernel_bootstrap=single_kernel_bootstrap)<br/>        if single_kernel_bootstrap:<br/>            kwds.update(dict(use_constant_memory_multi_iter=use_constant_memory))<br/>        else:<br/>            kwds.update(dict(use_constant_memory_single_iter=use_constant_memory))<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/><br/>test/test_gates.py:377: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f88deeba8&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_ntt_base_method_performance[bs_loop-cuda:0:0-ntt_base=c]</td>
          <td class="col-duration">2.57</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f8854ab70&gt;, single_kernel_bootstrap = False<br/>heavy_performance_load = False, ntt_base_method = &#x27;c&#x27;<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(<br/>        &#x27;ntt_base_method&#x27;, [&#x27;cuda_asm&#x27;, &#x27;c&#x27;], ids=[&#x27;ntt_base=cuda_asm&#x27;, &#x27;ntt_base=c&#x27;])<br/>    def test_ntt_base_method_performance(<br/>            thread, single_kernel_bootstrap, heavy_performance_load, ntt_base_method):<br/>    <br/>        if thread.api.get_id() != cuda_id() and ntt_base_method == &#x27;cuda_asm&#x27;:<br/>            pytest.skip()<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>        secret_key, cloud_key = make_key_pair(thread, rng, transform_type=&#x27;NTT&#x27;)<br/>    <br/>        # TODO: instead of creating a whole key and then checking if the parameters are supported,<br/>        # we can just create a parameter object separately.<br/>        if (single_kernel_bootstrap<br/>                and not single_kernel_bootstrap_supported(secret_key.params, thread.device_params)):<br/>            pytest.skip()<br/>    <br/>        perf_params = PerformanceParameters(<br/>            secret_key.params,<br/>            single_kernel_bootstrap=single_kernel_bootstrap,<br/>            ntt_base_method=ntt_base_method).for_device(thread.device_params)<br/>    <br/>&gt;       results = check_performance(thread, (secret_key, cloud_key), perf_params, shape=size)<br/><br/>test/test_gates.py:445: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_gates.py:276: in check_performance<br/>    shape=shape1, performance_test=True, perf_params=perf_params)<br/>test/test_gates.py:65: in check_gate<br/>    nufhe_func(thread, cloud_key, answer, *ciphertexts, perf_params)<br/>nufhe/gates.py:121: in gate_nand<br/>    MU, temp_result, perf_params)<br/>nufhe/bootstrap.py:229: in bootstrap<br/>    no_keyswitch=no_keyswitch)<br/>nufhe/bootstrap.py:190: in blind_rotate_and_extract<br/>    thr, acc, bk, bara, bk.in_out_params.size, bk_params, perf_params)<br/>nufhe/bootstrap.py:135: in blind_rotate<br/>    mux_rotate(thr, temp2, temp3, bk.tgsw, i, bara, bk_params, perf_params)<br/>nufhe/bootstrap.py:106: in mux_rotate<br/>    tgsw_transformed_external_mul(thr, result, bki, bk_idx, perf_params)<br/>nufhe/tgsw.py:171: in tgsw_transformed_external_mul<br/>    bootstrap_key.params, result.shape, bootstrap_key.shape[0], perf_params)<br/>nufhe/computation_cache.py:55: in get_computation<br/>    compiled_comp = comp.compile(thr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/tgsw_gpu.py:166: in _build_plan<br/>    plan.computation_call(decomp_and_ftr, tr_sample, accum)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:502: in computation_call<br/>    self._compiler_options, self._keep))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/transform/computation.py:67: in _build_plan<br/>    cdata = plan.persistent_array(cdata_arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:304: in persistent_array<br/>    self._persistent_values[name] = self._thread.to_device(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f8854ab70&gt;, shape = (2048,)<br/>dtype = dtype(&#x27;uint64&#x27;), strides = (8,), offset = 0, nbytes = 16384<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_ntt_lsh_method_performance[bs_loop-cuda:0:0-ntt_lsh=cuda_asm]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f881c8550&gt;, single_kernel_bootstrap = False<br/>heavy_performance_load = False, ntt_lsh_method = &#x27;cuda_asm&#x27;<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(<br/>        &#x27;ntt_lsh_method&#x27;,<br/>        [&#x27;cuda_asm&#x27;, &#x27;c_from_asm&#x27;, &#x27;c&#x27;],<br/>        ids=[&#x27;ntt_lsh=cuda_asm&#x27;, &#x27;ntt_lsh=c_from_asm&#x27;, &#x27;ntt_lsh=c&#x27;])<br/>    def test_ntt_lsh_method_performance(<br/>            thread, single_kernel_bootstrap, heavy_performance_load, ntt_lsh_method):<br/>    <br/>        if thread.api.get_id() != cuda_id() and ntt_lsh_method == &#x27;cuda_asm&#x27;:<br/>            pytest.skip()<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, transform_type=&#x27;NTT&#x27;)<br/><br/>test/test_gates.py:496: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f881c8550&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_ntt_lsh_method_performance[bs_loop-cuda:0:0-ntt_lsh=c_from_asm]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f881c8550&gt;, single_kernel_bootstrap = False<br/>heavy_performance_load = False, ntt_lsh_method = &#x27;c_from_asm&#x27;<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(<br/>        &#x27;ntt_lsh_method&#x27;,<br/>        [&#x27;cuda_asm&#x27;, &#x27;c_from_asm&#x27;, &#x27;c&#x27;],<br/>        ids=[&#x27;ntt_lsh=cuda_asm&#x27;, &#x27;ntt_lsh=c_from_asm&#x27;, &#x27;ntt_lsh=c&#x27;])<br/>    def test_ntt_lsh_method_performance(<br/>            thread, single_kernel_bootstrap, heavy_performance_load, ntt_lsh_method):<br/>    <br/>        if thread.api.get_id() != cuda_id() and ntt_lsh_method == &#x27;cuda_asm&#x27;:<br/>            pytest.skip()<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, transform_type=&#x27;NTT&#x27;)<br/><br/>test/test_gates.py:496: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f881c8550&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_ntt_lsh_method_performance[bs_loop-cuda:0:0-ntt_lsh=c]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f881c8550&gt;, single_kernel_bootstrap = False<br/>heavy_performance_load = False, ntt_lsh_method = &#x27;c&#x27;<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(<br/>        &#x27;ntt_lsh_method&#x27;,<br/>        [&#x27;cuda_asm&#x27;, &#x27;c_from_asm&#x27;, &#x27;c&#x27;],<br/>        ids=[&#x27;ntt_lsh=cuda_asm&#x27;, &#x27;ntt_lsh=c_from_asm&#x27;, &#x27;ntt_lsh=c&#x27;])<br/>    def test_ntt_lsh_method_performance(<br/>            thread, single_kernel_bootstrap, heavy_performance_load, ntt_lsh_method):<br/>    <br/>        if thread.api.get_id() != cuda_id() and ntt_lsh_method == &#x27;cuda_asm&#x27;:<br/>            pytest.skip()<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, transform_type=&#x27;NTT&#x27;)<br/><br/>test/test_gates.py:496: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f881c8550&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_single_kernel_bs[bs_kernel-cuda:0:0]</td>
          <td class="col-duration">5.83</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f882b4358&gt;<br/>key_pair = (&lt;nufhe.api_low_level.NuFHESecretKey object at 0x7f88461518&gt;, &lt;nufhe.api_low_level.NuFHECloudKey object at 0x7f88461fd0&gt;)<br/>single_kernel_bootstrap = True<br/><br/>    def test_single_kernel_bs(thread, key_pair, single_kernel_bootstrap):<br/>        # Test a gate that employs separate calls to bootstrap and keyswitch<br/>        secret_key, cloud_key = key_pair<br/>    <br/>        if (single_kernel_bootstrap<br/>                and not single_kernel_bootstrap_supported(secret_key.params, thread.device_params)):<br/>            pytest.skip()<br/>    <br/>        perf_params = PerformanceParameters(<br/>            secret_key.params, single_kernel_bootstrap=single_kernel_bootstrap)<br/>        perf_params = perf_params.for_device(thread.device_params)<br/>&gt;       check_gate(thread, key_pair, 3, gate_mux, mux_ref, perf_params=perf_params)<br/><br/>test/test_gates.py:128: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_gates.py:78: in check_gate<br/>    nufhe_func(thread, cloud_key, answer, *ciphertexts, perf_params)<br/>nufhe/gates.py:646: in gate_mux<br/>    perf_params, no_keyswitch=True)<br/>nufhe/bootstrap.py:229: in bootstrap<br/>    no_keyswitch=no_keyswitch)<br/>nufhe/bootstrap.py:186: in blind_rotate_and_extract<br/>    BlindRotate_gpu(result, acc, bk, ks, bara, perf_params, no_keyswitch=no_keyswitch)<br/>nufhe/blind_rotate.py:272: in BlindRotate_gpu<br/>    comp = get_computation(thr, BlindRotate, bk.bk_params, bk.in_out_params, shape, perf_params)<br/>nufhe/computation_cache.py:55: in get_computation<br/>    compiled_comp = comp.compile(thr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;nufhe.blind_rotate.BlindRotate object at 0x7f88273b00&gt;<br/>plan_factory = &lt;function Computation._get_plan.&lt;locals&gt;.&lt;lambda&gt; at 0x7f88238048&gt;<br/>device_params = &lt;reikna.cluda.cuda.DeviceParameters object at 0x7f884619b0&gt;<br/>lwe_a = KernelArgument(lwe_a), lwe_b = KernelArgument(lwe_b)<br/>accum_a = KernelArgument(accum_a), gsw = KernelArgument(gsw), bara = KernelArgument(bara)<br/><br/>    def _build_plan(self, plan_factory, device_params, lwe_a, lwe_b, accum_a, gsw, bara):<br/>    <br/>        params = self._params<br/>        tlwe_params = params.tlwe_params<br/>        decomp_length = params.decomp_length<br/>        mask_size = tlwe_params.mask_size<br/>    <br/>        perf_params = self._perf_params<br/>        transform_type = self._params.tlwe_params.transform_type<br/>        transform = get_transform(transform_type)<br/>    <br/>        transform_module = transform.transform_module(perf_params, multi_iter=True)<br/>    <br/>        batch_shape = accum_a.shape[:-2]<br/>    <br/>        min_local_size = decomp_length * (mask_size + 1) * transform_module.threads_per_transform<br/>        local_size = device_params.max_work_group_size<br/>        while local_size &gt;= min_local_size:<br/>    <br/>            plan = plan_factory()<br/>    <br/>            if transform_module.use_constant_memory:<br/>                cdata_forward = plan.constant_array(transform_module.cdata_fw)<br/>                cdata_inverse = plan.constant_array(transform_module.cdata_inv)<br/>            else:<br/>                cdata_forward = plan.persistent_array(transform_module.cdata_fw)<br/>                cdata_inverse = plan.persistent_array(transform_module.cdata_inv)<br/>    <br/>            try:<br/>                plan.kernel_call(<br/>                    TEMPLATE.get_def(&quot;blind_rotate&quot;),<br/>                    [lwe_a, lwe_b, accum_a, gsw, bara, cdata_forward, cdata_inverse],<br/>                    kernel_name=&quot;blind_rotate&quot;,<br/>                    global_size=(<br/>                        helpers.product(batch_shape),<br/>                        local_size),<br/>                    local_size=(1, local_size),<br/>                    render_kwds=dict(<br/>                        local_size=local_size,<br/>                        slices=(len(batch_shape), 1, 1),<br/>                        slices2=(len(batch_shape), 1),<br/>                        slices3=(len(batch_shape),),<br/>                        transform=transform_module,<br/>                        mask_size=mask_size,<br/>                        decomp_length=decomp_length,<br/>                        output_size=self._in_out_params.size,<br/>                        input_size=tlwe_params.extracted_lweparams.size,<br/>                        bs_log2_base=self._params.bs_log2_base,<br/>                        mul_prepared=transform.transformed_mul_prepared(perf_params),<br/>                        add=transform.transformed_add(perf_params),<br/>                        tr_ctype=transform.transformed_internal_ctype(),<br/>                        min_blocks=helpers.min_blocks,<br/>                        )<br/>                    )<br/>            except OutOfResourcesError:<br/>                local_size -= transform_module.threads_per_transform<br/>                continue<br/>    <br/>            return plan<br/>    <br/>&gt;       raise ValueError(&quot;Could not find suitable local size for the kernel&quot;)<br/><span class="error">E       ValueError: Could not find suitable local size for the kernel</span><br/><br/>nufhe/blind_rotate.py:187: ValueError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_single_kernel_bs_performance[bs_kernel-cuda:0:0-NTT-NAND]</td>
          <td class="col-duration">13.23</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f8839c518&gt;, transform_type = &#x27;NTT&#x27;<br/>single_kernel_bootstrap = True, test_function_name = &#x27;NAND&#x27;, heavy_performance_load = False<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(&#x27;test_function_name&#x27;, [&#x27;NAND&#x27;, &#x27;MUX&#x27;, &#x27;uint_min&#x27;])<br/>    def test_single_kernel_bs_performance(<br/>            thread, transform_type, single_kernel_bootstrap,<br/>            test_function_name, heavy_performance_load):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        test_function = dict(<br/>            NAND=(gate_nand, nand_ref, 2),<br/>            MUX=(gate_mux, mux_ref, 3),<br/>            uint_min=(uint_min, uint_min_ref, 2),<br/>            )[test_function_name]<br/>    <br/>        if test_function_name == &#x27;uint_min&#x27;:<br/>            shape = (128, 32) if heavy_performance_load else (4, 16)<br/>        else:<br/>            shape = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>        secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/>    <br/>        # TODO: instead of creating a whole key and then checking if the parameters are supported,<br/>        # we can just create a parameter object separately.<br/>        if (single_kernel_bootstrap<br/>                and not single_kernel_bootstrap_supported(secret_key.params, thread.device_params)):<br/>            pytest.skip()<br/>    <br/>        perf_params = PerformanceParameters(<br/>            secret_key.params, single_kernel_bootstrap=single_kernel_bootstrap)<br/>        perf_params = perf_params.for_device(thread.device_params)<br/>    <br/>        results = check_performance(<br/>&gt;           thread, (secret_key, cloud_key), perf_params, shape=shape, test_function=test_function)<br/><br/>test/test_gates.py:351: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_gates.py:276: in check_performance<br/>    shape=shape1, performance_test=True, perf_params=perf_params)<br/>test/test_gates.py:65: in check_gate<br/>    nufhe_func(thread, cloud_key, answer, *ciphertexts, perf_params)<br/>nufhe/gates.py:121: in gate_nand<br/>    MU, temp_result, perf_params)<br/>nufhe/bootstrap.py:229: in bootstrap<br/>    no_keyswitch=no_keyswitch)<br/>nufhe/bootstrap.py:186: in blind_rotate_and_extract<br/>    BlindRotate_gpu(result, acc, bk, ks, bara, perf_params, no_keyswitch=no_keyswitch)<br/>nufhe/blind_rotate.py:278: in BlindRotate_gpu<br/>    ks.log2_base, ks.decomp_length, perf_params)<br/>nufhe/computation_cache.py:55: in get_computation<br/>    compiled_comp = comp.compile(thr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/blind_rotate.py:250: in _build_plan<br/>    plan.computation_call(blind_rotate, extracted_a, extracted_b, accum_a, gsw, bara)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:502: in computation_call<br/>    self._compiler_options, self._keep))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;nufhe.blind_rotate.BlindRotate object at 0x7f883bd7f0&gt;<br/>plan_factory = &lt;function Computation._get_plan.&lt;locals&gt;.&lt;lambda&gt; at 0x7f883506a8&gt;<br/>device_params = &lt;reikna.cluda.cuda.DeviceParameters object at 0x7f8839cf98&gt;<br/>lwe_a = KernelArgument(_temp1), lwe_b = KernelArgument(_temp2)<br/>accum_a = KernelArgument(accum_a), gsw = KernelArgument(gsw), bara = KernelArgument(bara)<br/><br/>    def _build_plan(self, plan_factory, device_params, lwe_a, lwe_b, accum_a, gsw, bara):<br/>    <br/>        params = self._params<br/>        tlwe_params = params.tlwe_params<br/>        decomp_length = params.decomp_length<br/>        mask_size = tlwe_params.mask_size<br/>    <br/>        perf_params = self._perf_params<br/>        transform_type = self._params.tlwe_params.transform_type<br/>        transform = get_transform(transform_type)<br/>    <br/>        transform_module = transform.transform_module(perf_params, multi_iter=True)<br/>    <br/>        batch_shape = accum_a.shape[:-2]<br/>    <br/>        min_local_size = decomp_length * (mask_size + 1) * transform_module.threads_per_transform<br/>        local_size = device_params.max_work_group_size<br/>        while local_size &gt;= min_local_size:<br/>    <br/>            plan = plan_factory()<br/>    <br/>            if transform_module.use_constant_memory:<br/>                cdata_forward = plan.constant_array(transform_module.cdata_fw)<br/>                cdata_inverse = plan.constant_array(transform_module.cdata_inv)<br/>            else:<br/>                cdata_forward = plan.persistent_array(transform_module.cdata_fw)<br/>                cdata_inverse = plan.persistent_array(transform_module.cdata_inv)<br/>    <br/>            try:<br/>                plan.kernel_call(<br/>                    TEMPLATE.get_def(&quot;blind_rotate&quot;),<br/>                    [lwe_a, lwe_b, accum_a, gsw, bara, cdata_forward, cdata_inverse],<br/>                    kernel_name=&quot;blind_rotate&quot;,<br/>                    global_size=(<br/>                        helpers.product(batch_shape),<br/>                        local_size),<br/>                    local_size=(1, local_size),<br/>                    render_kwds=dict(<br/>                        local_size=local_size,<br/>                        slices=(len(batch_shape), 1, 1),<br/>                        slices2=(len(batch_shape), 1),<br/>                        slices3=(len(batch_shape),),<br/>                        transform=transform_module,<br/>                        mask_size=mask_size,<br/>                        decomp_length=decomp_length,<br/>                        output_size=self._in_out_params.size,<br/>                        input_size=tlwe_params.extracted_lweparams.size,<br/>                        bs_log2_base=self._params.bs_log2_base,<br/>                        mul_prepared=transform.transformed_mul_prepared(perf_params),<br/>                        add=transform.transformed_add(perf_params),<br/>                        tr_ctype=transform.transformed_internal_ctype(),<br/>                        min_blocks=helpers.min_blocks,<br/>                        )<br/>                    )<br/>            except OutOfResourcesError:<br/>                local_size -= transform_module.threads_per_transform<br/>                continue<br/>    <br/>            return plan<br/>    <br/>&gt;       raise ValueError(&quot;Could not find suitable local size for the kernel&quot;)<br/><span class="error">E       ValueError: Could not find suitable local size for the kernel</span><br/><br/>nufhe/blind_rotate.py:187: ValueError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_single_kernel_bs_performance[bs_kernel-cuda:0:0-NTT-MUX]</td>
          <td class="col-duration">6.08</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f8839c518&gt;, transform_type = &#x27;NTT&#x27;<br/>single_kernel_bootstrap = True, test_function_name = &#x27;MUX&#x27;, heavy_performance_load = False<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(&#x27;test_function_name&#x27;, [&#x27;NAND&#x27;, &#x27;MUX&#x27;, &#x27;uint_min&#x27;])<br/>    def test_single_kernel_bs_performance(<br/>            thread, transform_type, single_kernel_bootstrap,<br/>            test_function_name, heavy_performance_load):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        test_function = dict(<br/>            NAND=(gate_nand, nand_ref, 2),<br/>            MUX=(gate_mux, mux_ref, 3),<br/>            uint_min=(uint_min, uint_min_ref, 2),<br/>            )[test_function_name]<br/>    <br/>        if test_function_name == &#x27;uint_min&#x27;:<br/>            shape = (128, 32) if heavy_performance_load else (4, 16)<br/>        else:<br/>            shape = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>        secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/>    <br/>        # TODO: instead of creating a whole key and then checking if the parameters are supported,<br/>        # we can just create a parameter object separately.<br/>        if (single_kernel_bootstrap<br/>                and not single_kernel_bootstrap_supported(secret_key.params, thread.device_params)):<br/>            pytest.skip()<br/>    <br/>        perf_params = PerformanceParameters(<br/>            secret_key.params, single_kernel_bootstrap=single_kernel_bootstrap)<br/>        perf_params = perf_params.for_device(thread.device_params)<br/>    <br/>        results = check_performance(<br/>&gt;           thread, (secret_key, cloud_key), perf_params, shape=shape, test_function=test_function)<br/><br/>test/test_gates.py:351: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_gates.py:276: in check_performance<br/>    shape=shape1, performance_test=True, perf_params=perf_params)<br/>test/test_gates.py:65: in check_gate<br/>    nufhe_func(thread, cloud_key, answer, *ciphertexts, perf_params)<br/>nufhe/gates.py:646: in gate_mux<br/>    perf_params, no_keyswitch=True)<br/>nufhe/bootstrap.py:229: in bootstrap<br/>    no_keyswitch=no_keyswitch)<br/>nufhe/bootstrap.py:186: in blind_rotate_and_extract<br/>    BlindRotate_gpu(result, acc, bk, ks, bara, perf_params, no_keyswitch=no_keyswitch)<br/>nufhe/blind_rotate.py:272: in BlindRotate_gpu<br/>    comp = get_computation(thr, BlindRotate, bk.bk_params, bk.in_out_params, shape, perf_params)<br/>nufhe/computation_cache.py:55: in get_computation<br/>    compiled_comp = comp.compile(thr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/blind_rotate.py:178: in _build_plan<br/>    min_blocks=helpers.min_blocks,<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:473: in kernel_call<br/>    keep=self._keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:566: in compile_static<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:786: in __init__<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:655: in __init__<br/>    self.source, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:504: in _create_program<br/>    src, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:227: in _compile<br/>    return SourceModule(src, no_extern_c=True, options=options, keep=keep)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;pycuda.compiler.SourceModule object at 0x7f882890b8&gt;<br/>source = &#x27;\n\n\n    #define CUDA\n    // taken from pycuda._cluda\n    #define LOCAL_BARRIER __syncthreads()\n\n    #define WIT... _module28_(\n                batch_id, i, i == 0 ? shared_accum[0] : -shared_accum[1024 - i]);\n        }\n    }\n}\n&#x27;<br/>nvcc = &#x27;nvcc&#x27;, options = [], keep = False, no_extern_c = True, arch = None, code = None<br/>cache_dir = None, include_dirs = []<br/><br/>    def __init__(self, source, nvcc=&quot;nvcc&quot;, options=None, keep=False,<br/>            no_extern_c=False, arch=None, code=None, cache_dir=None,<br/>            include_dirs=[]):<br/>        self._check_arch(arch)<br/>    <br/>        cubin = compile(source, nvcc, options, keep, no_extern_c,<br/>                arch, code, cache_dir, include_dirs)<br/>    <br/>        from pycuda.driver import module_from_buffer<br/>&gt;       self.module = module_from_buffer(cubin)<br/><span class="error">E       pycuda._driver.LogicError: cuModuleLoadDataEx failed: invalid device context -</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/compiler.py:294: LogicError<br/> -------------------------------Captured log call-------------------------------- <br/>[1m[31mERROR   [0m root:api.py:507 Failed to compile:
1:
2:
3:
4:    #define CUDA
5:    // taken from pycuda._cluda
6:    #define LOCAL_BARRIER __syncthreads()
7:
8:    #define WITHIN_KERNEL __device__
9:    #define KERNEL extern &quot;C&quot; __global__
10:    #define GLOBAL_MEM /* empty */
11:    #define GLOBAL_MEM_ARG /* empty */
12:    #define LOCAL_MEM __shared__
13:    #define LOCAL_MEM_DYNAMIC extern __shared__
14:    #define LOCAL_MEM_ARG /* empty */
15:    #define CONSTANT_MEM __constant__
16:    #define CONSTANT_MEM_ARG /* empty */
17:    #define INLINE __forceinline__
18:    #define SIZE_T int
19:    #define VSIZE_T int
20:
21:    // used to align fields in structures
22:    #define ALIGN(bytes) __align__(bytes)
23:
24:    
25:
26:    WITHIN_KERNEL SIZE_T get_local_id(unsigned int dim)
27:    {
28:        if(dim == 0) return threadIdx.x;
29:        if(dim == 1) return threadIdx.y;
30:        if(dim == 2) return threadIdx.z;
31:        return 0;
32:    }
33:
34:    WITHIN_KERNEL SIZE_T get_group_id(unsigned int dim)
35:    {
36:        if(dim == 0) return blockIdx.x;
37:        if(dim == 1) return blockIdx.y;
38:        if(dim == 2) return blockIdx.z;
39:        return 0;
40:    }
41:
42:    WITHIN_KERNEL SIZE_T get_local_size(unsigned int dim)
43:    {
44:        if(dim == 0) return blockDim.x;
45:        if(dim == 1) return blockDim.y;
46:        if(dim == 2) return blockDim.z;
47:        return 1;
48:    }
49:
50:    WITHIN_KERNEL SIZE_T get_num_groups(unsigned int dim)
51:    {
52:        if(dim == 0) return gridDim.x;
53:        if(dim == 1) return gridDim.y;
54:        if(dim == 2) return gridDim.z;
55:        return 1;
56:    }
57:
58:    WITHIN_KERNEL SIZE_T get_global_size(unsigned int dim)
59:    {
60:        return get_num_groups(dim) * get_local_size(dim);
61:    }
62:
63:    WITHIN_KERNEL SIZE_T get_global_id(unsigned int dim)
64:    {
65:        return get_local_id(dim) + get_group_id(dim) * get_local_size(dim);
66:    }
67:
68:
69:
70:
71:    #define COMPLEX_CTR(T) make_##T
72:
73:    WITHIN_KERNEL float2 operator+(float2 a, float2 b)
74:    {
75:        return COMPLEX_CTR(float2)(a.x + b.x, a.y + b.y);
76:    }
77:    WITHIN_KERNEL float2 operator-(float2 a, float2 b)
78:    {
79:        return COMPLEX_CTR(float2)(a.x - b.x, a.y - b.y);
80:    }
81:    WITHIN_KERNEL float2 operator+(float2 a) { return a; }
82:    WITHIN_KERNEL float2 operator-(float2 a) { return COMPLEX_CTR(float2)(-a.x, -a.y); }
83:    WITHIN_KERNEL double2 operator+(double2 a, double2 b)
84:    {
85:        return COMPLEX_CTR(double2)(a.x + b.x, a.y + b.y);
86:    }
87:    WITHIN_KERNEL double2 operator-(double2 a, double2 b)
88:    {
89:        return COMPLEX_CTR(double2)(a.x - b.x, a.y - b.y);
90:    }
91:    WITHIN_KERNEL double2 operator+(double2 a) { return a; }
92:    WITHIN_KERNEL double2 operator-(double2 a) { return COMPLEX_CTR(double2)(-a.x, -a.y); }
93:
94:WITHIN_KERNEL VSIZE_T virtual_local_id(unsigned int dim)
95:{
96:    if (dim == 1)
97:    {
98:
99:        SIZE_T flat_id =
100:            get_local_id(0) * 1 +
101:            0;
102:
103:        return (flat_id / 1);
104:
105:    }
106:    if (dim == 0)
107:    {
108:
109:        return 0;
110:
111:    }
112:
113:    return 0;
114:}
115:
116:WITHIN_KERNEL VSIZE_T virtual_local_size(unsigned int dim)
117:{
118:    if (dim == 1)
119:    {
120:        return 512;
121:    }
122:    if (dim == 0)
123:    {
124:        return 1;
125:    }
126:
127:    return 1;
128:}
129:
130:WITHIN_KERNEL VSIZE_T virtual_group_id(unsigned int dim)
131:{
132:    if (dim == 1)
133:    {
134:
135:        return 0;
136:
137:    }
138:    if (dim == 0)
139:    {
140:
141:        SIZE_T flat_id =
142:            get_group_id(1) * 1 +
143:            0;
144:
145:        return (flat_id / 1);
146:
147:    }
148:
149:    return 0;
150:}
151:
152:WITHIN_KERNEL VSIZE_T virtual_num_groups(unsigned int dim)
153:{
154:    if (dim == 1)
155:    {
156:        return 1;
157:    }
158:    if (dim == 0)
159:    {
160:        return 64;
161:    }
162:
163:    return 1;
164:}
165:
166:WITHIN_KERNEL VSIZE_T virtual_global_id(unsigned int dim)
167:{
168:    return virtual_local_id(dim) + virtual_group_id(dim) * virtual_local_size(dim);
169:}
170:
171:WITHIN_KERNEL VSIZE_T virtual_global_size(unsigned int dim)
172:{
173:    if(dim == 1)
174:    {
175:        return 512;
176:    }
177:    if(dim == 0)
178:    {
179:        return 64;
180:    }
181:
182:    return 1;
183:}
184:
185:WITHIN_KERNEL VSIZE_T virtual_global_flat_id()
186:{
187:    return
188:        virtual_global_id(1) * 1 +
189:        virtual_global_id(0) * 512 +
190:        0;
191:}
192:
193:WITHIN_KERNEL VSIZE_T virtual_global_flat_size()
194:{
195:    return
196:        virtual_global_size(1) *
197:        virtual_global_size(0) *
198:        1;
199:}
200:
201:
202:WITHIN_KERNEL bool virtual_skip_local_threads()
203:{
204:
205:    return false;
206:}
207:
208:WITHIN_KERNEL bool virtual_skip_groups()
209:{
210:
211:    return false;
212:}
213:
214:WITHIN_KERNEL bool virtual_skip_global_threads()
215:{
216:
217:    return false;
218:}
219:
220:
221:
222:#ifndef CUDA
223:#define MARK_VIRTUAL_FUNCTIONS_AS_USED (void)(virtual_num_groups(0)); (void)(virtual_global_flat_id()); (void)(virtual_global_flat_size())
224:#else
225:#define MARK_VIRTUAL_FUNCTIONS_AS_USED
226:#endif
227:
228:#define VIRTUAL_SKIP_THREADS MARK_VIRTUAL_FUNCTIONS_AS_USED; if(virtual_skip_local_threads() || virtual_skip_groups() || virtual_skip_global_threads()) return
229:
230:typedef struct __module0_
231:{
232:    unsigned long val;
233:} _module0_;
234:
235:
236:WITHIN_KERNEL INLINE _module0_ _module0_pack(unsigned long x)
237:{
238:    _module0_ res = {x};
239:    return res;
240:}
241:
242:WITHIN_KERNEL INLINE unsigned long _module0_unpack(_module0_ x)
243:{
244:    return x.val;
245:}
246:
247:#define _module0_zero (_module0_pack(0));
248:
249:
250:#define _module0_UNPACK(hi, lo, src) {lo = (src); hi = (src) &gt;&gt; 32;}
251:
252:#ifdef CUDA
253:#define _module0_PACK(hi, lo) (((unsigned long)(hi) &lt;&lt; 32) | (lo))
254:#else
255:#define _module0_PACK(hi, lo) upsample(hi, lo)
256:#endif
257:
258:
259:#define _module0_SUB_CC(hi, lo, x1, x2) { bool cc = (x1) &lt; (x2); lo = (x1) - (x2); if (cc) { hi -= 1; } }
260:#define _module0_ADD_CC(hi, lo, x1, x2, hi_prev) { lo = (x1) + (x2); bool cc = lo &lt; (x2); hi = hi_prev; if (cc) { hi += 1; } }
261:
262:
263:
264:
265:
266:    // leaf input macro for &quot;accum_a&quot;
267:    #define _module2_(_idx0, _idx1, _idx2) (_leaf_accum_a[(_idx0) * (2048) + (_idx1) * (1024) + (_idx2) * (1) + (0)])
268:    
269:
270:
271:
272:
273:    
274:    INLINE WITHIN_KERNEL int _module1_func(
275:        GLOBAL_MEM int *_leaf_accum_a, VSIZE_T _c_idx0, VSIZE_T _c_idx1, VSIZE_T _c_idx2)
276:    {
277:        
278:
279:    
280:        
281:        VSIZE_T _idx0 = _c_idx0 / 1;
282:    
283:        
284:        VSIZE_T _idx1 = _c_idx1 / 1;
285:    
286:        
287:        VSIZE_T _idx2 = _c_idx2 / 1;
288:    
289:
290:
291:        return
292:        _module2_(_idx0, _idx1, _idx2);
293:    }
294:    
295:    #define _module1_(_c_idx0, _c_idx1, _c_idx2) _module1_func(        _leaf_accum_a, _c_idx0, _c_idx1, _c_idx2)
296:    
297:
298:
299:
300:
301:    // leaf input macro for &quot;bara&quot;
302:    #define _module4_(_idx0, _idx1) (_leaf_bara[(_idx0) * (500) + (_idx1) * (1) + (0)])
303:    
304:
305:
306:
307:
308:    
309:    INLINE WITHIN_KERNEL int _module3_func(
310:        GLOBAL_MEM int *_leaf_bara, VSIZE_T _c_idx0, VSIZE_T _c_idx1)
311:    {
312:        
313:
314:    
315:        
316:        VSIZE_T _idx0 = _c_idx0 / 1;
317:    
318:        
319:        VSIZE_T _idx1 = _c_idx1 / 1;
320:    
321:
322:
323:        return
324:        _module4_(_idx0, _idx1);
325:    }
326:    
327:    #define _module3_(_c_idx0, _c_idx1) _module3_func(        _leaf_bara, _c_idx0, _c_idx1)
328:    
329:
330:
331:
332:/** Subtraction in FF(P): val_ = a + b mod P. */
333:WITHIN_KERNEL INLINE _module0_ _module6_(_module0_ a, _module0_ b)
334:{
335:
336:    /*
337:    Algorithm:
338:    We calculate `s = x - y`
339:    Now there are three variants:
340:    - no underflow (x &gt;= y): all good, `result = s`.
341:    - underflow (detected if `s &gt; x`), so essentially `s = x - y + N`.
342:      This means we need to calculate `s - N + P = s - (2^32 - 1)`
343:    */
344:
345:    _module0_ res = {a.val - b.val};
346:    unsigned int x = -(res.val &gt; a.val);
347:    res.val -= x;
348:    return res;
349:
350:}
351:
352:
353:
354:// Addition in FF(P): val_ = a + b mod P.
355:WITHIN_KERNEL INLINE _module0_ _module7_(_module0_ a, _module0_ b)
356:{
357:
358:    /*
359:    Algorithm:
360:    We calculate `s = x + y`
361:    Now there are three variants:
362:    - `s &lt; P` and no integer overflow: all good, `result = s`.
363:    - `s &gt; P` and no integer overflow: `result = s - P = s + (2^32 - 1)`
364:    - integer overflow, so essentially `s = x + y - N`.
365:      This means that we need to calculate `result = s + N - P = s + (2^32 - 1)`.
366:    Note that the last two variants result in the same modifier being applied.
367:    */
368:    _module0_ res = {a.val + b.val};
369:    res.val += ((res.val &lt; b.val) || res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
370:    return res;
371:
372:}
373:
374:
375:
376:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
377:WITHIN_KERNEL INLINE _module0_ _module9_(unsigned long a)
378:{
379:
380:    // uses the fact that 2 * P &gt; max(UInt64)
381:    // and that a::UInt64 - P == a + 2^32 - 1
382:
383:    _module0_ res = {a};
384:    res.val += (res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
385:    return res;
386:
387:}
388:
389:
390:
391:
392:/**
393:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
394:* @param[in] l An integer in [0, 32)
395:*/
396:WITHIN_KERNEL INLINE _module0_ _module8_(_module0_ x, unsigned int l)
397:{
398:    /*
399:    Algorithm:
400:
401:    We can decompose the shift as
402:
403:        res = x * 2^l = x * M^k * 2^j,
404:
405:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
406:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
407:
408:    After the multiplication by 2^j, the result contains 3 32-bit parts:
409:
410:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
411:
412:    Thus
413:
414:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
415:
416:    Taking the modulus P = M^2 - M + 1, we get
417:
418:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
419:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
420:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
421:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
422:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
423:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
424:
425:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
426:
427:    The processing for the things inside the parentheses is simpler:
428:
429:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
430:    - (x + y) = PACK(s &lt; y, s), where s = x + y
431:      (that is, check for overflow and add 1 in the high half)
432:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
433:      (that is, check for overflow and add (M-1) in the low half)
434:    */
435:
436:
437:
438:
439:        asm(&quot;{\n\t&quot;
440:            &quot;.reg .u32      r0, r1;\n\t&quot;
441:            &quot;.reg .u32      t0, t1, t2;\n\t&quot;
442:            &quot;.reg .u32      n;\n\t&quot;
443:            &quot;.reg .u64      s;\n\t&quot;
444:            // t[2] = (uint32_t)(x &gt;&gt; (64-l));
445:            // t[1] = (uint32_t)(x &gt;&gt; (32-l));
446:            // t[0] = (uint32_t)(x &lt;&lt; l);
447:            &quot;mov.b64        {r0, r1}, %0;\n\t&quot;
448:            &quot;shl.b32        t0, r0, %1;\n\t&quot;
449:            &quot;sub.u32        n, 32, %1;\n\t&quot;
450:            &quot;shr.b64        s, %0, n;\n\t&quot;
451:            &quot;mov.b64        {t1, t2}, s;\n\t&quot;
452:            // mod P
453:            &quot;add.u32        r1, t1, t2;\n\t&quot;
454:            &quot;sub.cc.u32     r0, t0, t2;\n\t&quot;
455:            &quot;subc.u32       r1, r1, 0;\n\t&quot;
456:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
457:            // ret += (uint32_t)(-(ret &lt; ((uint64_t *)t)[0]));
458:            &quot;mov.b64        s, {t0, t1};\n\t&quot;
459:            &quot;set.lt.u32.u64 t2, %0, s;\n\t&quot;
460:            &quot;add.cc.u32     r0, r0, t2;\n\t&quot;
461:            &quot;addc.u32       r1, r1, 0;\n\t&quot;
462:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
463:            &quot;}&quot;
464:            : &quot;+l&quot;(x.val)
465:            : &quot;r&quot;(l));
466:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
467:        return _module9_(x.val);
468:
469:
470:
471:}
472:
473:
474:
475:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
476:WITHIN_KERNEL INLINE _module0_ _module11_(unsigned long a)
477:{
478:
479:    // uses the fact that 2 * P &gt; max(UInt64)
480:    // and that a::UInt64 - P == a + 2^32 - 1
481:
482:    _module0_ res = {a};
483:    res.val += (res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
484:    return res;
485:
486:}
487:
488:
489:
490:
491:/**
492:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
493:* @param[in] l An integer in [0, 32)
494:*/
495:WITHIN_KERNEL INLINE _module0_ _module10_(_module0_ x, unsigned int l)
496:{
497:    /*
498:    Algorithm:
499:
500:    We can decompose the shift as
501:
502:        res = x * 2^l = x * M^k * 2^j,
503:
504:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
505:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
506:
507:    After the multiplication by 2^j, the result contains 3 32-bit parts:
508:
509:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
510:
511:    Thus
512:
513:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
514:
515:    Taking the modulus P = M^2 - M + 1, we get
516:
517:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
518:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
519:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
520:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
521:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
522:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
523:
524:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
525:
526:    The processing for the things inside the parentheses is simpler:
527:
528:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
529:    - (x + y) = PACK(s &lt; y, s), where s = x + y
530:      (that is, check for overflow and add 1 in the high half)
531:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
532:      (that is, check for overflow and add (M-1) in the low half)
533:    */
534:
535:
536:
537:
538:        asm(&quot;{\n\t&quot;
539:            &quot;.reg .u32          r0, r1;\n\t&quot;
540:            &quot;.reg .u32          t0, t1, t2;\n\t&quot;
541:            &quot;.reg .u32          n;\n\t&quot;
542:            &quot;.reg .u64          s;\n\t&quot;
543:            &quot;.reg .pred         p, q;\n\t&quot;
544:            // t[2] = (uint32_t)(x &gt;&gt; (96-l));
545:            // t[1] = (uint32_t)(x &gt;&gt; (64-l));
546:            // t[0] = (uint32_t)(x &lt;&lt; (l-32));
547:            &quot;mov.b64            {r0, r1}, %0;\n\t&quot;
548:            &quot;sub.u32            n, %1, 32;\n\t&quot;
549:            &quot;shl.b32            t0, r0, n;\n\t&quot;
550:            &quot;sub.u32            n, 32, n;\n\t&quot;
551:            &quot;shr.b64            s, %0, n;\n\t&quot;
552:            &quot;mov.b64            {t1, t2}, s;\n\t&quot;
553:            // mod P
554:            &quot;add.u32            r1, t0, t1;\n\t&quot;
555:            &quot;sub.cc.u32         r0, 0, t1;\n\t&quot;
556:            &quot;subc.u32           r1, r1, 0;\n\t&quot;
557:            &quot;sub.cc.u32         r0, r0, t2;\n\t&quot;
558:            &quot;subc.u32           r1, r1, 0;\n\t&quot;
559:            &quot;mov.b64            %0, {r0, r1};\n\t&quot;
560:            // ret -= (uint32_t)(-(ret &gt; ((uint64_t)t[0] &lt;&lt; 32) &amp;&amp; t[1] == 0));
561:            &quot;setp.eq.u32        p|q, t1, 0;\n\t&quot;
562:            &quot;mov.b64            s, {0, t0};\n\t&quot;
563:            &quot;set.gt.and.u32.u64 t2, %0, s, p;\n\t&quot;
564:            &quot;sub.cc.u32         r0, r0, t2;\n\t&quot;
565:            &quot;subc.u32           r1, r1, 0;\n\t&quot;
566:            &quot;mov.b64            %0, {r0, r1};\n\t&quot;
567:            // ret += (uint32_t)(-(ret &lt; ((uint64_t)t[0] &lt;&lt; 32) &amp;&amp; t[1] != 0));
568:            &quot;set.lt.and.u32.u64 t2, %0, s, q;\n\t&quot;
569:            &quot;add.cc.u32         r0, r0, t2;\n\t&quot;
570:            &quot;addc.u32           r1, r1, 0;\n\t&quot;
571:            &quot;mov.b64            %0, {r0, r1};\n\t&quot;
572:            &quot;}&quot;
573:            : &quot;+l&quot;(x.val)
574:            : &quot;r&quot;(l));
575:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
576:        return _module11_(x.val);
577:
578:
579:
580:}
581:
582:
583:
584:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
585:WITHIN_KERNEL INLINE _module0_ _module13_(unsigned long a)
586:{
587:
588:    // uses the fact that 2 * P &gt; max(UInt64)
589:    // and that a::UInt64 - P == a + 2^32 - 1
590:
591:    _module0_ res = {a};
592:    res.val += (res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
593:    return res;
594:
595:}
596:
597:
598:
599:
600:/**
601:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
602:* @param[in] l An integer in [0, 32)
603:*/
604:WITHIN_KERNEL INLINE _module0_ _module12_(_module0_ x, unsigned int l)
605:{
606:    /*
607:    Algorithm:
608:
609:    We can decompose the shift as
610:
611:        res = x * 2^l = x * M^k * 2^j,
612:
613:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
614:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
615:
616:    After the multiplication by 2^j, the result contains 3 32-bit parts:
617:
618:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
619:
620:    Thus
621:
622:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
623:
624:    Taking the modulus P = M^2 - M + 1, we get
625:
626:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
627:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
628:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
629:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
630:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
631:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
632:
633:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
634:
635:    The processing for the things inside the parentheses is simpler:
636:
637:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
638:    - (x + y) = PACK(s &lt; y, s), where s = x + y
639:      (that is, check for overflow and add 1 in the high half)
640:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
641:      (that is, check for overflow and add (M-1) in the low half)
642:    */
643:
644:
645:
646:
647:        asm(&quot;{\n\t&quot;
648:            &quot;.reg .u32      r0, r1;\n\t&quot;
649:            &quot;.reg .u32      t0, t1, t2;\n\t&quot;
650:            &quot;.reg .u32      n;\n\t&quot;
651:            &quot;.reg .u64      s;\n\t&quot;
652:            // t[2] = (uint32_t)(x &gt;&gt; (128-l));
653:            // t[1] = (uint32_t)(x &gt;&gt; (96-l));
654:            // t[0] = (uint32_t)(x &lt;&lt; (l-64));
655:            &quot;mov.b64        {r0, r1}, %0;\n\t&quot;
656:            &quot;sub.u32        n, %1, 64;\n\t&quot;
657:            &quot;shl.b32        t0, r0, n;\n\t&quot;
658:            &quot;sub.u32        n, 32, n;\n\t&quot;
659:            &quot;shr.b64        s, %0, n;\n\t&quot;
660:            &quot;mov.b64        {t1, t2}, s;\n\t&quot;
661:            // mod P
662:            &quot;add.cc.u32     r0, t1, t0;\n\t&quot;
663:            &quot;addc.u32       r1, t2, 0;\n\t&quot;
664:            &quot;sub.u32        r1, r1, t0;\n\t&quot;
665:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
666:            // ret -= (uint32_t)(-(ret &gt; ((uint64_t *)t)[1]));
667:            &quot;mov.b64        s, {t1, t2};\n\t&quot;
668:            &quot;set.gt.u32.u64 t2, %0, s;\n\t&quot;
669:            &quot;sub.cc.u32     r0, r0, t2;\n\t&quot;
670:            &quot;subc.u32       r1, r1, 0;\n\t&quot;
671:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
672:            &quot;}&quot;
673:            : &quot;+l&quot;(x.val)
674:            : &quot;r&quot;(l));
675:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
676:        x = _module13_(x.val);
677:        x.val = 18446744069414584321UL - x.val;
678:        return x;
679:
680:
681:
682:}
683:
684:
685:
686:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
687:WITHIN_KERNEL INLINE _module0_ _module15_(unsigned long a)
688:{
689:
690:    // uses the fact that 2 * P &gt; max(UInt64)
691:    // and that a::UInt64 - P == a + 2^32 - 1
692:
693:    _module0_ res = {a};
694:    res.val += (res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
695:    return res;
696:
697:}
698:
699:
700:
701:
702:/**
703:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
704:* @param[in] l An integer in [0, 32)
705:*/
706:WITHIN_KERNEL INLINE _module0_ _module14_(_module0_ x, unsigned int l)
707:{
708:    /*
709:    Algorithm:
710:
711:    We can decompose the shift as
712:
713:        res = x * 2^l = x * M^k * 2^j,
714:
715:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
716:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
717:
718:    After the multiplication by 2^j, the result contains 3 32-bit parts:
719:
720:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
721:
722:    Thus
723:
724:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
725:
726:    Taking the modulus P = M^2 - M + 1, we get
727:
728:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
729:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
730:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
731:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
732:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
733:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
734:
735:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
736:
737:    The processing for the things inside the parentheses is simpler:
738:
739:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
740:    - (x + y) = PACK(s &lt; y, s), where s = x + y
741:      (that is, check for overflow and add 1 in the high half)
742:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
743:      (that is, check for overflow and add (M-1) in the low half)
744:    */
745:
746:
747:
748:
749:        asm(&quot;{\n\t&quot;
750:            &quot;.reg .u32      r0, r1;\n\t&quot;
751:            &quot;.reg .u32      t0, t1, t2;\n\t&quot;
752:            &quot;.reg .u32      n;\n\t&quot;
753:            &quot;.reg .u64      s;\n\t&quot;
754:            // t[2] = (uint32_t)(x &lt;&lt; (l-160));
755:            // t[1] = (uint32_t)(x &gt;&gt; (224-l));
756:            // t[0] = (uint32_t)(x &gt;&gt; (192-l));
757:            &quot;mov.b64        {r0, r1}, %0;\n\t&quot;
758:            &quot;sub.u32        n, %1, 160;\n\t&quot;
759:            &quot;shl.b32        t2, r0, n;\n\t&quot;
760:            &quot;sub.u32        n, 32, n;\n\t&quot;
761:            &quot;shr.b64        s, %0, n;\n\t&quot;
762:            &quot;mov.b64        {t0, t1}, s;\n\t&quot;
763:            // mod P
764:            &quot;add.cc.u32     r0, t0, t2;\n\t&quot;
765:            &quot;addc.u32       r1, t1, 0;\n\t&quot;
766:            &quot;sub.u32        r1, r1, t2;\n\t&quot;
767:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
768:            // ret += (uint32_t)(-(ret &gt; ((uint64_t *)t)[0]));
769:            &quot;mov.b64        s, {t0, t1};\n\t&quot;
770:            &quot;set.gt.u32.u64 t2, %0, s;\n\t&quot;
771:            &quot;sub.cc.u32     r0, r0, t2;\n\t&quot;
772:            &quot;subc.u32       r1, r1, 0;\n\t&quot;
773:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
774:            &quot;}&quot;
775:            : &quot;+l&quot;(x.val)
776:            : &quot;r&quot;(l));
777:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
778:        return _module15_(x.val);
779:
780:
781:
782:}
783:
784:
785:
786:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
787:WITHIN_KERNEL INLINE _module0_ _module17_(unsigned long a)
788:{
789:
790:    // uses the fact that 2 * P &gt; max(UInt64)
791:    // and that a::UInt64 - P == a + 2^32 - 1
792:
793:    _module0_ res = {a};
794:    res.val += (res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
795:    return res;
796:
797:}
798:
799:
800:
801:
802:/**
803:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
804:* @param[in] l An integer in [0, 32)
805:*/
806:WITHIN_KERNEL INLINE _module0_ _module16_(_module0_ x, unsigned int l)
807:{
808:    /*
809:    Algorithm:
810:
811:    We can decompose the shift as
812:
813:        res = x * 2^l = x * M^k * 2^j,
814:
815:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
816:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
817:
818:    After the multiplication by 2^j, the result contains 3 32-bit parts:
819:
820:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
821:
822:    Thus
823:
824:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
825:
826:    Taking the modulus P = M^2 - M + 1, we get
827:
828:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
829:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
830:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
831:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
832:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
833:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
834:
835:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
836:
837:    The processing for the things inside the parentheses is simpler:
838:
839:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
840:    - (x + y) = PACK(s &lt; y, s), where s = x + y
841:      (that is, check for overflow and add 1 in the high half)
842:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
843:      (that is, check for overflow and add (M-1) in the low half)
844:    */
845:
846:
847:
848:
849:        asm(&quot;{\n\t&quot;
850:            &quot;.reg .u32          r0, r1;\n\t&quot;
851:            &quot;.reg .u32          t0, t1, t2;\n\t&quot;
852:            &quot;.reg .u32          n;\n\t&quot;
853:            &quot;.reg .u64          s;\n\t&quot;
854:            &quot;.reg .pred         p, q;\n\t&quot;
855:            // t[2] = (uint32_t)(x &gt;&gt; (192-l));
856:            // t[1] = (uint32_t)(x &gt;&gt; (160-l));
857:            // t[0] = (uint32_t)(x &lt;&lt; (l-128));
858:            &quot;mov.b64            {r0, r1}, %0;\n\t&quot;
859:            &quot;sub.u32            n, %1, 128;\n\t&quot;
860:            &quot;shl.b32            t0, r0, n;\n\t&quot;
861:            &quot;sub.u32            n, 32, n;\n\t&quot;
862:            &quot;shr.b64            s, %0, n;\n\t&quot;
863:            &quot;mov.b64            {t1, t2}, s;\n\t&quot;
864:            // mod P
865:            &quot;add.u32            r1, t0, t1;\n\t&quot;
866:            &quot;sub.cc.u32         r0, 0, t1;\n\t&quot;
867:            &quot;subc.u32           r1, r1, 0;\n\t&quot;
868:            &quot;sub.cc.u32         r0, r0, t2;\n\t&quot;
869:            &quot;subc.u32           r1, r1, 0;\n\t&quot;
870:            &quot;mov.b64            %0, {r0, r1};\n\t&quot;
871:            // ret -= (uint32_t)(-(ret &gt; ((uint64_t)t[0] &lt;&lt; 32) &amp;&amp; t[1] == 0));
872:            &quot;setp.eq.u32        p|q, t1, 0;\n\t&quot;
873:            &quot;mov.b64            s, {0, t0};\n\t&quot;
874:            &quot;set.gt.and.u32.u64 t2, %0, s, p;\n\t&quot;
875:            &quot;sub.cc.u32         r0, r0, t2;\n\t&quot;
876:            &quot;subc.u32           r1, r1, 0;\n\t&quot;
877:            &quot;mov.b64            %0, {r0, r1};\n\t&quot;
878:            // ret += (uint32_t)(-(ret &lt; ((uint64_t)t[0] &lt;&lt; 32) &amp;&amp; t[1] != 0));
879:            &quot;set.lt.and.u32.u64 t2, %0, s, q;\n\t&quot;
880:            &quot;add.cc.u32         r0, r0, t2;\n\t&quot;
881:            &quot;addc.u32           r1, r1, 0;\n\t&quot;
882:            &quot;mov.b64            %0, {r0, r1};\n\t&quot;
883:            &quot;}&quot;
884:            : &quot;+l&quot;(x.val)
885:            : &quot;r&quot;(l));
886:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
887:        x = _module17_(x.val);
888:        x.val = 18446744069414584321UL - x.val;
889:        return x;
890:
891:
892:
893:}
894:
895:
896:
897:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
898:WITHIN_KERNEL INLINE _module0_ _module19_(unsigned long a)
899:{
900:
901:    // uses the fact that 2 * P &gt; max(UInt64)
902:    // and that a::UInt64 - P == a + 2^32 - 1
903:
904:    _module0_ res = {a};
905:    res.val += (res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
906:    return res;
907:
908:}
909:
910:
911:
912:
913:/**
914:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
915:* @param[in] l An integer in [0, 32)
916:*/
917:WITHIN_KERNEL INLINE _module0_ _module18_(_module0_ x, unsigned int l)
918:{
919:    /*
920:    Algorithm:
921:
922:    We can decompose the shift as
923:
924:        res = x * 2^l = x * M^k * 2^j,
925:
926:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
927:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
928:
929:    After the multiplication by 2^j, the result contains 3 32-bit parts:
930:
931:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
932:
933:    Thus
934:
935:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
936:
937:    Taking the modulus P = M^2 - M + 1, we get
938:
939:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
940:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
941:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
942:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
943:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
944:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
945:
946:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
947:
948:    The processing for the things inside the parentheses is simpler:
949:
950:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
951:    - (x + y) = PACK(s &lt; y, s), where s = x + y
952:      (that is, check for overflow and add 1 in the high half)
953:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
954:      (that is, check for overflow and add (M-1) in the low half)
955:    */
956:
957:
958:
959:
960:        asm(&quot;{\n\t&quot;
961:            &quot;.reg .u32      r0, r1;\n\t&quot;
962:            &quot;.reg .u32      t0, t1, t2;\n\t&quot;
963:            &quot;.reg .u32      n;\n\t&quot;
964:            &quot;.reg .u64      s;\n\t&quot;
965:            // t[2] = (uint32_t)(x &gt;&gt; (160-l));
966:            // t[1] = (uint32_t)(x &gt;&gt; (128-l));
967:            // t[0] = (uint32_t)(x &lt;&lt; (l-96));
968:            &quot;mov.b64        {r0, r1}, %0;\n\t&quot;
969:            &quot;sub.u32        n, %1, 96;\n\t&quot;
970:            &quot;shl.b32        t0, r0, n;\n\t&quot;
971:            &quot;sub.u32        n, 32, n;\n\t&quot;
972:            &quot;shr.b64        s, %0, n;\n\t&quot;
973:            &quot;mov.b64        {t1, t2}, s;\n\t&quot;
974:            // mod P
975:            &quot;add.u32        r1, t1, t2;\n\t&quot;
976:            &quot;sub.cc.u32     r0, t0, t2;\n\t&quot;
977:            &quot;subc.u32       r1, r1, 0;\n\t&quot;
978:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
979:            // ret += (uint32_t)(-(ret &lt; ((uint64_t *)t)[0]));
980:            &quot;mov.b64        s, {t0, t1};\n\t&quot;
981:            &quot;set.lt.u32.u64 t2, %0, s;\n\t&quot;
982:            &quot;add.cc.u32     r0, r0, t2;\n\t&quot;
983:            &quot;addc.u32       r1, r1, 0;\n\t&quot;
984:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
985:            &quot;}&quot;
986:            : &quot;+l&quot;(x.val)
987:            : &quot;r&quot;(l));
988:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
989:        x = _module19_(x.val);
990:        x.val = 18446744069414584321UL - x.val;
991:        return x;
992:
993:
994:
995:}
996:
997:
998:
999:/** Subtraction in FF(P): val_ = a + b mod P. */
1000:WITHIN_KERNEL INLINE _module0_ _module21_(_module0_ a, _module0_ b)
1001:{
1002:
1003:    /*
1004:    Algorithm:
1005:    We calculate `s = x - y`
1006:    Now there are three variants:
1007:    - no underflow (x &gt;= y): all good, `result = s`.
1008:    - underflow (detected if `s &gt; x`), so essentially `s = x - y + N`.
1009:      This means we need to calculate `s - N + P = s - (2^32 - 1)`
1010:    */
1011:
1012:    _module0_ res = {a.val - b.val};
1013:    unsigned int x = -(res.val &gt; a.val);
1014:    res.val -= x;
1015:    return res;
1016:
1017:}
1018:
1019:
1020:
1021:WITHIN_KERNEL INLINE _module0_ _module20_(_module0_ a, _module0_ b)
1022:{
1023:    /*
1024:    This function performs Montgomery multiplication with the fixed modulus (M=2**64-2**32+1)
1025:    and fixed word size (R=2**64), which helps simplify the algorithm.
1026:    The result is `a * b * R**(-1) mod M`.
1027:
1028:    Note that if you multiply two numbers `a` and `b` in Montgomery representation
1029:    (a&#x27; = a * R mod M, b&#x27; = b * R mod M), the result is the Montgomery representation of their
1030:    product (a&#x27; * b&#x27; * R**(-1) mod M = a * b * R mod M).
1031:    But if one of the numbers is in Montgomery representation and the other is not, you
1032:    get the normal product back (a * b&#x27; * R**(-1) mod M = a * b mod M).
1033:
1034:    This way if one of the factors is precalculated, you can convert it
1035:    into Montgomery representation, and use this function instead of the general
1036:    multiplication function (which is slower).
1037:    */
1038:
1039:
1040:    #ifdef CUDA
1041:    unsigned long hi = __umul64hi(a.val, b.val);
1042:    #else
1043:    unsigned long hi = mul_hi(a.val, b.val);
1044:    #endif
1045:    unsigned long lo = a.val * b.val;
1046:
1047:    unsigned long u = (lo &lt;&lt; 32) + lo;
1048:    unsigned long p2 = u - (u &gt;&gt; 32);
1049:    unsigned long uu = u &lt;&lt; 32;
1050:    if (uu &gt; u)
1051:    {
1052:        p2 -= 1;
1053:    }
1054:
1055:
1056:    _module0_ hi_ff = { hi };
1057:    _module0_ p2_ff = { p2 };
1058:
1059:    return _module21_(hi_ff, p2_ff);
1060:}
1061:
1062:
1063:
1064:
1065:
1066:#define _module5_CDATA_QUALIFIER GLOBAL_MEM_ARG
1067:
1068:
1069:WITHIN_KERNEL INLINE void _module5_swap(_module0_ *a, _module0_ *b)
1070:{
1071:    _module0_ t = *b;
1072:    *b = *a;
1073:    *a = t;
1074:}
1075:
1076:
1077:WITHIN_KERNEL INLINE void _module5_NTT2(_module0_* r0, _module0_* r1)
1078:{
1079:    _module0_ t = _module6_(*r0, *r1);
1080:    *r0 = _module7_(*r0, *r1);
1081:    *r1 = t;
1082:}
1083:
1084:
1085:WITHIN_KERNEL INLINE void _module5_NTT2_pair(_module0_* r)
1086:{
1087:    _module5_NTT2(&amp;r[0], &amp;r[1]);
1088:}
1089:
1090:
1091:WITHIN_KERNEL INLINE void _module5_NTTInv2(_module0_* r0, _module0_* r1)
1092:{
1093:    _module5_NTT2(r0, r1);
1094:}
1095:
1096:
1097:WITHIN_KERNEL INLINE void _module5_NTTInv2_pair(_module0_* r)
1098:{
1099:    _module5_NTT2(&amp;r[0], &amp;r[1]);
1100:}
1101:
1102:
1103:WITHIN_KERNEL INLINE void _module5_NTT8(_module0_* r)
1104:{
1105:    _module5_NTT2(&amp;r[0], &amp;r[4]);
1106:    _module5_NTT2(&amp;r[1], &amp;r[5]);
1107:    _module5_NTT2(&amp;r[2], &amp;r[6]);
1108:    _module5_NTT2(&amp;r[3], &amp;r[7]);
1109:    r[5] = _module8_(r[5], 24);
1110:    r[6] = _module10_(r[6], 48);
1111:    r[7] = _module12_(r[7], 72);
1112:    // instead of calling NTT4 ...
1113:    _module5_NTT2(&amp;r[0], &amp;r[2]);
1114:    _module5_NTT2(&amp;r[1], &amp;r[3]);
1115:    r[3] = _module10_(r[3], 48);
1116:    _module5_NTT2(&amp;r[4], &amp;r[6]);
1117:    _module5_NTT2(&amp;r[5], &amp;r[7]);
1118:    r[7] = _module10_(r[7], 48);
1119:    _module5_NTT2_pair(&amp;r[0]);
1120:    _module5_NTT2_pair(&amp;r[2]);
1121:    _module5_NTT2_pair(&amp;r[4]);
1122:    _module5_NTT2_pair(&amp;r[6]);
1123:    // ... we save 2 swaps (otherwise 4) here
1124:    _module5_swap(&amp;r[1], &amp;r[4]);
1125:    _module5_swap(&amp;r[3], &amp;r[6]);
1126:}
1127:
1128:
1129:WITHIN_KERNEL INLINE void _module5_NTTInv8(_module0_* r)
1130:{
1131:    _module5_NTTInv2(&amp;r[0], &amp;r[4]);
1132:    _module5_NTTInv2(&amp;r[1], &amp;r[5]);
1133:    _module5_NTTInv2(&amp;r[2], &amp;r[6]);
1134:    _module5_NTTInv2(&amp;r[3], &amp;r[7]);
1135:    r[5] = _module14_(r[5], 168);
1136:    r[6] = _module16_(r[6], 144);
1137:    r[7] = _module18_(r[7], 120);
1138:    // instead of calling NTT4 ...
1139:    _module5_NTTInv2(&amp;r[0], &amp;r[2]);
1140:    _module5_NTTInv2(&amp;r[1], &amp;r[3]);
1141:    r[3] = _module16_(r[3], 144);
1142:    _module5_NTTInv2(&amp;r[4], &amp;r[6]);
1143:    _module5_NTTInv2(&amp;r[5], &amp;r[7]);
1144:    r[7] = _module16_(r[7], 144);
1145:    _module5_NTTInv2_pair(&amp;r[0]);
1146:    _module5_NTTInv2_pair(&amp;r[2]);
1147:    _module5_NTTInv2_pair(&amp;r[4]);
1148:    _module5_NTTInv2_pair(&amp;r[6]);
1149:    // ... we save 2 swaps (otherwise 4) here
1150:    _module5_swap(&amp;r[1], &amp;r[4]);
1151:    _module5_swap(&amp;r[3], &amp;r[6]);
1152:}
1153:
1154:
1155:WITHIN_KERNEL INLINE void _module5_NTT8x2Lsh_1(_module0_* s)
1156:{
1157:    s[1] = _module8_(s[1], 12);
1158:    s[2] = _module8_(s[2], 24);
1159:    s[3] = _module10_(s[3], 36);
1160:    s[4] = _module10_(s[4], 48);
1161:    s[5] = _module10_(s[5], 60);
1162:    s[6] = _module12_(s[6], 72);
1163:    s[7] = _module12_(s[7], 84);
1164:}
1165:
1166:
1167:WITHIN_KERNEL INLINE void _module5_NTT8x2Lsh(_module0_* s, unsigned int col)
1168:{
1169:    if (1 == col)
1170:        _module5_NTT8x2Lsh_1(s);
1171:}
1172:
1173:
1174:WITHIN_KERNEL INLINE void _module5_NTTInv8x2Lsh_1(_module0_* s)
1175:{
1176:    s[1] = _module14_(s[1], 180);
1177:    s[2] = _module14_(s[2], 168);
1178:    s[3] = _module16_(s[3], 156);
1179:    s[4] = _module16_(s[4], 144);
1180:    s[5] = _module16_(s[5], 132);
1181:    s[6] = _module18_(s[6], 120);
1182:    s[7] = _module18_(s[7], 108);
1183:}
1184:
1185:
1186:WITHIN_KERNEL INLINE void _module5_NTTInv8x2Lsh(_module0_* s, unsigned int col)
1187:{
1188:    if (1 == col)
1189:        _module5_NTTInv8x2Lsh_1(s);
1190:}
1191:
1192:
1193:WITHIN_KERNEL INLINE void _module5_NTT8x8Lsh_1(_module0_* s)
1194:{
1195:    s[1] = _module8_(s[1], 3);
1196:    s[2] = _module8_(s[2], 6);
1197:    s[3] = _module8_(s[3], 9);
1198:    s[4] = _module8_(s[4], 12);
1199:    s[5] = _module8_(s[5], 15);
1200:    s[6] = _module8_(s[6], 18);
1201:    s[7] = _module8_(s[7], 21);
1202:}
1203:WITHIN_KERNEL INLINE void _module5_NTT8x8Lsh_2(_module0_* s)
1204:{
1205:    s[1] = _module8_(s[1], 6);
1206:    s[2] = _module8_(s[2], 12);
1207:    s[3] = _module8_(s[3], 18);
1208:    s[4] = _module8_(s[4], 24);
1209:    s[5] = _module8_(s[5], 30);
1210:    s[6] = _module10_(s[6], 36);
1211:    s[7] = _module10_(s[7], 42);
1212:}
1213:WITHIN_KERNEL INLINE void _module5_NTT8x8Lsh_3(_module0_* s)
1214:{
1215:    s[1] = _module8_(s[1], 9);
1216:    s[2] = _module8_(s[2], 18);
1217:    s[3] = _module8_(s[3], 27);
1218:    s[4] = _module10_(s[4], 36);
1219:    s[5] = _module10_(s[5], 45);
1220:    s[6] = _module10_(s[6], 54);
1221:    s[7] = _module10_(s[7], 63);
1222:}
1223:WITHIN_KERNEL INLINE void _module5_NTT8x8Lsh_4(_module0_* s)
1224:{
1225:    s[1] = _module8_(s[1], 12);
1226:    s[2] = _module8_(s[2], 24);
1227:    s[3] = _module10_(s[3], 36);
1228:    s[4] = _module10_(s[4], 48);
1229:    s[5] = _module10_(s[5], 60);
1230:    s[6] = _module12_(s[6], 72);
1231:    s[7] = _module12_(s[7], 84);
1232:}
1233:WITHIN_KERNEL INLINE void _module5_NTT8x8Lsh_5(_module0_* s)
1234:{
1235:    s[1] = _module8_(s[1], 15);
1236:    s[2] = _module8_(s[2], 30);
1237:    s[3] = _module10_(s[3], 45);
1238:    s[4] = _module10_(s[4], 60);
1239:    s[5] = _module12_(s[5], 75);
1240:    s[6] = _module12_(s[6], 90);
1241:    s[7] = _module18_(s[7], 105);
1242:}
1243:WITHIN_KERNEL INLINE void _module5_NTT8x8Lsh_6(_module0_* s)
1244:{
1245:    s[1] = _module8_(s[1], 18);
1246:    s[2] = _module10_(s[2], 36);
1247:    s[3] = _module10_(s[3], 54);
1248:    s[4] = _module12_(s[4], 72);
1249:    s[5] = _module12_(s[5], 90);
1250:    s[6] = _module18_(s[6], 108);
1251:    s[7] = _module18_(s[7], 126);
1252:}
1253:WITHIN_KERNEL INLINE void _module5_NTT8x8Lsh_7(_module0_* s)
1254:{
1255:    s[1] = _module8_(s[1], 21);
1256:    s[2] = _module10_(s[2], 42);
1257:    s[3] = _module10_(s[3], 63);
1258:    s[4] = _module12_(s[4], 84);
1259:    s[5] = _module18_(s[5], 105);
1260:    s[6] = _module18_(s[6], 126);
1261:    s[7] = _module16_(s[7], 147);
1262:}
1263:
1264:
1265:WITHIN_KERNEL INLINE void _module5_NTT8x8Lsh(_module0_* s, unsigned int col)
1266:{
1267:    if (1 == col)
1268:        _module5_NTT8x8Lsh_1(s);
1269:    else if (2 == col)
1270:        _module5_NTT8x8Lsh_2(s);
1271:    else if (3 == col)
1272:        _module5_NTT8x8Lsh_3(s);
1273:    else if (4 == col)
1274:        _module5_NTT8x8Lsh_4(s);
1275:    else if (5 == col)
1276:        _module5_NTT8x8Lsh_5(s);
1277:    else if (6 == col)
1278:        _module5_NTT8x8Lsh_6(s);
1279:    else if (7 == col)
1280:        _module5_NTT8x8Lsh_7(s);
1281:}
1282:
1283:
1284:WITHIN_KERNEL INLINE void _module5_NTTInv8x8Lsh_1(_module0_* s)
1285:{
1286:    s[1] = _module14_(s[1], 189);
1287:    s[2] = _module14_(s[2], 186);
1288:    s[3] = _module14_(s[3], 183);
1289:    s[4] = _module14_(s[4], 180);
1290:    s[5] = _module14_(s[5], 177);
1291:    s[6] = _module14_(s[6], 174);
1292:    s[7] = _module14_(s[7], 171);
1293:}
1294:WITHIN_KERNEL INLINE void _module5_NTTInv8x8Lsh_2(_module0_* s)
1295:{
1296:    s[1] = _module14_(s[1], 186);
1297:    s[2] = _module14_(s[2], 180);
1298:    s[3] = _module14_(s[3], 174);
1299:    s[4] = _module14_(s[4], 168);
1300:    s[5] = _module14_(s[5], 162);
1301:    s[6] = _module16_(s[6], 156);
1302:    s[7] = _module16_(s[7], 150);
1303:}
1304:WITHIN_KERNEL INLINE void _module5_NTTInv8x8Lsh_3(_module0_* s)
1305:{
1306:    s[1] = _module14_(s[1], 183);
1307:    s[2] = _module14_(s[2], 174);
1308:    s[3] = _module14_(s[3], 165);
1309:    s[4] = _module16_(s[4], 156);
1310:    s[5] = _module16_(s[5], 147);
1311:    s[6] = _module16_(s[6], 138);
1312:    s[7] = _module16_(s[7], 129);
1313:}
1314:WITHIN_KERNEL INLINE void _module5_NTTInv8x8Lsh_4(_module0_* s)
1315:{
1316:    s[1] = _module14_(s[1], 180);
1317:    s[2] = _module14_(s[2], 168);
1318:    s[3] = _module16_(s[3], 156);
1319:    s[4] = _module16_(s[4], 144);
1320:    s[5] = _module16_(s[5], 132);
1321:    s[6] = _module18_(s[6], 120);
1322:    s[7] = _module18_(s[7], 108);
1323:}
1324:WITHIN_KERNEL INLINE void _module5_NTTInv8x8Lsh_5(_module0_* s)
1325:{
1326:    s[1] = _module14_(s[1], 177);
1327:    s[2] = _module14_(s[2], 162);
1328:    s[3] = _module16_(s[3], 147);
1329:    s[4] = _module16_(s[4], 132);
1330:    s[5] = _module18_(s[5], 117);
1331:    s[6] = _module18_(s[6], 102);
1332:    s[7] = _module12_(s[7], 87);
1333:}
1334:WITHIN_KERNEL INLINE void _module5_NTTInv8x8Lsh_6(_module0_* s)
1335:{
1336:    s[1] = _module14_(s[1], 174);
1337:    s[2] = _module16_(s[2], 156);
1338:    s[3] = _module16_(s[3], 138);
1339:    s[4] = _module18_(s[4], 120);
1340:    s[5] = _module18_(s[5], 102);
1341:    s[6] = _module12_(s[6], 84);
1342:    s[7] = _module12_(s[7], 66);
1343:}
1344:WITHIN_KERNEL INLINE void _module5_NTTInv8x8Lsh_7(_module0_* s)
1345:{
1346:    s[1] = _module14_(s[1], 171);
1347:    s[2] = _module16_(s[2], 150);
1348:    s[3] = _module16_(s[3], 129);
1349:    s[4] = _module18_(s[4], 108);
1350:    s[5] = _module12_(s[5], 87);
1351:    s[6] = _module12_(s[6], 66);
1352:    s[7] = _module10_(s[7], 45);
1353:}
1354:
1355:
1356:WITHIN_KERNEL INLINE void _module5_NTTInv8x8Lsh(_module0_* s, unsigned int col)
1357:{
1358:    if (1 == col)
1359:        _module5_NTTInv8x8Lsh_1(s);
1360:    else if (2 == col)
1361:        _module5_NTTInv8x8Lsh_2(s);
1362:    else if (3 == col)
1363:        _module5_NTTInv8x8Lsh_3(s);
1364:    else if (4 == col)
1365:        _module5_NTTInv8x8Lsh_4(s);
1366:    else if (5 == col)
1367:        _module5_NTTInv8x8Lsh_5(s);
1368:    else if (6 == col)
1369:        _module5_NTTInv8x8Lsh_6(s);
1370:    else if (7 == col)
1371:        _module5_NTTInv8x8Lsh_7(s);
1372:}
1373:
1374:
1375:WITHIN_KERNEL INLINE void _module5_Index3DFrom1D(uint3 *t3d, unsigned int t1d, unsigned int dim_x, unsigned int dim_y, unsigned int dim_z)
1376:{
1377:    t3d-&gt;x = t1d % dim_x;
1378:    t1d /= dim_x;
1379:    t3d-&gt;y = t1d % dim_y;
1380:    t3d-&gt;z = t1d / dim_y;
1381:}
1382:
1383:
1384:WITHIN_KERNEL INLINE void _module5__forward(
1385:        _module0_* r,
1386:        LOCAL_MEM_ARG _module0_* s,
1387:        _module5_CDATA_QUALIFIER _module0_* twd,
1388:        const unsigned int t1d)
1389:{
1390:    uint3 t3d;
1391:    _module5_Index3DFrom1D(&amp;t3d, t1d, 8, 8, 2);
1392:
1393:    LOCAL_MEM_ARG _module0_* ptr;
1394:
1395:    _module5_NTT8(r);
1396:    _module5_NTT8x2Lsh(r, t3d.z);
1397:    ptr = &amp;s[(t3d.y &lt;&lt; 7) | (t3d.z &lt;&lt; 6) | (t3d.x &lt;&lt; 2)];
1398:    #pragma unroll
1399:    for (unsigned int i = 0; i &lt; 8; i ++)
1400:        ptr[(i &gt;&gt; 2 &lt;&lt; 5) | (i &amp; 0x3)] = r[i];
1401:    LOCAL_BARRIER;
1402:
1403:    ptr = &amp;s[(t3d.z &lt;&lt; 9) | (t3d.y &lt;&lt; 3) | t3d.x];
1404:    #pragma unroll
1405:    for (unsigned int i = 0; i &lt; 8; i ++)
1406:        r[i] = ptr[i &lt;&lt; 6];
1407:    _module5_NTT2_pair(r);
1408:    _module5_NTT2_pair(r + 2);
1409:    _module5_NTT2_pair(r + 4);
1410:    _module5_NTT2_pair(r + 6);
1411:    #pragma unroll
1412:    for (unsigned int i = 0; i &lt; 8; i ++)
1413:        ptr[i &lt;&lt; 6] = r[i];
1414:    LOCAL_BARRIER;
1415:
1416:    ptr = &amp;s[t1d];
1417:    #pragma unroll
1418:    for (unsigned int i = 0; i &lt; 8; i ++)
1419:        r[i] = _module20_(ptr[i &lt;&lt; 7], twd[i &lt;&lt; 7 | t1d]); // mult twiddle
1420:    _module5_NTT8(r);
1421:    #pragma unroll
1422:    for (unsigned int i = 0; i &lt; 8; i ++)
1423:        ptr[i &lt;&lt; 7] = r[i];
1424:    LOCAL_BARRIER;
1425:
1426:    ptr = &amp;s[(t1d &gt;&gt; 2 &lt;&lt; 5) | (t3d.x &amp; 0x3)];
1427:    #pragma unroll
1428:    for (unsigned int i = 0; i &lt; 8; i ++)
1429:        r[i] = ptr[i &lt;&lt; 2];
1430:    _module5_NTT8x8Lsh(r, t1d &gt;&gt; 4); // less divergence if put here!
1431:    _module5_NTT8(r);
1432:}
1433:
1434:
1435:WITHIN_KERNEL INLINE void _module5__inverse(
1436:        _module0_* r,
1437:        LOCAL_MEM_ARG _module0_* s,
1438:        _module5_CDATA_QUALIFIER _module0_* twd,
1439:        const unsigned int t1d)
1440:{
1441:    uint3 t3d;
1442:    _module5_Index3DFrom1D(&amp;t3d, t1d, 8, 8, 2);
1443:
1444:    LOCAL_MEM_ARG _module0_* ptr;
1445:
1446:    _module5_NTTInv8(r);
1447:    _module5_NTTInv8x2Lsh(r, t3d.z);
1448:    ptr = &amp;s[(t3d.y &lt;&lt; 7) | (t3d.z &lt;&lt; 6) | (t3d.x &lt;&lt; 2)];
1449:    #pragma unroll
1450:    for (unsigned int i = 0; i &lt; 8; i ++)
1451:        ptr[(i &gt;&gt; 2 &lt;&lt; 5) | (i &amp; 0x3)] = r[i];
1452:    LOCAL_BARRIER;
1453:
1454:    ptr = &amp;s[(t3d.z &lt;&lt; 9) | (t3d.y &lt;&lt; 3) | t3d.x];
1455:    #pragma unroll
1456:    for (unsigned int i = 0; i &lt; 8; i ++)
1457:        r[i] = ptr[i &lt;&lt; 6];
1458:    _module5_NTT2_pair(r);
1459:    _module5_NTT2_pair(r + 2);
1460:    _module5_NTT2_pair(r + 4);
1461:    _module5_NTT2_pair(r + 6);
1462:    #pragma unroll
1463:    for (unsigned int i = 0; i &lt; 8; i ++)
1464:        ptr[i &lt;&lt; 6] = r[i];
1465:    LOCAL_BARRIER;
1466:
1467:    ptr = &amp;s[t1d];
1468:    #pragma unroll
1469:    for (unsigned int i = 0; i &lt; 8; i ++)
1470:        r[i] = _module20_(ptr[i &lt;&lt; 7], twd[i &lt;&lt; 7 | t1d]); // mult twiddle
1471:    _module5_NTTInv8(r);
1472:    #pragma unroll
1473:    for (unsigned int i = 0; i &lt; 8; i ++)
1474:        ptr[i &lt;&lt; 7] = r[i];
1475:    LOCAL_BARRIER;
1476:
1477:    ptr = &amp;s[(t1d &gt;&gt; 2 &lt;&lt; 5) | (t3d.x &amp; 0x3)];
1478:    #pragma unroll
1479:        for (unsigned int i = 0; i &lt; 8; i ++)
1480:    r[i] = ptr[i &lt;&lt; 2];
1481:    _module5_NTTInv8x8Lsh(r, t1d &gt;&gt; 4); // less divergence if put here!
1482:    _module5_NTTInv8(r);
1483:}
1484:
1485:
1486:WITHIN_KERNEL INLINE void _module5_forward(
1487:        _module0_* r_out,
1488:        _module0_* r_in,
1489:        LOCAL_MEM_ARG _module0_* temp,
1490:        _module5_CDATA_QUALIFIER _module0_* cdata,
1491:        unsigned int thread_in_xform)
1492:{
1493:    // Preprocess
1494:    r_out[0] = _module20_(
1495:        r_in[0],
1496:        cdata[1024 + 0 + thread_in_xform]
1497:        );
1498:    r_out[1] = _module20_(
1499:        r_in[1],
1500:        cdata[1024 + 128 + thread_in_xform]
1501:        );
1502:    r_out[2] = _module20_(
1503:        r_in[2],
1504:        cdata[1024 + 256 + thread_in_xform]
1505:        );
1506:    r_out[3] = _module20_(
1507:        r_in[3],
1508:        cdata[1024 + 384 + thread_in_xform]
1509:        );
1510:    r_out[4] = _module20_(
1511:        r_in[4],
1512:        cdata[1024 + 512 + thread_in_xform]
1513:        );
1514:    r_out[5] = _module20_(
1515:        r_in[5],
1516:        cdata[1024 + 640 + thread_in_xform]
1517:        );
1518:    r_out[6] = _module20_(
1519:        r_in[6],
1520:        cdata[1024 + 768 + thread_in_xform]
1521:        );
1522:    r_out[7] = _module20_(
1523:        r_in[7],
1524:        cdata[1024 + 896 + thread_in_xform]
1525:        );
1526:
1527:    _module5__forward(r_out, temp, cdata, thread_in_xform);
1528:}
1529:
1530:
1531:WITHIN_KERNEL INLINE void _module5_inverse(
1532:        _module0_* r_out,
1533:        _module0_* r_in,
1534:        LOCAL_MEM_ARG _module0_* temp,
1535:        _module5_CDATA_QUALIFIER _module0_* cdata,
1536:        unsigned int thread_in_xform)
1537:{
1538:    _module5__inverse(r_in, temp, cdata, thread_in_xform);
1539:
1540:    // Postprocess
1541:    r_out[0] = _module20_(
1542:        r_in[0],
1543:        cdata[1024 + 0 + thread_in_xform]
1544:        );
1545:    r_out[1] = _module20_(
1546:        r_in[1],
1547:        cdata[1024 + 128 + thread_in_xform]
1548:        );
1549:    r_out[2] = _module20_(
1550:        r_in[2],
1551:        cdata[1024 + 256 + thread_in_xform]
1552:        );
1553:    r_out[3] = _module20_(
1554:        r_in[3],
1555:        cdata[1024 + 384 + thread_in_xform]
1556:        );
1557:    r_out[4] = _module20_(
1558:        r_in[4],
1559:        cdata[1024 + 512 + thread_in_xform]
1560:        );
1561:    r_out[5] = _module20_(
1562:        r_in[5],
1563:        cdata[1024 + 640 + thread_in_xform]
1564:        );
1565:    r_out[6] = _module20_(
1566:        r_in[6],
1567:        cdata[1024 + 768 + thread_in_xform]
1568:        );
1569:    r_out[7] = _module20_(
1570:        r_in[7],
1571:        cdata[1024 + 896 + thread_in_xform]
1572:        );
1573:}
1574:
1575:
1576:WITHIN_KERNEL INLINE _module0_ _module5_i32_to_elem(int x)
1577:{
1578:    _module0_ res = { (unsigned long)x - (unsigned int)(-(x &lt; 0)) };
1579:    return res;
1580:}
1581:
1582:
1583:WITHIN_KERNEL INLINE int _module5_ff_to_i32(_module0_ x)
1584:{
1585:    // Interpreting anything &gt; P/2 as a negative integer,
1586:    // then taking modulo 2^31
1587:    const unsigned long med = 18446744069414584321UL / 2;
1588:    return (int)(x.val) - (x.val &gt; med);
1589:}
1590:
1591:
1592:
1593:WITHIN_KERNEL INLINE void _module5_forward_i32(
1594:        _module0_* r_out,
1595:        int* r_in,
1596:        LOCAL_MEM_ARG _module0_* temp,
1597:        _module5_CDATA_QUALIFIER _module0_* cdata,
1598:        unsigned int thread_in_xform)
1599:{
1600:    r_out[0] = _module5_i32_to_elem(r_in[0]);
1601:    r_out[1] = _module5_i32_to_elem(r_in[1]);
1602:    r_out[2] = _module5_i32_to_elem(r_in[2]);
1603:    r_out[3] = _module5_i32_to_elem(r_in[3]);
1604:    r_out[4] = _module5_i32_to_elem(r_in[4]);
1605:    r_out[5] = _module5_i32_to_elem(r_in[5]);
1606:    r_out[6] = _module5_i32_to_elem(r_in[6]);
1607:    r_out[7] = _module5_i32_to_elem(r_in[7]);
1608:    _module5_forward(r_out, r_out, temp, cdata, thread_in_xform);
1609:}
1610:
1611:
1612:WITHIN_KERNEL INLINE void _module5_inverse_i32(
1613:        int* r_out,
1614:        _module0_* r_in,
1615:        LOCAL_MEM_ARG _module0_* temp,
1616:        _module5_CDATA_QUALIFIER _module0_* cdata,
1617:        unsigned int thread_in_xform)
1618:{
1619:    _module5_inverse(r_in, r_in, temp, cdata, thread_in_xform);
1620:    r_out[0] = _module5_ff_to_i32(r_in[0]);
1621:    r_out[1] = _module5_ff_to_i32(r_in[1]);
1622:    r_out[2] = _module5_ff_to_i32(r_in[2]);
1623:    r_out[3] = _module5_ff_to_i32(r_in[3]);
1624:    r_out[4] = _module5_ff_to_i32(r_in[4]);
1625:    r_out[5] = _module5_ff_to_i32(r_in[5]);
1626:    r_out[6] = _module5_ff_to_i32(r_in[6]);
1627:    r_out[7] = _module5_ff_to_i32(r_in[7]);
1628:}
1629:
1630:
1631:WITHIN_KERNEL INLINE void _module5_noop()
1632:{
1633:    LOCAL_BARRIER;
1634:    LOCAL_BARRIER;
1635:    LOCAL_BARRIER;
1636:}
1637:
1638:
1639:WITHIN_KERNEL INLINE void _module5_forward_i32_shared(
1640:        LOCAL_MEM_ARG _module0_* in_out,
1641:        LOCAL_MEM_ARG _module0_* temp,
1642:        _module5_CDATA_QUALIFIER _module0_* cdata,
1643:        unsigned int thread_in_xform)
1644:{
1645:    _module0_ r[8];
1646:    r[0] = in_out[0 + thread_in_xform];
1647:    r[1] = in_out[128 + thread_in_xform];
1648:    r[2] = in_out[256 + thread_in_xform];
1649:    r[3] = in_out[384 + thread_in_xform];
1650:    r[4] = in_out[512 + thread_in_xform];
1651:    r[5] = in_out[640 + thread_in_xform];
1652:    r[6] = in_out[768 + thread_in_xform];
1653:    r[7] = in_out[896 + thread_in_xform];
1654:    LOCAL_BARRIER;
1655:    _module5_forward(r, r, temp, cdata, thread_in_xform);
1656:    LOCAL_BARRIER;
1657:    in_out[0 + thread_in_xform] = r[0];
1658:    in_out[128 + thread_in_xform] = r[1];
1659:    in_out[256 + thread_in_xform] = r[2];
1660:    in_out[384 + thread_in_xform] = r[3];
1661:    in_out[512 + thread_in_xform] = r[4];
1662:    in_out[640 + thread_in_xform] = r[5];
1663:    in_out[768 + thread_in_xform] = r[6];
1664:    in_out[896 + thread_in_xform] = r[7];
1665:}
1666:
1667:
1668:WITHIN_KERNEL INLINE void _module5_inverse_i32_shared_add(
1669:        LOCAL_MEM_ARG int* out,
1670:        LOCAL_MEM_ARG _module0_* in,
1671:        LOCAL_MEM_ARG _module0_* temp,
1672:        _module5_CDATA_QUALIFIER _module0_* cdata,
1673:        unsigned int thread_in_xform)
1674:{
1675:    _module0_ r[8];
1676:    r[0] = in[0 + thread_in_xform];
1677:    r[1] = in[128 + thread_in_xform];
1678:    r[2] = in[256 + thread_in_xform];
1679:    r[3] = in[384 + thread_in_xform];
1680:    r[4] = in[512 + thread_in_xform];
1681:    r[5] = in[640 + thread_in_xform];
1682:    r[6] = in[768 + thread_in_xform];
1683:    r[7] = in[896 + thread_in_xform];
1684:    LOCAL_BARRIER;
1685:    _module5_inverse(r, r, temp, cdata, thread_in_xform);
1686:    LOCAL_BARRIER;
1687:    out[0 + thread_in_xform] += _module5_ff_to_i32(r[0]);
1688:    out[128 + thread_in_xform] += _module5_ff_to_i32(r[1]);
1689:    out[256 + thread_in_xform] += _module5_ff_to_i32(r[2]);
1690:    out[384 + thread_in_xform] += _module5_ff_to_i32(r[3]);
1691:    out[512 + thread_in_xform] += _module5_ff_to_i32(r[4]);
1692:    out[640 + thread_in_xform] += _module5_ff_to_i32(r[5]);
1693:    out[768 + thread_in_xform] += _module5_ff_to_i32(r[6]);
1694:    out[896 + thread_in_xform] += _module5_ff_to_i32(r[7]);
1695:}
1696:
1697:
1698:WITHIN_KERNEL INLINE void _module5_noop_shared()
1699:{
1700:    LOCAL_BARRIER;
1701:    _module5_noop();
1702:    LOCAL_BARRIER;
1703:}
1704:
1705:
1706:
1707:
1708:
1709:    // leaf input macro for &quot;gsw&quot;
1710:    #define _module22_(_idx0, _idx1, _idx2, _idx3, _idx4) (_leaf_gsw[(_idx0) * (8192) + (_idx1) * (4096) + (_idx2) * (2048) + (_idx3) * (1024) + (_idx4) * (1) + (0)])
1711:    
1712:
1713:
1714:
1715:// Addition in FF(P): val_ = a + b mod P.
1716:WITHIN_KERNEL INLINE _module0_ _module23_(_module0_ a, _module0_ b)
1717:{
1718:
1719:    /*
1720:    Algorithm:
1721:    We calculate `s = x + y`
1722:    Now there are three variants:
1723:    - `s &lt; P` and no integer overflow: all good, `result = s`.
1724:    - `s &gt; P` and no integer overflow: `result = s - P = s + (2^32 - 1)`
1725:    - integer overflow, so essentially `s = x + y - N`.
1726:      This means that we need to calculate `result = s + N - P = s + (2^32 - 1)`.
1727:    Note that the last two variants result in the same modifier being applied.
1728:    */
1729:    _module0_ res = {a.val + b.val};
1730:    res.val += ((res.val &lt; b.val) || res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
1731:    return res;
1732:
1733:}
1734:
1735:
1736:
1737:/** Subtraction in FF(P): val_ = a + b mod P. */
1738:WITHIN_KERNEL INLINE _module0_ _module25_(_module0_ a, _module0_ b)
1739:{
1740:
1741:    /*
1742:    Algorithm:
1743:    We calculate `s = x - y`
1744:    Now there are three variants:
1745:    - no underflow (x &gt;= y): all good, `result = s`.
1746:    - underflow (detected if `s &gt; x`), so essentially `s = x - y + N`.
1747:      This means we need to calculate `s - N + P = s - (2^32 - 1)`
1748:    */
1749:
1750:    _module0_ res = {a.val - b.val};
1751:    unsigned int x = -(res.val &gt; a.val);
1752:    res.val -= x;
1753:    return res;
1754:
1755:}
1756:
1757:
1758:
1759:WITHIN_KERNEL INLINE _module0_ _module24_(_module0_ a, _module0_ b)
1760:{
1761:    /*
1762:    This function performs Montgomery multiplication with the fixed modulus (M=2**64-2**32+1)
1763:    and fixed word size (R=2**64), which helps simplify the algorithm.
1764:    The result is `a * b * R**(-1) mod M`.
1765:
1766:    Note that if you multiply two numbers `a` and `b` in Montgomery representation
1767:    (a&#x27; = a * R mod M, b&#x27; = b * R mod M), the result is the Montgomery representation of their
1768:    product (a&#x27; * b&#x27; * R**(-1) mod M = a * b * R mod M).
1769:    But if one of the numbers is in Montgomery representation and the other is not, you
1770:    get the normal product back (a * b&#x27; * R**(-1) mod M = a * b mod M).
1771:
1772:    This way if one of the factors is precalculated, you can convert it
1773:    into Montgomery representation, and use this function instead of the general
1774:    multiplication function (which is slower).
1775:    */
1776:
1777:
1778:    #ifdef CUDA
1779:    unsigned long hi = __umul64hi(a.val, b.val);
1780:    #else
1781:    unsigned long hi = mul_hi(a.val, b.val);
1782:    #endif
1783:    unsigned long lo = a.val * b.val;
1784:
1785:    unsigned long u = (lo &lt;&lt; 32) + lo;
1786:    unsigned long p2 = u - (u &gt;&gt; 32);
1787:    unsigned long uu = u &lt;&lt; 32;
1788:    if (uu &gt; u)
1789:    {
1790:        p2 -= 1;
1791:    }
1792:
1793:
1794:    _module0_ hi_ff = { hi };
1795:    _module0_ p2_ff = { p2 };
1796:
1797:    return _module25_(hi_ff, p2_ff);
1798:}
1799:
1800:
1801:
1802:
1803:    // leaf output macro for &quot;lwe_b&quot;
1804:    #define _module27_(_idx0, _val) _leaf_lwe_b[(_idx0) * (1) + (0)] = (_val)
1805:    
1806:
1807:
1808:
1809:
1810:    
1811:    INLINE WITHIN_KERNEL void _module26_func(
1812:        GLOBAL_MEM int *_leaf_lwe_b, VSIZE_T _c_idx0, int _val)
1813:    {
1814:        
1815:
1816:    
1817:        
1818:        VSIZE_T _idx0 = _c_idx0 / 1;
1819:    
1820:
1821:
1822:        _module27_(_idx0, _val);
1823:    }
1824:    
1825:    #define _module26_(_c_idx0, _val) _module26_func(        _leaf_lwe_b, _c_idx0, _val)
1826:    
1827:
1828:
1829:
1830:
1831:    // leaf output macro for &quot;lwe_a&quot;
1832:    #define _module29_(_idx0, _idx1, _val) _leaf_lwe_a[(_idx0) * (1024) + (_idx1) * (1) + (0)] = (_val)
1833:    
1834:
1835:
1836:
1837:
1838:    
1839:    INLINE WITHIN_KERNEL void _module28_func(
1840:        GLOBAL_MEM int *_leaf_lwe_a, VSIZE_T _c_idx0, VSIZE_T _c_idx1, int _val)
1841:    {
1842:        
1843:
1844:    
1845:        
1846:        VSIZE_T _idx0 = _c_idx0 / 1;
1847:    
1848:        
1849:        VSIZE_T _idx1 = _c_idx1 / 1;
1850:    
1851:
1852:
1853:        _module29_(_idx0, _idx1, _val);
1854:    }
1855:    
1856:    #define _module28_(_c_idx0, _c_idx1, _val) _module28_func(        _leaf_lwe_a, _c_idx0, _c_idx1, _val)
1857:    
1858:
1859:
1860:
1861:
1862:
1863:
1864:
1865:KERNEL void blind_rotate(GLOBAL_MEM int *_leaf_lwe_a, GLOBAL_MEM int *_leaf_lwe_b, GLOBAL_MEM int *_leaf_accum_a, GLOBAL_MEM unsigned long *_leaf_gsw, GLOBAL_MEM int *_leaf_bara, GLOBAL_MEM unsigned long *_leaf__value1, GLOBAL_MEM unsigned long *_leaf__value2)
1866:
1867:{
1868:    VIRTUAL_SKIP_THREADS;
1869:
1870:    // We are trying to minimize the number of local memory buffers used.
1871:    // We need `decomp_length * (mask_size + 1)` buffers to store the transformed data,
1872:    // then all of them are used to fill `(mask_size + 1)` output buffers.
1873:    // `mask_size` of them should be put outside of the input buffers,
1874:    // but the last one can overwrite one of the input buffers.
1875:    //
1876:    // So, for example, for `mask_size=1` and `decomp_length=2`, we need
1877:    // `(1 + 1) * 2 = 4` input buffers and one additional output buffer.
1878:    LOCAL_MEM char sh_char[40960];
1879:    LOCAL_MEM int shared_accum[2048];
1880:
1881:    LOCAL_MEM_ARG _module0_* sh = (LOCAL_MEM_ARG _module0_*)sh_char;
1882:
1883:    const unsigned int batch_id = virtual_group_id(0);
1884:    const unsigned int tid = virtual_local_id(1);
1885:    const unsigned int transform_id = tid / 128;
1886:    const unsigned int mask_id = transform_id % 2;
1887:    const unsigned int decomp_id = transform_id / 2;
1888:    const unsigned int thread_in_transform = tid % 128;
1889:
1890:    // Load accum
1891:    if (tid &lt; 256)
1892:    {
1893:        #pragma unroll
1894:        for (unsigned int i = 0; i &lt; 8; i++)
1895:        {
1896:            shared_accum[mask_id * 1024 + i * 128 + thread_in_transform] =
1897:                _module1_(
1898:                    batch_id, mask_id, i * 128 + thread_in_transform);
1899:        }
1900:    }
1901:
1902:    LOCAL_BARRIER;
1903:
1904:    for (unsigned int bk_idx = 0; bk_idx &lt; 500; bk_idx++)
1905:    {
1906:
1907:    int ai = _module3_(batch_id, bk_idx);
1908:
1909:    {
1910:        
1911:
1912:        int temp0;
1913:
1914:        #pragma unroll
1915:        for (int i = tid; i &lt; 1024; i += 512)
1916:        {
1917:            int i0 = i + 0;
1918:            unsigned int cmp0 = (unsigned int)(i0 &lt; (ai &amp; 1023));
1919:            unsigned int neg0 = -(cmp0 ^ (ai &gt;&gt; 10));
1920:            unsigned int pos0 = -((1 - cmp0) ^ (ai &gt;&gt; 10));
1921:
1922:
1923:                temp0 = shared_accum[(0) | ((i0 - ai) &amp; 1023)];
1924:                temp0 = (temp0 &amp; pos0) + ((-temp0) &amp; neg0);
1925:                temp0 -= shared_accum[(0) | i0];
1926:                // decomp temp
1927:                temp0 += 2149580800;
1928:
1929:                    sh[0 + i] =
1930:                        _module5_i32_to_elem(
1931:                            ((temp0 &gt;&gt; 22)
1932:                                &amp; 1023) - 512
1933:                        );
1934:                    sh[2048 + i] =
1935:                        _module5_i32_to_elem(
1936:                            ((temp0 &gt;&gt; 12)
1937:                                &amp; 1023) - 512
1938:                        );
1939:
1940:                temp0 = shared_accum[(1024) | ((i0 - ai) &amp; 1023)];
1941:                temp0 = (temp0 &amp; pos0) + ((-temp0) &amp; neg0);
1942:                temp0 -= shared_accum[(1024) | i0];
1943:                // decomp temp
1944:                temp0 += 2149580800;
1945:
1946:                    sh[1024 + i] =
1947:                        _module5_i32_to_elem(
1948:                            ((temp0 &gt;&gt; 22)
1949:                                &amp; 1023) - 512
1950:                        );
1951:                    sh[3072 + i] =
1952:                        _module5_i32_to_elem(
1953:                            ((temp0 &gt;&gt; 12)
1954:                                &amp; 1023) - 512
1955:                        );
1956:        }
1957:    }
1958:
1959:    LOCAL_BARRIER;
1960:
1961:    if (tid &lt; 512)
1962:    {
1963:        // Forward transform
1964:        _module5_forward_i32_shared(
1965:            sh + (decomp_id * 2 + mask_id) * 1024,
1966:            (LOCAL_MEM_ARG _module0_*)(
1967:                sh + (decomp_id * 2 + mask_id) * 1024),
1968:            (_module5_CDATA_QUALIFIER _module0_*)_leaf__value1,
1969:            thread_in_transform);
1970:    }
1971:    else
1972:    {
1973:        _module5_noop_shared();
1974:    }
1975:
1976:    LOCAL_BARRIER;
1977:
1978:    {
1979:        _module0_ t, a, b;
1980:        {
1981:            int idx = 0 + tid;
1982:
1983:
1984:            t = _module0_zero;
1985:            a = sh[0 + idx];
1986:            b = _module0_pack(
1987:                    _module22_(bk_idx, 0, 0, 1, idx));
1988:            t = _module23_(t, _module24_(a, b));
1989:            a = sh[2048 + idx];
1990:            b = _module0_pack(
1991:                    _module22_(bk_idx, 0, 1, 1, idx));
1992:            t = _module23_(t, _module24_(a, b));
1993:            a = sh[1024 + idx];
1994:            b = _module0_pack(
1995:                    _module22_(bk_idx, 1, 0, 1, idx));
1996:            t = _module23_(t, _module24_(a, b));
1997:            a = sh[3072 + idx];
1998:            b = _module0_pack(
1999:                    _module22_(bk_idx, 1, 1, 1, idx));
2000:            t = _module23_(t, _module24_(a, b));
2001:
2002:            
2003:            sh[4096 + idx] = t;
2004:
2005:        }
2006:        {
2007:            int idx = 512 + tid;
2008:
2009:
2010:            t = _module0_zero;
2011:            a = sh[0 + idx];
2012:            b = _module0_pack(
2013:                    _module22_(bk_idx, 0, 0, 1, idx));
2014:            t = _module23_(t, _module24_(a, b));
2015:            a = sh[2048 + idx];
2016:            b = _module0_pack(
2017:                    _module22_(bk_idx, 0, 1, 1, idx));
2018:            t = _module23_(t, _module24_(a, b));
2019:            a = sh[1024 + idx];
2020:            b = _module0_pack(
2021:                    _module22_(bk_idx, 1, 0, 1, idx));
2022:            t = _module23_(t, _module24_(a, b));
2023:            a = sh[3072 + idx];
2024:            b = _module0_pack(
2025:                    _module22_(bk_idx, 1, 1, 1, idx));
2026:            t = _module23_(t, _module24_(a, b));
2027:
2028:            
2029:            sh[4096 + idx] = t;
2030:
2031:        }
2032:    }
2033:    LOCAL_BARRIER;
2034:    {
2035:        _module0_ t, a, b;
2036:        {
2037:            int idx = 0 + tid;
2038:
2039:
2040:            t = _module0_zero;
2041:            a = sh[0 + idx];
2042:            b = _module0_pack(
2043:                    _module22_(bk_idx, 0, 0, 0, idx));
2044:            t = _module23_(t, _module24_(a, b));
2045:            a = sh[2048 + idx];
2046:            b = _module0_pack(
2047:                    _module22_(bk_idx, 0, 1, 0, idx));
2048:            t = _module23_(t, _module24_(a, b));
2049:            a = sh[1024 + idx];
2050:            b = _module0_pack(
2051:                    _module22_(bk_idx, 1, 0, 0, idx));
2052:            t = _module23_(t, _module24_(a, b));
2053:            a = sh[3072 + idx];
2054:            b = _module0_pack(
2055:                    _module22_(bk_idx, 1, 1, 0, idx));
2056:            t = _module23_(t, _module24_(a, b));
2057:
2058:            
2059:            sh[0 + idx] = t;
2060:
2061:        }
2062:        {
2063:            int idx = 512 + tid;
2064:
2065:
2066:            t = _module0_zero;
2067:            a = sh[0 + idx];
2068:            b = _module0_pack(
2069:                    _module22_(bk_idx, 0, 0, 0, idx));
2070:            t = _module23_(t, _module24_(a, b));
2071:            a = sh[2048 + idx];
2072:            b = _module0_pack(
2073:                    _module22_(bk_idx, 0, 1, 0, idx));
2074:            t = _module23_(t, _module24_(a, b));
2075:            a = sh[1024 + idx];
2076:            b = _module0_pack(
2077:                    _module22_(bk_idx, 1, 0, 0, idx));
2078:            t = _module23_(t, _module24_(a, b));
2079:            a = sh[3072 + idx];
2080:            b = _module0_pack(
2081:                    _module22_(bk_idx, 1, 1, 0, idx));
2082:            t = _module23_(t, _module24_(a, b));
2083:
2084:            
2085:            sh[0 + idx] = t;
2086:
2087:        }
2088:    }
2089:    LOCAL_BARRIER;
2090:
2091:    // Inverse transform
2092:    if (tid &lt; 256)
2093:    {
2094:        // Following the temporary buffer usage scheme described at the beginning of the kernel.
2095:        int temp_id = mask_id == 0 ? 0 : 3 + mask_id;
2096:
2097:        _module5_inverse_i32_shared_add(
2098:            shared_accum + mask_id * 1024,
2099:            sh + temp_id * 1024,
2100:            (LOCAL_MEM_ARG _module0_*)(sh + temp_id * 1024),
2101:            (_module5_CDATA_QUALIFIER _module0_*)_leaf__value2,
2102:            thread_in_transform);
2103:    }
2104:    else
2105:    {
2106:        _module5_noop_shared();
2107:    }
2108:
2109:    LOCAL_BARRIER;
2110:    }
2111:
2112:    for (int i = tid; i &lt;= 1024; i += 512)
2113:    {
2114:        if (i == 1024)
2115:        {
2116:            _module26_(batch_id, shared_accum[1024]);
2117:        }
2118:        else
2119:        {
2120:            _module28_(
2121:                batch_id, i, i == 0 ? shared_accum[0] : -shared_accum[1024 - i]);
2122:        }
2123:    }
2124:}
2125:<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_single_kernel_bs_performance[bs_kernel-cuda:0:0-NTT-uint_min]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f8839c518&gt;, transform_type = &#x27;NTT&#x27;<br/>single_kernel_bootstrap = True, test_function_name = &#x27;uint_min&#x27;<br/>heavy_performance_load = False<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(&#x27;test_function_name&#x27;, [&#x27;NAND&#x27;, &#x27;MUX&#x27;, &#x27;uint_min&#x27;])<br/>    def test_single_kernel_bs_performance(<br/>            thread, transform_type, single_kernel_bootstrap,<br/>            test_function_name, heavy_performance_load):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        test_function = dict(<br/>            NAND=(gate_nand, nand_ref, 2),<br/>            MUX=(gate_mux, mux_ref, 3),<br/>            uint_min=(uint_min, uint_min_ref, 2),<br/>            )[test_function_name]<br/>    <br/>        if test_function_name == &#x27;uint_min&#x27;:<br/>            shape = (128, 32) if heavy_performance_load else (4, 16)<br/>        else:<br/>            shape = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/><br/>test/test_gates.py:338: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f8839c518&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_single_kernel_bs_performance[bs_kernel-cuda:0:0-FFT-NAND]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f8839c518&gt;, transform_type = &#x27;FFT&#x27;<br/>single_kernel_bootstrap = True, test_function_name = &#x27;NAND&#x27;, heavy_performance_load = False<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(&#x27;test_function_name&#x27;, [&#x27;NAND&#x27;, &#x27;MUX&#x27;, &#x27;uint_min&#x27;])<br/>    def test_single_kernel_bs_performance(<br/>            thread, transform_type, single_kernel_bootstrap,<br/>            test_function_name, heavy_performance_load):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        test_function = dict(<br/>            NAND=(gate_nand, nand_ref, 2),<br/>            MUX=(gate_mux, mux_ref, 3),<br/>            uint_min=(uint_min, uint_min_ref, 2),<br/>            )[test_function_name]<br/>    <br/>        if test_function_name == &#x27;uint_min&#x27;:<br/>            shape = (128, 32) if heavy_performance_load else (4, 16)<br/>        else:<br/>            shape = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/><br/>test/test_gates.py:338: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f8839c518&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_single_kernel_bs_performance[bs_kernel-cuda:0:0-FFT-MUX]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f8839c518&gt;, transform_type = &#x27;FFT&#x27;<br/>single_kernel_bootstrap = True, test_function_name = &#x27;MUX&#x27;, heavy_performance_load = False<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(&#x27;test_function_name&#x27;, [&#x27;NAND&#x27;, &#x27;MUX&#x27;, &#x27;uint_min&#x27;])<br/>    def test_single_kernel_bs_performance(<br/>            thread, transform_type, single_kernel_bootstrap,<br/>            test_function_name, heavy_performance_load):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        test_function = dict(<br/>            NAND=(gate_nand, nand_ref, 2),<br/>            MUX=(gate_mux, mux_ref, 3),<br/>            uint_min=(uint_min, uint_min_ref, 2),<br/>            )[test_function_name]<br/>    <br/>        if test_function_name == &#x27;uint_min&#x27;:<br/>            shape = (128, 32) if heavy_performance_load else (4, 16)<br/>        else:<br/>            shape = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/><br/>test/test_gates.py:338: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f8839c518&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_single_kernel_bs_performance[bs_kernel-cuda:0:0-FFT-uint_min]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f8839c518&gt;, transform_type = &#x27;FFT&#x27;<br/>single_kernel_bootstrap = True, test_function_name = &#x27;uint_min&#x27;<br/>heavy_performance_load = False<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(&#x27;test_function_name&#x27;, [&#x27;NAND&#x27;, &#x27;MUX&#x27;, &#x27;uint_min&#x27;])<br/>    def test_single_kernel_bs_performance(<br/>            thread, transform_type, single_kernel_bootstrap,<br/>            test_function_name, heavy_performance_load):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        test_function = dict(<br/>            NAND=(gate_nand, nand_ref, 2),<br/>            MUX=(gate_mux, mux_ref, 3),<br/>            uint_min=(uint_min, uint_min_ref, 2),<br/>            )[test_function_name]<br/>    <br/>        if test_function_name == &#x27;uint_min&#x27;:<br/>            shape = (128, 32) if heavy_performance_load else (4, 16)<br/>        else:<br/>            shape = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/><br/>test/test_gates.py:338: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f8839c518&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_constant_mem_performance[bs_kernel-cuda:0:0-NTT-global_mem]</td>
          <td class="col-duration">13.41</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f8848c6a0&gt;, transform_type = &#x27;NTT&#x27;<br/>single_kernel_bootstrap = True, heavy_performance_load = False, use_constant_memory = False<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(&#x27;use_constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_constant_mem_performance(<br/>            thread, transform_type, single_kernel_bootstrap, heavy_performance_load,<br/>            use_constant_memory):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        # We want to test the effect of using constant memory on the bootstrap calculation.<br/>        # A single-kernel bootstrap uses the `use_constant_memory_multi_iter` option,<br/>        # and a multi-kernel bootstrap uses the `use_constant_memory_single_iter` option.<br/>        kwds = dict(single_kernel_bootstrap=single_kernel_bootstrap)<br/>        if single_kernel_bootstrap:<br/>            kwds.update(dict(use_constant_memory_multi_iter=use_constant_memory))<br/>        else:<br/>            kwds.update(dict(use_constant_memory_single_iter=use_constant_memory))<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>        secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/>    <br/>        # TODO: instead of creating a whole key and then checking if the parameters are supported,<br/>        # we can just create a parameter object separately.<br/>        if (single_kernel_bootstrap<br/>                and not single_kernel_bootstrap_supported(secret_key.params, thread.device_params)):<br/>            pytest.skip()<br/>    <br/>        perf_params = PerformanceParameters(secret_key.params, **kwds).for_device(thread.device_params)<br/>    <br/>&gt;       results = check_performance(thread, (secret_key, cloud_key), perf_params, shape=size)<br/><br/>test/test_gates.py:387: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_gates.py:276: in check_performance<br/>    shape=shape1, performance_test=True, perf_params=perf_params)<br/>test/test_gates.py:65: in check_gate<br/>    nufhe_func(thread, cloud_key, answer, *ciphertexts, perf_params)<br/>nufhe/gates.py:121: in gate_nand<br/>    MU, temp_result, perf_params)<br/>nufhe/bootstrap.py:229: in bootstrap<br/>    no_keyswitch=no_keyswitch)<br/>nufhe/bootstrap.py:186: in blind_rotate_and_extract<br/>    BlindRotate_gpu(result, acc, bk, ks, bara, perf_params, no_keyswitch=no_keyswitch)<br/>nufhe/blind_rotate.py:278: in BlindRotate_gpu<br/>    ks.log2_base, ks.decomp_length, perf_params)<br/>nufhe/computation_cache.py:55: in get_computation<br/>    compiled_comp = comp.compile(thr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/blind_rotate.py:250: in _build_plan<br/>    plan.computation_call(blind_rotate, extracted_a, extracted_b, accum_a, gsw, bara)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:502: in computation_call<br/>    self._compiler_options, self._keep))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;nufhe.blind_rotate.BlindRotate object at 0x7f88359ba8&gt;<br/>plan_factory = &lt;function Computation._get_plan.&lt;locals&gt;.&lt;lambda&gt; at 0x7f88559510&gt;<br/>device_params = &lt;reikna.cluda.cuda.DeviceParameters object at 0x7f8848c048&gt;<br/>lwe_a = KernelArgument(_temp1), lwe_b = KernelArgument(_temp2)<br/>accum_a = KernelArgument(accum_a), gsw = KernelArgument(gsw), bara = KernelArgument(bara)<br/><br/>    def _build_plan(self, plan_factory, device_params, lwe_a, lwe_b, accum_a, gsw, bara):<br/>    <br/>        params = self._params<br/>        tlwe_params = params.tlwe_params<br/>        decomp_length = params.decomp_length<br/>        mask_size = tlwe_params.mask_size<br/>    <br/>        perf_params = self._perf_params<br/>        transform_type = self._params.tlwe_params.transform_type<br/>        transform = get_transform(transform_type)<br/>    <br/>        transform_module = transform.transform_module(perf_params, multi_iter=True)<br/>    <br/>        batch_shape = accum_a.shape[:-2]<br/>    <br/>        min_local_size = decomp_length * (mask_size + 1) * transform_module.threads_per_transform<br/>        local_size = device_params.max_work_group_size<br/>        while local_size &gt;= min_local_size:<br/>    <br/>            plan = plan_factory()<br/>    <br/>            if transform_module.use_constant_memory:<br/>                cdata_forward = plan.constant_array(transform_module.cdata_fw)<br/>                cdata_inverse = plan.constant_array(transform_module.cdata_inv)<br/>            else:<br/>                cdata_forward = plan.persistent_array(transform_module.cdata_fw)<br/>                cdata_inverse = plan.persistent_array(transform_module.cdata_inv)<br/>    <br/>            try:<br/>                plan.kernel_call(<br/>                    TEMPLATE.get_def(&quot;blind_rotate&quot;),<br/>                    [lwe_a, lwe_b, accum_a, gsw, bara, cdata_forward, cdata_inverse],<br/>                    kernel_name=&quot;blind_rotate&quot;,<br/>                    global_size=(<br/>                        helpers.product(batch_shape),<br/>                        local_size),<br/>                    local_size=(1, local_size),<br/>                    render_kwds=dict(<br/>                        local_size=local_size,<br/>                        slices=(len(batch_shape), 1, 1),<br/>                        slices2=(len(batch_shape), 1),<br/>                        slices3=(len(batch_shape),),<br/>                        transform=transform_module,<br/>                        mask_size=mask_size,<br/>                        decomp_length=decomp_length,<br/>                        output_size=self._in_out_params.size,<br/>                        input_size=tlwe_params.extracted_lweparams.size,<br/>                        bs_log2_base=self._params.bs_log2_base,<br/>                        mul_prepared=transform.transformed_mul_prepared(perf_params),<br/>                        add=transform.transformed_add(perf_params),<br/>                        tr_ctype=transform.transformed_internal_ctype(),<br/>                        min_blocks=helpers.min_blocks,<br/>                        )<br/>                    )<br/>            except OutOfResourcesError:<br/>                local_size -= transform_module.threads_per_transform<br/>                continue<br/>    <br/>            return plan<br/>    <br/>&gt;       raise ValueError(&quot;Could not find suitable local size for the kernel&quot;)<br/><span class="error">E       ValueError: Could not find suitable local size for the kernel</span><br/><br/>nufhe/blind_rotate.py:187: ValueError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_constant_mem_performance[bs_kernel-cuda:0:0-NTT-constant_mem]</td>
          <td class="col-duration">5.90</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f8848c6a0&gt;, transform_type = &#x27;NTT&#x27;<br/>single_kernel_bootstrap = True, heavy_performance_load = False, use_constant_memory = True<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(&#x27;use_constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_constant_mem_performance(<br/>            thread, transform_type, single_kernel_bootstrap, heavy_performance_load,<br/>            use_constant_memory):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        # We want to test the effect of using constant memory on the bootstrap calculation.<br/>        # A single-kernel bootstrap uses the `use_constant_memory_multi_iter` option,<br/>        # and a multi-kernel bootstrap uses the `use_constant_memory_single_iter` option.<br/>        kwds = dict(single_kernel_bootstrap=single_kernel_bootstrap)<br/>        if single_kernel_bootstrap:<br/>            kwds.update(dict(use_constant_memory_multi_iter=use_constant_memory))<br/>        else:<br/>            kwds.update(dict(use_constant_memory_single_iter=use_constant_memory))<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>        secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/>    <br/>        # TODO: instead of creating a whole key and then checking if the parameters are supported,<br/>        # we can just create a parameter object separately.<br/>        if (single_kernel_bootstrap<br/>                and not single_kernel_bootstrap_supported(secret_key.params, thread.device_params)):<br/>            pytest.skip()<br/>    <br/>        perf_params = PerformanceParameters(secret_key.params, **kwds).for_device(thread.device_params)<br/>    <br/>&gt;       results = check_performance(thread, (secret_key, cloud_key), perf_params, shape=size)<br/><br/>test/test_gates.py:387: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_gates.py:276: in check_performance<br/>    shape=shape1, performance_test=True, perf_params=perf_params)<br/>test/test_gates.py:65: in check_gate<br/>    nufhe_func(thread, cloud_key, answer, *ciphertexts, perf_params)<br/>nufhe/gates.py:121: in gate_nand<br/>    MU, temp_result, perf_params)<br/>nufhe/bootstrap.py:229: in bootstrap<br/>    no_keyswitch=no_keyswitch)<br/>nufhe/bootstrap.py:186: in blind_rotate_and_extract<br/>    BlindRotate_gpu(result, acc, bk, ks, bara, perf_params, no_keyswitch=no_keyswitch)<br/>nufhe/blind_rotate.py:278: in BlindRotate_gpu<br/>    ks.log2_base, ks.decomp_length, perf_params)<br/>nufhe/computation_cache.py:55: in get_computation<br/>    compiled_comp = comp.compile(thr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/blind_rotate.py:250: in _build_plan<br/>    plan.computation_call(blind_rotate, extracted_a, extracted_b, accum_a, gsw, bara)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:502: in computation_call<br/>    self._compiler_options, self._keep))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;nufhe.blind_rotate.BlindRotate object at 0x7f882805f8&gt;<br/>plan_factory = &lt;function Computation._get_plan.&lt;locals&gt;.&lt;lambda&gt; at 0x7f88546b70&gt;<br/>device_params = &lt;reikna.cluda.cuda.DeviceParameters object at 0x7f8848c048&gt;<br/>lwe_a = KernelArgument(_temp1), lwe_b = KernelArgument(_temp2)<br/>accum_a = KernelArgument(accum_a), gsw = KernelArgument(gsw), bara = KernelArgument(bara)<br/><br/>    def _build_plan(self, plan_factory, device_params, lwe_a, lwe_b, accum_a, gsw, bara):<br/>    <br/>        params = self._params<br/>        tlwe_params = params.tlwe_params<br/>        decomp_length = params.decomp_length<br/>        mask_size = tlwe_params.mask_size<br/>    <br/>        perf_params = self._perf_params<br/>        transform_type = self._params.tlwe_params.transform_type<br/>        transform = get_transform(transform_type)<br/>    <br/>        transform_module = transform.transform_module(perf_params, multi_iter=True)<br/>    <br/>        batch_shape = accum_a.shape[:-2]<br/>    <br/>        min_local_size = decomp_length * (mask_size + 1) * transform_module.threads_per_transform<br/>        local_size = device_params.max_work_group_size<br/>        while local_size &gt;= min_local_size:<br/>    <br/>            plan = plan_factory()<br/>    <br/>            if transform_module.use_constant_memory:<br/>                cdata_forward = plan.constant_array(transform_module.cdata_fw)<br/>                cdata_inverse = plan.constant_array(transform_module.cdata_inv)<br/>            else:<br/>                cdata_forward = plan.persistent_array(transform_module.cdata_fw)<br/>                cdata_inverse = plan.persistent_array(transform_module.cdata_inv)<br/>    <br/>            try:<br/>                plan.kernel_call(<br/>                    TEMPLATE.get_def(&quot;blind_rotate&quot;),<br/>                    [lwe_a, lwe_b, accum_a, gsw, bara, cdata_forward, cdata_inverse],<br/>                    kernel_name=&quot;blind_rotate&quot;,<br/>                    global_size=(<br/>                        helpers.product(batch_shape),<br/>                        local_size),<br/>                    local_size=(1, local_size),<br/>                    render_kwds=dict(<br/>                        local_size=local_size,<br/>                        slices=(len(batch_shape), 1, 1),<br/>                        slices2=(len(batch_shape), 1),<br/>                        slices3=(len(batch_shape),),<br/>                        transform=transform_module,<br/>                        mask_size=mask_size,<br/>                        decomp_length=decomp_length,<br/>                        output_size=self._in_out_params.size,<br/>                        input_size=tlwe_params.extracted_lweparams.size,<br/>                        bs_log2_base=self._params.bs_log2_base,<br/>                        mul_prepared=transform.transformed_mul_prepared(perf_params),<br/>                        add=transform.transformed_add(perf_params),<br/>                        tr_ctype=transform.transformed_internal_ctype(),<br/>                        min_blocks=helpers.min_blocks,<br/>                        )<br/>                    )<br/>            except OutOfResourcesError:<br/>                local_size -= transform_module.threads_per_transform<br/>                continue<br/>    <br/>            return plan<br/>    <br/>&gt;       raise ValueError(&quot;Could not find suitable local size for the kernel&quot;)<br/><span class="error">E       ValueError: Could not find suitable local size for the kernel</span><br/><br/>nufhe/blind_rotate.py:187: ValueError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_constant_mem_performance[bs_kernel-cuda:0:0-FFT-global_mem]</td>
          <td class="col-duration">41.19</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f8848c6a0&gt;, transform_type = &#x27;FFT&#x27;<br/>single_kernel_bootstrap = True, heavy_performance_load = False, use_constant_memory = False<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(&#x27;use_constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_constant_mem_performance(<br/>            thread, transform_type, single_kernel_bootstrap, heavy_performance_load,<br/>            use_constant_memory):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        # We want to test the effect of using constant memory on the bootstrap calculation.<br/>        # A single-kernel bootstrap uses the `use_constant_memory_multi_iter` option,<br/>        # and a multi-kernel bootstrap uses the `use_constant_memory_single_iter` option.<br/>        kwds = dict(single_kernel_bootstrap=single_kernel_bootstrap)<br/>        if single_kernel_bootstrap:<br/>            kwds.update(dict(use_constant_memory_multi_iter=use_constant_memory))<br/>        else:<br/>            kwds.update(dict(use_constant_memory_single_iter=use_constant_memory))<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>        secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/>    <br/>        # TODO: instead of creating a whole key and then checking if the parameters are supported,<br/>        # we can just create a parameter object separately.<br/>        if (single_kernel_bootstrap<br/>                and not single_kernel_bootstrap_supported(secret_key.params, thread.device_params)):<br/>            pytest.skip()<br/>    <br/>        perf_params = PerformanceParameters(secret_key.params, **kwds).for_device(thread.device_params)<br/>    <br/>&gt;       results = check_performance(thread, (secret_key, cloud_key), perf_params, shape=size)<br/><br/>test/test_gates.py:387: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_gates.py:279: in check_performance<br/>    shape=shape2, performance_test=True, perf_params=perf_params)<br/>test/test_gates.py:55: in check_gate<br/>    ciphertexts = [encrypt(thread, rng, secret_key, plaintext) for plaintext in plaintexts]<br/>test/test_gates.py:55: in &lt;listcomp&gt;<br/>    ciphertexts = [encrypt(thread, rng, secret_key, plaintext) for plaintext in plaintexts]<br/>nufhe/api_low_level.py:280: in encrypt<br/>    lwe_encrypt(thr, rng, result, mus, noise, key.lwe_key)<br/>nufhe/lwe.py:332: in lwe_encrypt<br/>    comp = get_computation(thr, LweEncrypt, messages.shape, lwe_size, noise)<br/>nufhe/computation_cache.py:55: in get_computation<br/>    compiled_comp = comp.compile(thr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/lwe_gpu.py:238: in _build_plan<br/>    plan.computation_call(mul_key, result_b, result_cv, messages, noises_b, noises_a, key)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:502: in computation_call<br/>    self._compiler_options, self._keep))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/lwe_gpu.py:58: in _build_plan<br/>    plan.computation_call(summation, output, matrix, vector)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:502: in computation_call<br/>    self._compiler_options, self._keep))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/algorithms/reduce.py:168: in _build_plan<br/>    plan_factory, device_params.warp_size, max_wg_size, output, input_)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/algorithms/reduce.py:152: in _build_plan_for_wg_size<br/>    render_kwds=render_kwds)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:473: in kernel_call<br/>    keep=self._keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:566: in compile_static<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:786: in __init__<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:655: in __init__<br/>    self.source, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:504: in _create_program<br/>    src, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:227: in _compile<br/>    return SourceModule(src, no_extern_c=True, options=options, keep=keep)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;pycuda.compiler.SourceModule object at 0x7f881ca9e8&gt;<br/>source = &#x27;\n\n\n    #define CUDA\n    // taken from pycuda._cluda\n    #define LOCAL_BARRIER __syncthreads()\n\n    #define WIT...BARRIER;\n\n    if (tid == 0)\n    {\n        v = local_mem_[0];\n\n        _module6_(part_num, bid, v);\n    }\n}\n\n&#x27;<br/>nvcc = &#x27;nvcc&#x27;, options = [], keep = False, no_extern_c = True, arch = None, code = None<br/>cache_dir = None, include_dirs = []<br/><br/>    def __init__(self, source, nvcc=&quot;nvcc&quot;, options=None, keep=False,<br/>            no_extern_c=False, arch=None, code=None, cache_dir=None,<br/>            include_dirs=[]):<br/>        self._check_arch(arch)<br/>    <br/>        cubin = compile(source, nvcc, options, keep, no_extern_c,<br/>                arch, code, cache_dir, include_dirs)<br/>    <br/>        from pycuda.driver import module_from_buffer<br/>&gt;       self.module = module_from_buffer(cubin)<br/><span class="error">E       pycuda._driver.LogicError: cuModuleLoadDataEx failed: invalid device context -</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/compiler.py:294: LogicError<br/> -------------------------------Captured log call-------------------------------- <br/>[1m[31mERROR   [0m root:api.py:507 Failed to compile:
1:
2:
3:
4:    #define CUDA
5:    // taken from pycuda._cluda
6:    #define LOCAL_BARRIER __syncthreads()
7:
8:    #define WITHIN_KERNEL __device__
9:    #define KERNEL extern &quot;C&quot; __global__
10:    #define GLOBAL_MEM /* empty */
11:    #define GLOBAL_MEM_ARG /* empty */
12:    #define LOCAL_MEM __shared__
13:    #define LOCAL_MEM_DYNAMIC extern __shared__
14:    #define LOCAL_MEM_ARG /* empty */
15:    #define CONSTANT_MEM __constant__
16:    #define CONSTANT_MEM_ARG /* empty */
17:    #define INLINE __forceinline__
18:    #define SIZE_T int
19:    #define VSIZE_T int
20:
21:    // used to align fields in structures
22:    #define ALIGN(bytes) __align__(bytes)
23:
24:    
25:
26:    WITHIN_KERNEL SIZE_T get_local_id(unsigned int dim)
27:    {
28:        if(dim == 0) return threadIdx.x;
29:        if(dim == 1) return threadIdx.y;
30:        if(dim == 2) return threadIdx.z;
31:        return 0;
32:    }
33:
34:    WITHIN_KERNEL SIZE_T get_group_id(unsigned int dim)
35:    {
36:        if(dim == 0) return blockIdx.x;
37:        if(dim == 1) return blockIdx.y;
38:        if(dim == 2) return blockIdx.z;
39:        return 0;
40:    }
41:
42:    WITHIN_KERNEL SIZE_T get_local_size(unsigned int dim)
43:    {
44:        if(dim == 0) return blockDim.x;
45:        if(dim == 1) return blockDim.y;
46:        if(dim == 2) return blockDim.z;
47:        return 1;
48:    }
49:
50:    WITHIN_KERNEL SIZE_T get_num_groups(unsigned int dim)
51:    {
52:        if(dim == 0) return gridDim.x;
53:        if(dim == 1) return gridDim.y;
54:        if(dim == 2) return gridDim.z;
55:        return 1;
56:    }
57:
58:    WITHIN_KERNEL SIZE_T get_global_size(unsigned int dim)
59:    {
60:        return get_num_groups(dim) * get_local_size(dim);
61:    }
62:
63:    WITHIN_KERNEL SIZE_T get_global_id(unsigned int dim)
64:    {
65:        return get_local_id(dim) + get_group_id(dim) * get_local_size(dim);
66:    }
67:
68:
69:
70:
71:    #define COMPLEX_CTR(T) make_##T
72:
73:    WITHIN_KERNEL float2 operator+(float2 a, float2 b)
74:    {
75:        return COMPLEX_CTR(float2)(a.x + b.x, a.y + b.y);
76:    }
77:    WITHIN_KERNEL float2 operator-(float2 a, float2 b)
78:    {
79:        return COMPLEX_CTR(float2)(a.x - b.x, a.y - b.y);
80:    }
81:    WITHIN_KERNEL float2 operator+(float2 a) { return a; }
82:    WITHIN_KERNEL float2 operator-(float2 a) { return COMPLEX_CTR(float2)(-a.x, -a.y); }
83:    WITHIN_KERNEL double2 operator+(double2 a, double2 b)
84:    {
85:        return COMPLEX_CTR(double2)(a.x + b.x, a.y + b.y);
86:    }
87:    WITHIN_KERNEL double2 operator-(double2 a, double2 b)
88:    {
89:        return COMPLEX_CTR(double2)(a.x - b.x, a.y - b.y);
90:    }
91:    WITHIN_KERNEL double2 operator+(double2 a) { return a; }
92:    WITHIN_KERNEL double2 operator-(double2 a) { return COMPLEX_CTR(double2)(-a.x, -a.y); }
93:
94:WITHIN_KERNEL VSIZE_T virtual_local_id(unsigned int dim)
95:{
96:    if (dim == 1)
97:    {
98:
99:        SIZE_T flat_id =
100:            get_local_id(0) * 1 +
101:            0;
102:
103:        return (flat_id / 1);
104:
105:    }
106:    if (dim == 0)
107:    {
108:
109:        return 0;
110:
111:    }
112:
113:    return 0;
114:}
115:
116:WITHIN_KERNEL VSIZE_T virtual_local_size(unsigned int dim)
117:{
118:    if (dim == 1)
119:    {
120:        return 512;
121:    }
122:    if (dim == 0)
123:    {
124:        return 1;
125:    }
126:
127:    return 1;
128:}
129:
130:WITHIN_KERNEL VSIZE_T virtual_group_id(unsigned int dim)
131:{
132:    if (dim == 1)
133:    {
134:
135:        return 0;
136:
137:    }
138:    if (dim == 0)
139:    {
140:
141:        SIZE_T flat_id =
142:            get_group_id(1) * 1 +
143:            0;
144:
145:        return (flat_id / 1);
146:
147:    }
148:
149:    return 0;
150:}
151:
152:WITHIN_KERNEL VSIZE_T virtual_num_groups(unsigned int dim)
153:{
154:    if (dim == 1)
155:    {
156:        return 1;
157:    }
158:    if (dim == 0)
159:    {
160:        return 32;
161:    }
162:
163:    return 1;
164:}
165:
166:WITHIN_KERNEL VSIZE_T virtual_global_id(unsigned int dim)
167:{
168:    return virtual_local_id(dim) + virtual_group_id(dim) * virtual_local_size(dim);
169:}
170:
171:WITHIN_KERNEL VSIZE_T virtual_global_size(unsigned int dim)
172:{
173:    if(dim == 1)
174:    {
175:        return 512;
176:    }
177:    if(dim == 0)
178:    {
179:        return 32;
180:    }
181:
182:    return 1;
183:}
184:
185:WITHIN_KERNEL VSIZE_T virtual_global_flat_id()
186:{
187:    return
188:        virtual_global_id(1) * 1 +
189:        virtual_global_id(0) * 512 +
190:        0;
191:}
192:
193:WITHIN_KERNEL VSIZE_T virtual_global_flat_size()
194:{
195:    return
196:        virtual_global_size(1) *
197:        virtual_global_size(0) *
198:        1;
199:}
200:
201:
202:WITHIN_KERNEL bool virtual_skip_local_threads()
203:{
204:
205:    return false;
206:}
207:
208:WITHIN_KERNEL bool virtual_skip_groups()
209:{
210:
211:    return false;
212:}
213:
214:WITHIN_KERNEL bool virtual_skip_global_threads()
215:{
216:
217:    return false;
218:}
219:
220:
221:
222:#ifndef CUDA
223:#define MARK_VIRTUAL_FUNCTIONS_AS_USED (void)(virtual_num_groups(0)); (void)(virtual_global_flat_id()); (void)(virtual_global_flat_size())
224:#else
225:#define MARK_VIRTUAL_FUNCTIONS_AS_USED
226:#endif
227:
228:#define VIRTUAL_SKIP_THREADS MARK_VIRTUAL_FUNCTIONS_AS_USED; if(virtual_skip_local_threads() || virtual_skip_groups() || virtual_skip_global_threads()) return
229:
230:
231:WITHIN_KERNEL int _module2_(
232:    int a1, int a2)
233:{
234:
235:    
236:        int temp0 = (
237:                a1 * a2
238:            );
239:    
240:
241:    
242:    return temp0;
243:}
244:
245:
246:
247:
248:    // leaf input macro for &quot;noises_a&quot;
249:    #define _module4_(_idx0, _idx1) (_leaf_noises_a[(_idx0) * (500) + (_idx1) * (1) + (0)])
250:    
251:
252:
253:
254:
255:    // input for a transformation for &quot;noises_a&quot;
256:    #define _module3_ _module4_(_idx0, _idx1)
257:    
258:
259:
260:
261:
262:    // leaf input macro for &quot;key&quot;
263:    #define _module5_(_idx0) (_leaf_key[(_idx0) * (1) + (0)])
264:    
265:
266:
267:
268:
269:    // input transformation node for &quot;_nested1_nested1_input&quot;
270:    
271:    INLINE WITHIN_KERNEL int _module1_func(
272:        GLOBAL_MEM int *_leaf_noises_a,
273:GLOBAL_MEM int *_leaf_key,
274:VSIZE_T _idx0,
275:VSIZE_T _idx1)
276:    {
277:        int _val;
278:
279:        
280:
281:            _val =(_module2_(_module3_, _module5_(_idx1)));
282:            
283:
284:
285:        return _val;
286:    }
287:    
288:    #define _module1_(_idx0, _idx1) _module1_func(        _leaf_noises_a, _leaf_key, _idx0, _idx1)
289:    
290:
291:
292:
293:
294:    
295:    INLINE WITHIN_KERNEL int _module0_func(
296:        GLOBAL_MEM int *_leaf_noises_a, GLOBAL_MEM int *_leaf_key, VSIZE_T _c_idx0, VSIZE_T _c_idx1)
297:    {
298:        
299:
300:    
301:        
302:        VSIZE_T _idx0 = _c_idx0 / 1;
303:    
304:        
305:        VSIZE_T _idx1 = _c_idx1 / 1;
306:    
307:
308:
309:        return
310:        _module1_(_idx0, _idx1);
311:    }
312:    
313:    #define _module0_(_c_idx0, _c_idx1) _module0_func(        _leaf_noises_a, _leaf_key, _c_idx0, _c_idx1)
314:    
315:
316:
317:
318:
319:    // leaf output macro for &quot;result_b&quot;
320:    #define _module9_(_idx0, _val) _leaf_result_b[(_idx0) * (1) + (0)] = (_val)
321:    
322:
323:
324:
325:
326:    // output for a transformation for &quot;result_b&quot;
327:    #define _module8_(_val) _module9_(_idx0, _val)
328:    
329:
330:
331:
332:
333:    // leaf input macro for &quot;noises_b&quot;
334:    #define _module11_(_idx0) (_leaf_noises_b[(_idx0) * (1) + (0)])
335:    
336:
337:
338:
339:
340:    // input for a transformation for &quot;noises_b&quot;
341:    #define _module10_ _module11_(_idx0)
342:    
343:
344:
345:
346:
347:    // leaf input macro for &quot;messages&quot;
348:    #define _module13_(_idx0) (_leaf_messages[(_idx0) * (1) + (0)])
349:    
350:
351:
352:
353:
354:    // input for a transformation for &quot;messages&quot;
355:    #define _module12_ _module13_(_idx0)
356:    
357:
358:
359:
360:
361:    // leaf output macro for &quot;result_cv&quot;
362:    #define _module15_(_idx0, _val) _leaf_result_cv[(_idx0) * (1) + (0)] = (_val)
363:    
364:
365:
366:
367:
368:    // output for a transformation for &quot;result_cv&quot;
369:    #define _module14_(_val) _module15_(_idx0, _val)
370:    
371:
372:
373:
374:
375:    // output transformation node for &quot;_nested1_output&quot;
376:    
377:    INLINE WITHIN_KERNEL void _module7_func(
378:        GLOBAL_MEM int *_leaf_result_b,
379:GLOBAL_MEM float *_leaf_result_cv,
380:GLOBAL_MEM int *_leaf_messages,
381:GLOBAL_MEM int *_leaf_noises_b,
382:VSIZE_T _idx0,
383:int _val)
384:    {
385:
386:        
387:
388:            _module8_(
389:                _module10_
390:                + _module12_
391:                + _val);
392:            _module14_(5.928983654524958e-10);
393:            
394:
395:
396:    }
397:    
398:    #define _module7_(_idx0, _val) _module7_func(        _leaf_result_b, _leaf_result_cv, _leaf_messages, _leaf_noises_b, _idx0, _val)
399:    
400:
401:
402:
403:
404:    
405:    INLINE WITHIN_KERNEL void _module6_func(
406:        GLOBAL_MEM int *_leaf_result_b, GLOBAL_MEM float *_leaf_result_cv, GLOBAL_MEM int *_leaf_messages, GLOBAL_MEM int *_leaf_noises_b, VSIZE_T _c_idx0, VSIZE_T _c_idx1, int _val)
407:    {
408:        
409:
410:    
411:        
412:        VSIZE_T _idx0 = _c_idx0 / 1;
413:    
414:    
415:
416:
417:        _module7_(_idx0, _val);
418:    }
419:    
420:    #define _module6_(_c_idx0, _c_idx1, _val) _module6_func(        _leaf_result_b, _leaf_result_cv, _leaf_messages, _leaf_noises_b, _c_idx0, _c_idx1, _val)
421:    
422:
423:
424:
425:
426:
427:
428:INLINE WITHIN_KERNEL int reduction_op(int input1, int input2)
429:{
430:    
431:return input1 + input2;
432:
433:}
434:
435:
436:KERNEL void kernel_reduce(GLOBAL_MEM int *_leaf_result_b, GLOBAL_MEM float *_leaf_result_cv, GLOBAL_MEM int *_leaf_messages, GLOBAL_MEM int *_leaf_noises_b, GLOBAL_MEM int *_leaf_noises_a, GLOBAL_MEM int *_leaf_key)
437:
438:{
439:    VIRTUAL_SKIP_THREADS;
440:
441:    LOCAL_MEM int local_mem_[512];
442:
443:    const VSIZE_T tid = virtual_local_id(1);
444:    const VSIZE_T bid = virtual_group_id(1);
445:    const VSIZE_T part_num = virtual_global_id(0);
446:
447:    const VSIZE_T index_in_part = 512 * bid + tid;
448:    const int empty = 0;
449:
450:    int v;
451:    
452:        if(tid &lt; 500)
453:        {
454:            const int t =
455:                _module0_(
456:                    part_num, index_in_part + 0);
457:            v = t;
458:        }
459:        else
460:        {
461:            v = empty;
462:        }
463:
464:
465:    local_mem_[tid] = v;
466:    LOCAL_BARRIER;
467:
468:    // We could use the volatile trick here and execute the last several iterations
469:    // (that fit in a single warp) without LOCAL_BARRIERs, but it gives only
470:    // a minor performance boost, and works only for some platforms (and only for simple types).
471:        if(tid &lt; 256)
472:        {
473:            int val1, val2;
474:            val1 = local_mem_[tid];
475:            val2 = local_mem_[tid + 256];
476:            const int val = reduction_op(val1, val2);
477:
478:            local_mem_[tid] = val;
479:        }
480:        LOCAL_BARRIER;
481:        if(tid &lt; 128)
482:        {
483:            int val1, val2;
484:            val1 = local_mem_[tid];
485:            val2 = local_mem_[tid + 128];
486:            const int val = reduction_op(val1, val2);
487:
488:            local_mem_[tid] = val;
489:        }
490:        LOCAL_BARRIER;
491:        if(tid &lt; 64)
492:        {
493:            int val1, val2;
494:            val1 = local_mem_[tid];
495:            val2 = local_mem_[tid + 64];
496:            const int val = reduction_op(val1, val2);
497:
498:            local_mem_[tid] = val;
499:        }
500:        LOCAL_BARRIER;
501:        if(tid &lt; 32)
502:        {
503:            int val1, val2;
504:            val1 = local_mem_[tid];
505:            val2 = local_mem_[tid + 32];
506:            const int val = reduction_op(val1, val2);
507:
508:            local_mem_[tid] = val;
509:        }
510:        LOCAL_BARRIER;
511:        if(tid &lt; 16)
512:        {
513:            int val1, val2;
514:            val1 = local_mem_[tid];
515:            val2 = local_mem_[tid + 16];
516:            const int val = reduction_op(val1, val2);
517:
518:            local_mem_[tid] = val;
519:        }
520:        LOCAL_BARRIER;
521:        if(tid &lt; 8)
522:        {
523:            int val1, val2;
524:            val1 = local_mem_[tid];
525:            val2 = local_mem_[tid + 8];
526:            const int val = reduction_op(val1, val2);
527:
528:            local_mem_[tid] = val;
529:        }
530:        LOCAL_BARRIER;
531:        if(tid &lt; 4)
532:        {
533:            int val1, val2;
534:            val1 = local_mem_[tid];
535:            val2 = local_mem_[tid + 4];
536:            const int val = reduction_op(val1, val2);
537:
538:            local_mem_[tid] = val;
539:        }
540:        LOCAL_BARRIER;
541:        if(tid &lt; 2)
542:        {
543:            int val1, val2;
544:            val1 = local_mem_[tid];
545:            val2 = local_mem_[tid + 2];
546:            const int val = reduction_op(val1, val2);
547:
548:            local_mem_[tid] = val;
549:        }
550:        LOCAL_BARRIER;
551:        if(tid &lt; 1)
552:        {
553:            int val1, val2;
554:            val1 = local_mem_[tid];
555:            val2 = local_mem_[tid + 1];
556:            const int val = reduction_op(val1, val2);
557:
558:            local_mem_[tid] = val;
559:        }
560:        LOCAL_BARRIER;
561:
562:    if (tid == 0)
563:    {
564:        v = local_mem_[0];
565:
566:        _module6_(part_num, bid, v);
567:    }
568:}
569:
570:<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_constant_mem_performance[bs_kernel-cuda:0:0-FFT-constant_mem]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f8848c6a0&gt;, transform_type = &#x27;FFT&#x27;<br/>single_kernel_bootstrap = True, heavy_performance_load = False, use_constant_memory = True<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(&#x27;use_constant_memory&#x27;, [False, True], ids=[&#x27;global_mem&#x27;, &#x27;constant_mem&#x27;])<br/>    def test_constant_mem_performance(<br/>            thread, transform_type, single_kernel_bootstrap, heavy_performance_load,<br/>            use_constant_memory):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        # We want to test the effect of using constant memory on the bootstrap calculation.<br/>        # A single-kernel bootstrap uses the `use_constant_memory_multi_iter` option,<br/>        # and a multi-kernel bootstrap uses the `use_constant_memory_single_iter` option.<br/>        kwds = dict(single_kernel_bootstrap=single_kernel_bootstrap)<br/>        if single_kernel_bootstrap:<br/>            kwds.update(dict(use_constant_memory_multi_iter=use_constant_memory))<br/>        else:<br/>            kwds.update(dict(use_constant_memory_single_iter=use_constant_memory))<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/><br/>test/test_gates.py:377: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f8848c6a0&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_ntt_base_method_performance[bs_kernel-cuda:0:0-ntt_base=cuda_asm]</td>
          <td class="col-duration">14.35</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f82d82eb8&gt;, single_kernel_bootstrap = True<br/>heavy_performance_load = False, ntt_base_method = &#x27;cuda_asm&#x27;<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(<br/>        &#x27;ntt_base_method&#x27;, [&#x27;cuda_asm&#x27;, &#x27;c&#x27;], ids=[&#x27;ntt_base=cuda_asm&#x27;, &#x27;ntt_base=c&#x27;])<br/>    def test_ntt_base_method_performance(<br/>            thread, single_kernel_bootstrap, heavy_performance_load, ntt_base_method):<br/>    <br/>        if thread.api.get_id() != cuda_id() and ntt_base_method == &#x27;cuda_asm&#x27;:<br/>            pytest.skip()<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>        secret_key, cloud_key = make_key_pair(thread, rng, transform_type=&#x27;NTT&#x27;)<br/>    <br/>        # TODO: instead of creating a whole key and then checking if the parameters are supported,<br/>        # we can just create a parameter object separately.<br/>        if (single_kernel_bootstrap<br/>                and not single_kernel_bootstrap_supported(secret_key.params, thread.device_params)):<br/>            pytest.skip()<br/>    <br/>        perf_params = PerformanceParameters(<br/>            secret_key.params,<br/>            single_kernel_bootstrap=single_kernel_bootstrap,<br/>            ntt_base_method=ntt_base_method).for_device(thread.device_params)<br/>    <br/>&gt;       results = check_performance(thread, (secret_key, cloud_key), perf_params, shape=size)<br/><br/>test/test_gates.py:445: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_gates.py:276: in check_performance<br/>    shape=shape1, performance_test=True, perf_params=perf_params)<br/>test/test_gates.py:65: in check_gate<br/>    nufhe_func(thread, cloud_key, answer, *ciphertexts, perf_params)<br/>nufhe/gates.py:121: in gate_nand<br/>    MU, temp_result, perf_params)<br/>nufhe/bootstrap.py:229: in bootstrap<br/>    no_keyswitch=no_keyswitch)<br/>nufhe/bootstrap.py:186: in blind_rotate_and_extract<br/>    BlindRotate_gpu(result, acc, bk, ks, bara, perf_params, no_keyswitch=no_keyswitch)<br/>nufhe/blind_rotate.py:278: in BlindRotate_gpu<br/>    ks.log2_base, ks.decomp_length, perf_params)<br/>nufhe/computation_cache.py:55: in get_computation<br/>    compiled_comp = comp.compile(thr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/blind_rotate.py:250: in _build_plan<br/>    plan.computation_call(blind_rotate, extracted_a, extracted_b, accum_a, gsw, bara)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:502: in computation_call<br/>    self._compiler_options, self._keep))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;nufhe.blind_rotate.BlindRotate object at 0x7f8241ec50&gt;<br/>plan_factory = &lt;function Computation._get_plan.&lt;locals&gt;.&lt;lambda&gt; at 0x7f88515d90&gt;<br/>device_params = &lt;reikna.cluda.cuda.DeviceParameters object at 0x7f82d82908&gt;<br/>lwe_a = KernelArgument(_temp1), lwe_b = KernelArgument(_temp2)<br/>accum_a = KernelArgument(accum_a), gsw = KernelArgument(gsw), bara = KernelArgument(bara)<br/><br/>    def _build_plan(self, plan_factory, device_params, lwe_a, lwe_b, accum_a, gsw, bara):<br/>    <br/>        params = self._params<br/>        tlwe_params = params.tlwe_params<br/>        decomp_length = params.decomp_length<br/>        mask_size = tlwe_params.mask_size<br/>    <br/>        perf_params = self._perf_params<br/>        transform_type = self._params.tlwe_params.transform_type<br/>        transform = get_transform(transform_type)<br/>    <br/>        transform_module = transform.transform_module(perf_params, multi_iter=True)<br/>    <br/>        batch_shape = accum_a.shape[:-2]<br/>    <br/>        min_local_size = decomp_length * (mask_size + 1) * transform_module.threads_per_transform<br/>        local_size = device_params.max_work_group_size<br/>        while local_size &gt;= min_local_size:<br/>    <br/>            plan = plan_factory()<br/>    <br/>            if transform_module.use_constant_memory:<br/>                cdata_forward = plan.constant_array(transform_module.cdata_fw)<br/>                cdata_inverse = plan.constant_array(transform_module.cdata_inv)<br/>            else:<br/>                cdata_forward = plan.persistent_array(transform_module.cdata_fw)<br/>                cdata_inverse = plan.persistent_array(transform_module.cdata_inv)<br/>    <br/>            try:<br/>                plan.kernel_call(<br/>                    TEMPLATE.get_def(&quot;blind_rotate&quot;),<br/>                    [lwe_a, lwe_b, accum_a, gsw, bara, cdata_forward, cdata_inverse],<br/>                    kernel_name=&quot;blind_rotate&quot;,<br/>                    global_size=(<br/>                        helpers.product(batch_shape),<br/>                        local_size),<br/>                    local_size=(1, local_size),<br/>                    render_kwds=dict(<br/>                        local_size=local_size,<br/>                        slices=(len(batch_shape), 1, 1),<br/>                        slices2=(len(batch_shape), 1),<br/>                        slices3=(len(batch_shape),),<br/>                        transform=transform_module,<br/>                        mask_size=mask_size,<br/>                        decomp_length=decomp_length,<br/>                        output_size=self._in_out_params.size,<br/>                        input_size=tlwe_params.extracted_lweparams.size,<br/>                        bs_log2_base=self._params.bs_log2_base,<br/>                        mul_prepared=transform.transformed_mul_prepared(perf_params),<br/>                        add=transform.transformed_add(perf_params),<br/>                        tr_ctype=transform.transformed_internal_ctype(),<br/>                        min_blocks=helpers.min_blocks,<br/>                        )<br/>                    )<br/>            except OutOfResourcesError:<br/>                local_size -= transform_module.threads_per_transform<br/>                continue<br/>    <br/>            return plan<br/>    <br/>&gt;       raise ValueError(&quot;Could not find suitable local size for the kernel&quot;)<br/><span class="error">E       ValueError: Could not find suitable local size for the kernel</span><br/><br/>nufhe/blind_rotate.py:187: ValueError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_ntt_base_method_performance[bs_kernel-cuda:0:0-ntt_base=c]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f82d82eb8&gt;, single_kernel_bootstrap = True<br/>heavy_performance_load = False, ntt_base_method = &#x27;c&#x27;<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(<br/>        &#x27;ntt_base_method&#x27;, [&#x27;cuda_asm&#x27;, &#x27;c&#x27;], ids=[&#x27;ntt_base=cuda_asm&#x27;, &#x27;ntt_base=c&#x27;])<br/>    def test_ntt_base_method_performance(<br/>            thread, single_kernel_bootstrap, heavy_performance_load, ntt_base_method):<br/>    <br/>        if thread.api.get_id() != cuda_id() and ntt_base_method == &#x27;cuda_asm&#x27;:<br/>            pytest.skip()<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, transform_type=&#x27;NTT&#x27;)<br/><br/>test/test_gates.py:432: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f82d82eb8&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_ntt_mul_method_performance[bs_kernel-cuda:0:0-ntt_mul=cuda_asm]</td>
          <td class="col-duration">13.65</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f88dee208&gt;, single_kernel_bootstrap = True<br/>heavy_performance_load = False, ntt_mul_method = &#x27;cuda_asm&#x27;<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(<br/>        &#x27;ntt_mul_method&#x27;,<br/>        [&#x27;cuda_asm&#x27;, &#x27;c_from_asm&#x27;, &#x27;c&#x27;],<br/>        ids=[&#x27;ntt_mul=cuda_asm&#x27;, &#x27;ntt_mul=c_from_asm&#x27;, &#x27;ntt_mul=c&#x27;])<br/>    def test_ntt_mul_method_performance(<br/>            thread, single_kernel_bootstrap, heavy_performance_load, ntt_mul_method):<br/>    <br/>        if thread.api.get_id() != cuda_id() and ntt_mul_method == &#x27;cuda_asm&#x27;:<br/>            pytest.skip()<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>        secret_key, cloud_key = make_key_pair(thread, rng, transform_type=&#x27;NTT&#x27;)<br/>    <br/>        # TODO: instead of creating a whole key and then checking if the parameters are supported,<br/>        # we can just create a parameter object separately.<br/>        if (single_kernel_bootstrap<br/>                and not single_kernel_bootstrap_supported(secret_key.params, thread.device_params)):<br/>            pytest.skip()<br/>    <br/>        perf_params = PerformanceParameters(<br/>            secret_key.params,<br/>            single_kernel_bootstrap=single_kernel_bootstrap,<br/>            ntt_mul_method=ntt_mul_method).for_device(thread.device_params)<br/>    <br/>&gt;       results = check_performance(thread, (secret_key, cloud_key), perf_params, shape=size)<br/><br/>test/test_gates.py:477: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_gates.py:276: in check_performance<br/>    shape=shape1, performance_test=True, perf_params=perf_params)<br/>test/test_gates.py:65: in check_gate<br/>    nufhe_func(thread, cloud_key, answer, *ciphertexts, perf_params)<br/>nufhe/gates.py:121: in gate_nand<br/>    MU, temp_result, perf_params)<br/>nufhe/bootstrap.py:229: in bootstrap<br/>    no_keyswitch=no_keyswitch)<br/>nufhe/bootstrap.py:186: in blind_rotate_and_extract<br/>    BlindRotate_gpu(result, acc, bk, ks, bara, perf_params, no_keyswitch=no_keyswitch)<br/>nufhe/blind_rotate.py:278: in BlindRotate_gpu<br/>    ks.log2_base, ks.decomp_length, perf_params)<br/>nufhe/computation_cache.py:55: in get_computation<br/>    compiled_comp = comp.compile(thr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/blind_rotate.py:250: in _build_plan<br/>    plan.computation_call(blind_rotate, extracted_a, extracted_b, accum_a, gsw, bara)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:502: in computation_call<br/>    self._compiler_options, self._keep))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;nufhe.blind_rotate.BlindRotate object at 0x7f881e7ba8&gt;<br/>plan_factory = &lt;function Computation._get_plan.&lt;locals&gt;.&lt;lambda&gt; at 0x7f88332400&gt;<br/>device_params = &lt;reikna.cluda.cuda.DeviceParameters object at 0x7f88deea20&gt;<br/>lwe_a = KernelArgument(_temp1), lwe_b = KernelArgument(_temp2)<br/>accum_a = KernelArgument(accum_a), gsw = KernelArgument(gsw), bara = KernelArgument(bara)<br/><br/>    def _build_plan(self, plan_factory, device_params, lwe_a, lwe_b, accum_a, gsw, bara):<br/>    <br/>        params = self._params<br/>        tlwe_params = params.tlwe_params<br/>        decomp_length = params.decomp_length<br/>        mask_size = tlwe_params.mask_size<br/>    <br/>        perf_params = self._perf_params<br/>        transform_type = self._params.tlwe_params.transform_type<br/>        transform = get_transform(transform_type)<br/>    <br/>        transform_module = transform.transform_module(perf_params, multi_iter=True)<br/>    <br/>        batch_shape = accum_a.shape[:-2]<br/>    <br/>        min_local_size = decomp_length * (mask_size + 1) * transform_module.threads_per_transform<br/>        local_size = device_params.max_work_group_size<br/>        while local_size &gt;= min_local_size:<br/>    <br/>            plan = plan_factory()<br/>    <br/>            if transform_module.use_constant_memory:<br/>                cdata_forward = plan.constant_array(transform_module.cdata_fw)<br/>                cdata_inverse = plan.constant_array(transform_module.cdata_inv)<br/>            else:<br/>                cdata_forward = plan.persistent_array(transform_module.cdata_fw)<br/>                cdata_inverse = plan.persistent_array(transform_module.cdata_inv)<br/>    <br/>            try:<br/>                plan.kernel_call(<br/>                    TEMPLATE.get_def(&quot;blind_rotate&quot;),<br/>                    [lwe_a, lwe_b, accum_a, gsw, bara, cdata_forward, cdata_inverse],<br/>                    kernel_name=&quot;blind_rotate&quot;,<br/>                    global_size=(<br/>                        helpers.product(batch_shape),<br/>                        local_size),<br/>                    local_size=(1, local_size),<br/>                    render_kwds=dict(<br/>                        local_size=local_size,<br/>                        slices=(len(batch_shape), 1, 1),<br/>                        slices2=(len(batch_shape), 1),<br/>                        slices3=(len(batch_shape),),<br/>                        transform=transform_module,<br/>                        mask_size=mask_size,<br/>                        decomp_length=decomp_length,<br/>                        output_size=self._in_out_params.size,<br/>                        input_size=tlwe_params.extracted_lweparams.size,<br/>                        bs_log2_base=self._params.bs_log2_base,<br/>                        mul_prepared=transform.transformed_mul_prepared(perf_params),<br/>                        add=transform.transformed_add(perf_params),<br/>                        tr_ctype=transform.transformed_internal_ctype(),<br/>                        min_blocks=helpers.min_blocks,<br/>                        )<br/>                    )<br/>            except OutOfResourcesError:<br/>                local_size -= transform_module.threads_per_transform<br/>                continue<br/>    <br/>            return plan<br/>    <br/>&gt;       raise ValueError(&quot;Could not find suitable local size for the kernel&quot;)<br/><span class="error">E       ValueError: Could not find suitable local size for the kernel</span><br/><br/>nufhe/blind_rotate.py:187: ValueError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_ntt_mul_method_performance[bs_kernel-cuda:0:0-ntt_mul=c_from_asm]</td>
          <td class="col-duration">5.78</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f88dee208&gt;, single_kernel_bootstrap = True<br/>heavy_performance_load = False, ntt_mul_method = &#x27;c_from_asm&#x27;<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(<br/>        &#x27;ntt_mul_method&#x27;,<br/>        [&#x27;cuda_asm&#x27;, &#x27;c_from_asm&#x27;, &#x27;c&#x27;],<br/>        ids=[&#x27;ntt_mul=cuda_asm&#x27;, &#x27;ntt_mul=c_from_asm&#x27;, &#x27;ntt_mul=c&#x27;])<br/>    def test_ntt_mul_method_performance(<br/>            thread, single_kernel_bootstrap, heavy_performance_load, ntt_mul_method):<br/>    <br/>        if thread.api.get_id() != cuda_id() and ntt_mul_method == &#x27;cuda_asm&#x27;:<br/>            pytest.skip()<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>        secret_key, cloud_key = make_key_pair(thread, rng, transform_type=&#x27;NTT&#x27;)<br/>    <br/>        # TODO: instead of creating a whole key and then checking if the parameters are supported,<br/>        # we can just create a parameter object separately.<br/>        if (single_kernel_bootstrap<br/>                and not single_kernel_bootstrap_supported(secret_key.params, thread.device_params)):<br/>            pytest.skip()<br/>    <br/>        perf_params = PerformanceParameters(<br/>            secret_key.params,<br/>            single_kernel_bootstrap=single_kernel_bootstrap,<br/>            ntt_mul_method=ntt_mul_method).for_device(thread.device_params)<br/>    <br/>&gt;       results = check_performance(thread, (secret_key, cloud_key), perf_params, shape=size)<br/><br/>test/test_gates.py:477: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_gates.py:276: in check_performance<br/>    shape=shape1, performance_test=True, perf_params=perf_params)<br/>test/test_gates.py:65: in check_gate<br/>    nufhe_func(thread, cloud_key, answer, *ciphertexts, perf_params)<br/>nufhe/gates.py:121: in gate_nand<br/>    MU, temp_result, perf_params)<br/>nufhe/bootstrap.py:229: in bootstrap<br/>    no_keyswitch=no_keyswitch)<br/>nufhe/bootstrap.py:186: in blind_rotate_and_extract<br/>    BlindRotate_gpu(result, acc, bk, ks, bara, perf_params, no_keyswitch=no_keyswitch)<br/>nufhe/blind_rotate.py:278: in BlindRotate_gpu<br/>    ks.log2_base, ks.decomp_length, perf_params)<br/>nufhe/computation_cache.py:55: in get_computation<br/>    compiled_comp = comp.compile(thr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/blind_rotate.py:250: in _build_plan<br/>    plan.computation_call(blind_rotate, extracted_a, extracted_b, accum_a, gsw, bara)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:502: in computation_call<br/>    self._compiler_options, self._keep))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;nufhe.blind_rotate.BlindRotate object at 0x7f881d6390&gt;<br/>plan_factory = &lt;function Computation._get_plan.&lt;locals&gt;.&lt;lambda&gt; at 0x7f8826f8c8&gt;<br/>device_params = &lt;reikna.cluda.cuda.DeviceParameters object at 0x7f88deea20&gt;<br/>lwe_a = KernelArgument(_temp1), lwe_b = KernelArgument(_temp2)<br/>accum_a = KernelArgument(accum_a), gsw = KernelArgument(gsw), bara = KernelArgument(bara)<br/><br/>    def _build_plan(self, plan_factory, device_params, lwe_a, lwe_b, accum_a, gsw, bara):<br/>    <br/>        params = self._params<br/>        tlwe_params = params.tlwe_params<br/>        decomp_length = params.decomp_length<br/>        mask_size = tlwe_params.mask_size<br/>    <br/>        perf_params = self._perf_params<br/>        transform_type = self._params.tlwe_params.transform_type<br/>        transform = get_transform(transform_type)<br/>    <br/>        transform_module = transform.transform_module(perf_params, multi_iter=True)<br/>    <br/>        batch_shape = accum_a.shape[:-2]<br/>    <br/>        min_local_size = decomp_length * (mask_size + 1) * transform_module.threads_per_transform<br/>        local_size = device_params.max_work_group_size<br/>        while local_size &gt;= min_local_size:<br/>    <br/>            plan = plan_factory()<br/>    <br/>            if transform_module.use_constant_memory:<br/>                cdata_forward = plan.constant_array(transform_module.cdata_fw)<br/>                cdata_inverse = plan.constant_array(transform_module.cdata_inv)<br/>            else:<br/>                cdata_forward = plan.persistent_array(transform_module.cdata_fw)<br/>                cdata_inverse = plan.persistent_array(transform_module.cdata_inv)<br/>    <br/>            try:<br/>                plan.kernel_call(<br/>                    TEMPLATE.get_def(&quot;blind_rotate&quot;),<br/>                    [lwe_a, lwe_b, accum_a, gsw, bara, cdata_forward, cdata_inverse],<br/>                    kernel_name=&quot;blind_rotate&quot;,<br/>                    global_size=(<br/>                        helpers.product(batch_shape),<br/>                        local_size),<br/>                    local_size=(1, local_size),<br/>                    render_kwds=dict(<br/>                        local_size=local_size,<br/>                        slices=(len(batch_shape), 1, 1),<br/>                        slices2=(len(batch_shape), 1),<br/>                        slices3=(len(batch_shape),),<br/>                        transform=transform_module,<br/>                        mask_size=mask_size,<br/>                        decomp_length=decomp_length,<br/>                        output_size=self._in_out_params.size,<br/>                        input_size=tlwe_params.extracted_lweparams.size,<br/>                        bs_log2_base=self._params.bs_log2_base,<br/>                        mul_prepared=transform.transformed_mul_prepared(perf_params),<br/>                        add=transform.transformed_add(perf_params),<br/>                        tr_ctype=transform.transformed_internal_ctype(),<br/>                        min_blocks=helpers.min_blocks,<br/>                        )<br/>                    )<br/>            except OutOfResourcesError:<br/>                local_size -= transform_module.threads_per_transform<br/>                continue<br/>    <br/>            return plan<br/>    <br/>&gt;       raise ValueError(&quot;Could not find suitable local size for the kernel&quot;)<br/><span class="error">E       ValueError: Could not find suitable local size for the kernel</span><br/><br/>nufhe/blind_rotate.py:187: ValueError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_ntt_mul_method_performance[bs_kernel-cuda:0:0-ntt_mul=c]</td>
          <td class="col-duration">5.25</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f88dee208&gt;, single_kernel_bootstrap = True<br/>heavy_performance_load = False, ntt_mul_method = &#x27;c&#x27;<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(<br/>        &#x27;ntt_mul_method&#x27;,<br/>        [&#x27;cuda_asm&#x27;, &#x27;c_from_asm&#x27;, &#x27;c&#x27;],<br/>        ids=[&#x27;ntt_mul=cuda_asm&#x27;, &#x27;ntt_mul=c_from_asm&#x27;, &#x27;ntt_mul=c&#x27;])<br/>    def test_ntt_mul_method_performance(<br/>            thread, single_kernel_bootstrap, heavy_performance_load, ntt_mul_method):<br/>    <br/>        if thread.api.get_id() != cuda_id() and ntt_mul_method == &#x27;cuda_asm&#x27;:<br/>            pytest.skip()<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>        secret_key, cloud_key = make_key_pair(thread, rng, transform_type=&#x27;NTT&#x27;)<br/>    <br/>        # TODO: instead of creating a whole key and then checking if the parameters are supported,<br/>        # we can just create a parameter object separately.<br/>        if (single_kernel_bootstrap<br/>                and not single_kernel_bootstrap_supported(secret_key.params, thread.device_params)):<br/>            pytest.skip()<br/>    <br/>        perf_params = PerformanceParameters(<br/>            secret_key.params,<br/>            single_kernel_bootstrap=single_kernel_bootstrap,<br/>            ntt_mul_method=ntt_mul_method).for_device(thread.device_params)<br/>    <br/>&gt;       results = check_performance(thread, (secret_key, cloud_key), perf_params, shape=size)<br/><br/>test/test_gates.py:477: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_gates.py:276: in check_performance<br/>    shape=shape1, performance_test=True, perf_params=perf_params)<br/>test/test_gates.py:65: in check_gate<br/>    nufhe_func(thread, cloud_key, answer, *ciphertexts, perf_params)<br/>nufhe/gates.py:121: in gate_nand<br/>    MU, temp_result, perf_params)<br/>nufhe/bootstrap.py:229: in bootstrap<br/>    no_keyswitch=no_keyswitch)<br/>nufhe/bootstrap.py:186: in blind_rotate_and_extract<br/>    BlindRotate_gpu(result, acc, bk, ks, bara, perf_params, no_keyswitch=no_keyswitch)<br/>nufhe/blind_rotate.py:278: in BlindRotate_gpu<br/>    ks.log2_base, ks.decomp_length, perf_params)<br/>nufhe/computation_cache.py:55: in get_computation<br/>    compiled_comp = comp.compile(thr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/blind_rotate.py:250: in _build_plan<br/>    plan.computation_call(blind_rotate, extracted_a, extracted_b, accum_a, gsw, bara)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:502: in computation_call<br/>    self._compiler_options, self._keep))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/blind_rotate.py:178: in _build_plan<br/>    min_blocks=helpers.min_blocks,<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:473: in kernel_call<br/>    keep=self._keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:566: in compile_static<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:786: in __init__<br/>    constant_arrays=constant_arrays, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:655: in __init__<br/>    self.source, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:504: in _create_program<br/>    src, fast_math=fast_math, compiler_options=compiler_options, keep=keep)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:227: in _compile<br/>    return SourceModule(src, no_extern_c=True, options=options, keep=keep)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;pycuda.compiler.SourceModule object at 0x7f881d6320&gt;<br/>source = &#x27;\n\n\n    #define CUDA\n    // taken from pycuda._cluda\n    #define LOCAL_BARRIER __syncthreads()\n\n    #define WIT... _module28_(\n                batch_id, i, i == 0 ? shared_accum[0] : -shared_accum[1024 - i]);\n        }\n    }\n}\n&#x27;<br/>nvcc = &#x27;nvcc&#x27;, options = [], keep = False, no_extern_c = True, arch = None, code = None<br/>cache_dir = None, include_dirs = []<br/><br/>    def __init__(self, source, nvcc=&quot;nvcc&quot;, options=None, keep=False,<br/>            no_extern_c=False, arch=None, code=None, cache_dir=None,<br/>            include_dirs=[]):<br/>        self._check_arch(arch)<br/>    <br/>        cubin = compile(source, nvcc, options, keep, no_extern_c,<br/>                arch, code, cache_dir, include_dirs)<br/>    <br/>        from pycuda.driver import module_from_buffer<br/>&gt;       self.module = module_from_buffer(cubin)<br/><span class="error">E       pycuda._driver.LogicError</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/pycuda-2020.1-py3.6-linux-aarch64.egg/pycuda/compiler.py:294: LogicError<br/> -------------------------------Captured log call-------------------------------- <br/>[1m[31mERROR   [0m root:api.py:507 Failed to compile:
1:
2:
3:
4:    #define CUDA
5:    // taken from pycuda._cluda
6:    #define LOCAL_BARRIER __syncthreads()
7:
8:    #define WITHIN_KERNEL __device__
9:    #define KERNEL extern &quot;C&quot; __global__
10:    #define GLOBAL_MEM /* empty */
11:    #define GLOBAL_MEM_ARG /* empty */
12:    #define LOCAL_MEM __shared__
13:    #define LOCAL_MEM_DYNAMIC extern __shared__
14:    #define LOCAL_MEM_ARG /* empty */
15:    #define CONSTANT_MEM __constant__
16:    #define CONSTANT_MEM_ARG /* empty */
17:    #define INLINE __forceinline__
18:    #define SIZE_T int
19:    #define VSIZE_T int
20:
21:    // used to align fields in structures
22:    #define ALIGN(bytes) __align__(bytes)
23:
24:    
25:
26:    WITHIN_KERNEL SIZE_T get_local_id(unsigned int dim)
27:    {
28:        if(dim == 0) return threadIdx.x;
29:        if(dim == 1) return threadIdx.y;
30:        if(dim == 2) return threadIdx.z;
31:        return 0;
32:    }
33:
34:    WITHIN_KERNEL SIZE_T get_group_id(unsigned int dim)
35:    {
36:        if(dim == 0) return blockIdx.x;
37:        if(dim == 1) return blockIdx.y;
38:        if(dim == 2) return blockIdx.z;
39:        return 0;
40:    }
41:
42:    WITHIN_KERNEL SIZE_T get_local_size(unsigned int dim)
43:    {
44:        if(dim == 0) return blockDim.x;
45:        if(dim == 1) return blockDim.y;
46:        if(dim == 2) return blockDim.z;
47:        return 1;
48:    }
49:
50:    WITHIN_KERNEL SIZE_T get_num_groups(unsigned int dim)
51:    {
52:        if(dim == 0) return gridDim.x;
53:        if(dim == 1) return gridDim.y;
54:        if(dim == 2) return gridDim.z;
55:        return 1;
56:    }
57:
58:    WITHIN_KERNEL SIZE_T get_global_size(unsigned int dim)
59:    {
60:        return get_num_groups(dim) * get_local_size(dim);
61:    }
62:
63:    WITHIN_KERNEL SIZE_T get_global_id(unsigned int dim)
64:    {
65:        return get_local_id(dim) + get_group_id(dim) * get_local_size(dim);
66:    }
67:
68:
69:
70:
71:    #define COMPLEX_CTR(T) make_##T
72:
73:    WITHIN_KERNEL float2 operator+(float2 a, float2 b)
74:    {
75:        return COMPLEX_CTR(float2)(a.x + b.x, a.y + b.y);
76:    }
77:    WITHIN_KERNEL float2 operator-(float2 a, float2 b)
78:    {
79:        return COMPLEX_CTR(float2)(a.x - b.x, a.y - b.y);
80:    }
81:    WITHIN_KERNEL float2 operator+(float2 a) { return a; }
82:    WITHIN_KERNEL float2 operator-(float2 a) { return COMPLEX_CTR(float2)(-a.x, -a.y); }
83:    WITHIN_KERNEL double2 operator+(double2 a, double2 b)
84:    {
85:        return COMPLEX_CTR(double2)(a.x + b.x, a.y + b.y);
86:    }
87:    WITHIN_KERNEL double2 operator-(double2 a, double2 b)
88:    {
89:        return COMPLEX_CTR(double2)(a.x - b.x, a.y - b.y);
90:    }
91:    WITHIN_KERNEL double2 operator+(double2 a) { return a; }
92:    WITHIN_KERNEL double2 operator-(double2 a) { return COMPLEX_CTR(double2)(-a.x, -a.y); }
93:
94:WITHIN_KERNEL VSIZE_T virtual_local_id(unsigned int dim)
95:{
96:    if (dim == 1)
97:    {
98:
99:        SIZE_T flat_id =
100:            get_local_id(0) * 1 +
101:            0;
102:
103:        return (flat_id / 1);
104:
105:    }
106:    if (dim == 0)
107:    {
108:
109:        return 0;
110:
111:    }
112:
113:    return 0;
114:}
115:
116:WITHIN_KERNEL VSIZE_T virtual_local_size(unsigned int dim)
117:{
118:    if (dim == 1)
119:    {
120:        return 640;
121:    }
122:    if (dim == 0)
123:    {
124:        return 1;
125:    }
126:
127:    return 1;
128:}
129:
130:WITHIN_KERNEL VSIZE_T virtual_group_id(unsigned int dim)
131:{
132:    if (dim == 1)
133:    {
134:
135:        return 0;
136:
137:    }
138:    if (dim == 0)
139:    {
140:
141:        SIZE_T flat_id =
142:            get_group_id(1) * 1 +
143:            0;
144:
145:        return (flat_id / 1);
146:
147:    }
148:
149:    return 0;
150:}
151:
152:WITHIN_KERNEL VSIZE_T virtual_num_groups(unsigned int dim)
153:{
154:    if (dim == 1)
155:    {
156:        return 1;
157:    }
158:    if (dim == 0)
159:    {
160:        return 64;
161:    }
162:
163:    return 1;
164:}
165:
166:WITHIN_KERNEL VSIZE_T virtual_global_id(unsigned int dim)
167:{
168:    return virtual_local_id(dim) + virtual_group_id(dim) * virtual_local_size(dim);
169:}
170:
171:WITHIN_KERNEL VSIZE_T virtual_global_size(unsigned int dim)
172:{
173:    if(dim == 1)
174:    {
175:        return 640;
176:    }
177:    if(dim == 0)
178:    {
179:        return 64;
180:    }
181:
182:    return 1;
183:}
184:
185:WITHIN_KERNEL VSIZE_T virtual_global_flat_id()
186:{
187:    return
188:        virtual_global_id(1) * 1 +
189:        virtual_global_id(0) * 640 +
190:        0;
191:}
192:
193:WITHIN_KERNEL VSIZE_T virtual_global_flat_size()
194:{
195:    return
196:        virtual_global_size(1) *
197:        virtual_global_size(0) *
198:        1;
199:}
200:
201:
202:WITHIN_KERNEL bool virtual_skip_local_threads()
203:{
204:
205:    return false;
206:}
207:
208:WITHIN_KERNEL bool virtual_skip_groups()
209:{
210:
211:    return false;
212:}
213:
214:WITHIN_KERNEL bool virtual_skip_global_threads()
215:{
216:
217:    return false;
218:}
219:
220:
221:
222:#ifndef CUDA
223:#define MARK_VIRTUAL_FUNCTIONS_AS_USED (void)(virtual_num_groups(0)); (void)(virtual_global_flat_id()); (void)(virtual_global_flat_size())
224:#else
225:#define MARK_VIRTUAL_FUNCTIONS_AS_USED
226:#endif
227:
228:#define VIRTUAL_SKIP_THREADS MARK_VIRTUAL_FUNCTIONS_AS_USED; if(virtual_skip_local_threads() || virtual_skip_groups() || virtual_skip_global_threads()) return
229:
230:typedef struct __module0_
231:{
232:    unsigned long val;
233:} _module0_;
234:
235:
236:WITHIN_KERNEL INLINE _module0_ _module0_pack(unsigned long x)
237:{
238:    _module0_ res = {x};
239:    return res;
240:}
241:
242:WITHIN_KERNEL INLINE unsigned long _module0_unpack(_module0_ x)
243:{
244:    return x.val;
245:}
246:
247:#define _module0_zero (_module0_pack(0));
248:
249:
250:#define _module0_UNPACK(hi, lo, src) {lo = (src); hi = (src) &gt;&gt; 32;}
251:
252:#ifdef CUDA
253:#define _module0_PACK(hi, lo) (((unsigned long)(hi) &lt;&lt; 32) | (lo))
254:#else
255:#define _module0_PACK(hi, lo) upsample(hi, lo)
256:#endif
257:
258:
259:#define _module0_SUB_CC(hi, lo, x1, x2) { bool cc = (x1) &lt; (x2); lo = (x1) - (x2); if (cc) { hi -= 1; } }
260:#define _module0_ADD_CC(hi, lo, x1, x2, hi_prev) { lo = (x1) + (x2); bool cc = lo &lt; (x2); hi = hi_prev; if (cc) { hi += 1; } }
261:
262:
263:
264:
265:
266:    // leaf input macro for &quot;accum_a&quot;
267:    #define _module2_(_idx0, _idx1, _idx2) (_leaf_accum_a[(_idx0) * (2048) + (_idx1) * (1024) + (_idx2) * (1) + (0)])
268:    
269:
270:
271:
272:
273:    
274:    INLINE WITHIN_KERNEL int _module1_func(
275:        GLOBAL_MEM int *_leaf_accum_a, VSIZE_T _c_idx0, VSIZE_T _c_idx1, VSIZE_T _c_idx2)
276:    {
277:        
278:
279:    
280:        
281:        VSIZE_T _idx0 = _c_idx0 / 1;
282:    
283:        
284:        VSIZE_T _idx1 = _c_idx1 / 1;
285:    
286:        
287:        VSIZE_T _idx2 = _c_idx2 / 1;
288:    
289:
290:
291:        return
292:        _module2_(_idx0, _idx1, _idx2);
293:    }
294:    
295:    #define _module1_(_c_idx0, _c_idx1, _c_idx2) _module1_func(        _leaf_accum_a, _c_idx0, _c_idx1, _c_idx2)
296:    
297:
298:
299:
300:
301:    // leaf input macro for &quot;bara&quot;
302:    #define _module4_(_idx0, _idx1) (_leaf_bara[(_idx0) * (500) + (_idx1) * (1) + (0)])
303:    
304:
305:
306:
307:
308:    
309:    INLINE WITHIN_KERNEL int _module3_func(
310:        GLOBAL_MEM int *_leaf_bara, VSIZE_T _c_idx0, VSIZE_T _c_idx1)
311:    {
312:        
313:
314:    
315:        
316:        VSIZE_T _idx0 = _c_idx0 / 1;
317:    
318:        
319:        VSIZE_T _idx1 = _c_idx1 / 1;
320:    
321:
322:
323:        return
324:        _module4_(_idx0, _idx1);
325:    }
326:    
327:    #define _module3_(_c_idx0, _c_idx1) _module3_func(        _leaf_bara, _c_idx0, _c_idx1)
328:    
329:
330:
331:
332:/** Subtraction in FF(P): val_ = a + b mod P. */
333:WITHIN_KERNEL INLINE _module0_ _module6_(_module0_ a, _module0_ b)
334:{
335:
336:    /*
337:    Algorithm:
338:    We calculate `s = x - y`
339:    Now there are three variants:
340:    - no underflow (x &gt;= y): all good, `result = s`.
341:    - underflow (detected if `s &gt; x`), so essentially `s = x - y + N`.
342:      This means we need to calculate `s - N + P = s - (2^32 - 1)`
343:    */
344:
345:    _module0_ res = {a.val - b.val};
346:    unsigned int x = -(res.val &gt; a.val);
347:    res.val -= x;
348:    return res;
349:
350:}
351:
352:
353:
354:// Addition in FF(P): val_ = a + b mod P.
355:WITHIN_KERNEL INLINE _module0_ _module7_(_module0_ a, _module0_ b)
356:{
357:
358:    /*
359:    Algorithm:
360:    We calculate `s = x + y`
361:    Now there are three variants:
362:    - `s &lt; P` and no integer overflow: all good, `result = s`.
363:    - `s &gt; P` and no integer overflow: `result = s - P = s + (2^32 - 1)`
364:    - integer overflow, so essentially `s = x + y - N`.
365:      This means that we need to calculate `result = s + N - P = s + (2^32 - 1)`.
366:    Note that the last two variants result in the same modifier being applied.
367:    */
368:    _module0_ res = {a.val + b.val};
369:    res.val += ((res.val &lt; b.val) || res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
370:    return res;
371:
372:}
373:
374:
375:
376:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
377:WITHIN_KERNEL INLINE _module0_ _module9_(unsigned long a)
378:{
379:
380:    // uses the fact that 2 * P &gt; max(UInt64)
381:    // and that a::UInt64 - P == a + 2^32 - 1
382:
383:    _module0_ res = {a};
384:    res.val += (res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
385:    return res;
386:
387:}
388:
389:
390:
391:
392:/**
393:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
394:* @param[in] l An integer in [0, 32)
395:*/
396:WITHIN_KERNEL INLINE _module0_ _module8_(_module0_ x, unsigned int l)
397:{
398:    /*
399:    Algorithm:
400:
401:    We can decompose the shift as
402:
403:        res = x * 2^l = x * M^k * 2^j,
404:
405:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
406:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
407:
408:    After the multiplication by 2^j, the result contains 3 32-bit parts:
409:
410:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
411:
412:    Thus
413:
414:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
415:
416:    Taking the modulus P = M^2 - M + 1, we get
417:
418:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
419:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
420:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
421:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
422:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
423:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
424:
425:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
426:
427:    The processing for the things inside the parentheses is simpler:
428:
429:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
430:    - (x + y) = PACK(s &lt; y, s), where s = x + y
431:      (that is, check for overflow and add 1 in the high half)
432:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
433:      (that is, check for overflow and add (M-1) in the low half)
434:    */
435:
436:
437:
438:
439:        asm(&quot;{\n\t&quot;
440:            &quot;.reg .u32      r0, r1;\n\t&quot;
441:            &quot;.reg .u32      t0, t1, t2;\n\t&quot;
442:            &quot;.reg .u32      n;\n\t&quot;
443:            &quot;.reg .u64      s;\n\t&quot;
444:            // t[2] = (uint32_t)(x &gt;&gt; (64-l));
445:            // t[1] = (uint32_t)(x &gt;&gt; (32-l));
446:            // t[0] = (uint32_t)(x &lt;&lt; l);
447:            &quot;mov.b64        {r0, r1}, %0;\n\t&quot;
448:            &quot;shl.b32        t0, r0, %1;\n\t&quot;
449:            &quot;sub.u32        n, 32, %1;\n\t&quot;
450:            &quot;shr.b64        s, %0, n;\n\t&quot;
451:            &quot;mov.b64        {t1, t2}, s;\n\t&quot;
452:            // mod P
453:            &quot;add.u32        r1, t1, t2;\n\t&quot;
454:            &quot;sub.cc.u32     r0, t0, t2;\n\t&quot;
455:            &quot;subc.u32       r1, r1, 0;\n\t&quot;
456:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
457:            // ret += (uint32_t)(-(ret &lt; ((uint64_t *)t)[0]));
458:            &quot;mov.b64        s, {t0, t1};\n\t&quot;
459:            &quot;set.lt.u32.u64 t2, %0, s;\n\t&quot;
460:            &quot;add.cc.u32     r0, r0, t2;\n\t&quot;
461:            &quot;addc.u32       r1, r1, 0;\n\t&quot;
462:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
463:            &quot;}&quot;
464:            : &quot;+l&quot;(x.val)
465:            : &quot;r&quot;(l));
466:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
467:        return _module9_(x.val);
468:
469:
470:
471:}
472:
473:
474:
475:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
476:WITHIN_KERNEL INLINE _module0_ _module11_(unsigned long a)
477:{
478:
479:    // uses the fact that 2 * P &gt; max(UInt64)
480:    // and that a::UInt64 - P == a + 2^32 - 1
481:
482:    _module0_ res = {a};
483:    res.val += (res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
484:    return res;
485:
486:}
487:
488:
489:
490:
491:/**
492:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
493:* @param[in] l An integer in [0, 32)
494:*/
495:WITHIN_KERNEL INLINE _module0_ _module10_(_module0_ x, unsigned int l)
496:{
497:    /*
498:    Algorithm:
499:
500:    We can decompose the shift as
501:
502:        res = x * 2^l = x * M^k * 2^j,
503:
504:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
505:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
506:
507:    After the multiplication by 2^j, the result contains 3 32-bit parts:
508:
509:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
510:
511:    Thus
512:
513:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
514:
515:    Taking the modulus P = M^2 - M + 1, we get
516:
517:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
518:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
519:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
520:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
521:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
522:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
523:
524:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
525:
526:    The processing for the things inside the parentheses is simpler:
527:
528:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
529:    - (x + y) = PACK(s &lt; y, s), where s = x + y
530:      (that is, check for overflow and add 1 in the high half)
531:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
532:      (that is, check for overflow and add (M-1) in the low half)
533:    */
534:
535:
536:
537:
538:        asm(&quot;{\n\t&quot;
539:            &quot;.reg .u32          r0, r1;\n\t&quot;
540:            &quot;.reg .u32          t0, t1, t2;\n\t&quot;
541:            &quot;.reg .u32          n;\n\t&quot;
542:            &quot;.reg .u64          s;\n\t&quot;
543:            &quot;.reg .pred         p, q;\n\t&quot;
544:            // t[2] = (uint32_t)(x &gt;&gt; (96-l));
545:            // t[1] = (uint32_t)(x &gt;&gt; (64-l));
546:            // t[0] = (uint32_t)(x &lt;&lt; (l-32));
547:            &quot;mov.b64            {r0, r1}, %0;\n\t&quot;
548:            &quot;sub.u32            n, %1, 32;\n\t&quot;
549:            &quot;shl.b32            t0, r0, n;\n\t&quot;
550:            &quot;sub.u32            n, 32, n;\n\t&quot;
551:            &quot;shr.b64            s, %0, n;\n\t&quot;
552:            &quot;mov.b64            {t1, t2}, s;\n\t&quot;
553:            // mod P
554:            &quot;add.u32            r1, t0, t1;\n\t&quot;
555:            &quot;sub.cc.u32         r0, 0, t1;\n\t&quot;
556:            &quot;subc.u32           r1, r1, 0;\n\t&quot;
557:            &quot;sub.cc.u32         r0, r0, t2;\n\t&quot;
558:            &quot;subc.u32           r1, r1, 0;\n\t&quot;
559:            &quot;mov.b64            %0, {r0, r1};\n\t&quot;
560:            // ret -= (uint32_t)(-(ret &gt; ((uint64_t)t[0] &lt;&lt; 32) &amp;&amp; t[1] == 0));
561:            &quot;setp.eq.u32        p|q, t1, 0;\n\t&quot;
562:            &quot;mov.b64            s, {0, t0};\n\t&quot;
563:            &quot;set.gt.and.u32.u64 t2, %0, s, p;\n\t&quot;
564:            &quot;sub.cc.u32         r0, r0, t2;\n\t&quot;
565:            &quot;subc.u32           r1, r1, 0;\n\t&quot;
566:            &quot;mov.b64            %0, {r0, r1};\n\t&quot;
567:            // ret += (uint32_t)(-(ret &lt; ((uint64_t)t[0] &lt;&lt; 32) &amp;&amp; t[1] != 0));
568:            &quot;set.lt.and.u32.u64 t2, %0, s, q;\n\t&quot;
569:            &quot;add.cc.u32         r0, r0, t2;\n\t&quot;
570:            &quot;addc.u32           r1, r1, 0;\n\t&quot;
571:            &quot;mov.b64            %0, {r0, r1};\n\t&quot;
572:            &quot;}&quot;
573:            : &quot;+l&quot;(x.val)
574:            : &quot;r&quot;(l));
575:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
576:        return _module11_(x.val);
577:
578:
579:
580:}
581:
582:
583:
584:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
585:WITHIN_KERNEL INLINE _module0_ _module13_(unsigned long a)
586:{
587:
588:    // uses the fact that 2 * P &gt; max(UInt64)
589:    // and that a::UInt64 - P == a + 2^32 - 1
590:
591:    _module0_ res = {a};
592:    res.val += (res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
593:    return res;
594:
595:}
596:
597:
598:
599:
600:/**
601:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
602:* @param[in] l An integer in [0, 32)
603:*/
604:WITHIN_KERNEL INLINE _module0_ _module12_(_module0_ x, unsigned int l)
605:{
606:    /*
607:    Algorithm:
608:
609:    We can decompose the shift as
610:
611:        res = x * 2^l = x * M^k * 2^j,
612:
613:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
614:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
615:
616:    After the multiplication by 2^j, the result contains 3 32-bit parts:
617:
618:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
619:
620:    Thus
621:
622:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
623:
624:    Taking the modulus P = M^2 - M + 1, we get
625:
626:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
627:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
628:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
629:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
630:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
631:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
632:
633:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
634:
635:    The processing for the things inside the parentheses is simpler:
636:
637:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
638:    - (x + y) = PACK(s &lt; y, s), where s = x + y
639:      (that is, check for overflow and add 1 in the high half)
640:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
641:      (that is, check for overflow and add (M-1) in the low half)
642:    */
643:
644:
645:
646:
647:        asm(&quot;{\n\t&quot;
648:            &quot;.reg .u32      r0, r1;\n\t&quot;
649:            &quot;.reg .u32      t0, t1, t2;\n\t&quot;
650:            &quot;.reg .u32      n;\n\t&quot;
651:            &quot;.reg .u64      s;\n\t&quot;
652:            // t[2] = (uint32_t)(x &gt;&gt; (128-l));
653:            // t[1] = (uint32_t)(x &gt;&gt; (96-l));
654:            // t[0] = (uint32_t)(x &lt;&lt; (l-64));
655:            &quot;mov.b64        {r0, r1}, %0;\n\t&quot;
656:            &quot;sub.u32        n, %1, 64;\n\t&quot;
657:            &quot;shl.b32        t0, r0, n;\n\t&quot;
658:            &quot;sub.u32        n, 32, n;\n\t&quot;
659:            &quot;shr.b64        s, %0, n;\n\t&quot;
660:            &quot;mov.b64        {t1, t2}, s;\n\t&quot;
661:            // mod P
662:            &quot;add.cc.u32     r0, t1, t0;\n\t&quot;
663:            &quot;addc.u32       r1, t2, 0;\n\t&quot;
664:            &quot;sub.u32        r1, r1, t0;\n\t&quot;
665:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
666:            // ret -= (uint32_t)(-(ret &gt; ((uint64_t *)t)[1]));
667:            &quot;mov.b64        s, {t1, t2};\n\t&quot;
668:            &quot;set.gt.u32.u64 t2, %0, s;\n\t&quot;
669:            &quot;sub.cc.u32     r0, r0, t2;\n\t&quot;
670:            &quot;subc.u32       r1, r1, 0;\n\t&quot;
671:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
672:            &quot;}&quot;
673:            : &quot;+l&quot;(x.val)
674:            : &quot;r&quot;(l));
675:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
676:        x = _module13_(x.val);
677:        x.val = 18446744069414584321UL - x.val;
678:        return x;
679:
680:
681:
682:}
683:
684:
685:
686:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
687:WITHIN_KERNEL INLINE _module0_ _module15_(unsigned long a)
688:{
689:
690:    // uses the fact that 2 * P &gt; max(UInt64)
691:    // and that a::UInt64 - P == a + 2^32 - 1
692:
693:    _module0_ res = {a};
694:    res.val += (res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
695:    return res;
696:
697:}
698:
699:
700:
701:
702:/**
703:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
704:* @param[in] l An integer in [0, 32)
705:*/
706:WITHIN_KERNEL INLINE _module0_ _module14_(_module0_ x, unsigned int l)
707:{
708:    /*
709:    Algorithm:
710:
711:    We can decompose the shift as
712:
713:        res = x * 2^l = x * M^k * 2^j,
714:
715:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
716:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
717:
718:    After the multiplication by 2^j, the result contains 3 32-bit parts:
719:
720:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
721:
722:    Thus
723:
724:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
725:
726:    Taking the modulus P = M^2 - M + 1, we get
727:
728:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
729:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
730:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
731:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
732:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
733:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
734:
735:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
736:
737:    The processing for the things inside the parentheses is simpler:
738:
739:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
740:    - (x + y) = PACK(s &lt; y, s), where s = x + y
741:      (that is, check for overflow and add 1 in the high half)
742:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
743:      (that is, check for overflow and add (M-1) in the low half)
744:    */
745:
746:
747:
748:
749:        asm(&quot;{\n\t&quot;
750:            &quot;.reg .u32      r0, r1;\n\t&quot;
751:            &quot;.reg .u32      t0, t1, t2;\n\t&quot;
752:            &quot;.reg .u32      n;\n\t&quot;
753:            &quot;.reg .u64      s;\n\t&quot;
754:            // t[2] = (uint32_t)(x &lt;&lt; (l-160));
755:            // t[1] = (uint32_t)(x &gt;&gt; (224-l));
756:            // t[0] = (uint32_t)(x &gt;&gt; (192-l));
757:            &quot;mov.b64        {r0, r1}, %0;\n\t&quot;
758:            &quot;sub.u32        n, %1, 160;\n\t&quot;
759:            &quot;shl.b32        t2, r0, n;\n\t&quot;
760:            &quot;sub.u32        n, 32, n;\n\t&quot;
761:            &quot;shr.b64        s, %0, n;\n\t&quot;
762:            &quot;mov.b64        {t0, t1}, s;\n\t&quot;
763:            // mod P
764:            &quot;add.cc.u32     r0, t0, t2;\n\t&quot;
765:            &quot;addc.u32       r1, t1, 0;\n\t&quot;
766:            &quot;sub.u32        r1, r1, t2;\n\t&quot;
767:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
768:            // ret += (uint32_t)(-(ret &gt; ((uint64_t *)t)[0]));
769:            &quot;mov.b64        s, {t0, t1};\n\t&quot;
770:            &quot;set.gt.u32.u64 t2, %0, s;\n\t&quot;
771:            &quot;sub.cc.u32     r0, r0, t2;\n\t&quot;
772:            &quot;subc.u32       r1, r1, 0;\n\t&quot;
773:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
774:            &quot;}&quot;
775:            : &quot;+l&quot;(x.val)
776:            : &quot;r&quot;(l));
777:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
778:        return _module15_(x.val);
779:
780:
781:
782:}
783:
784:
785:
786:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
787:WITHIN_KERNEL INLINE _module0_ _module17_(unsigned long a)
788:{
789:
790:    // uses the fact that 2 * P &gt; max(UInt64)
791:    // and that a::UInt64 - P == a + 2^32 - 1
792:
793:    _module0_ res = {a};
794:    res.val += (res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
795:    return res;
796:
797:}
798:
799:
800:
801:
802:/**
803:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
804:* @param[in] l An integer in [0, 32)
805:*/
806:WITHIN_KERNEL INLINE _module0_ _module16_(_module0_ x, unsigned int l)
807:{
808:    /*
809:    Algorithm:
810:
811:    We can decompose the shift as
812:
813:        res = x * 2^l = x * M^k * 2^j,
814:
815:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
816:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
817:
818:    After the multiplication by 2^j, the result contains 3 32-bit parts:
819:
820:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
821:
822:    Thus
823:
824:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
825:
826:    Taking the modulus P = M^2 - M + 1, we get
827:
828:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
829:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
830:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
831:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
832:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
833:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
834:
835:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
836:
837:    The processing for the things inside the parentheses is simpler:
838:
839:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
840:    - (x + y) = PACK(s &lt; y, s), where s = x + y
841:      (that is, check for overflow and add 1 in the high half)
842:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
843:      (that is, check for overflow and add (M-1) in the low half)
844:    */
845:
846:
847:
848:
849:        asm(&quot;{\n\t&quot;
850:            &quot;.reg .u32          r0, r1;\n\t&quot;
851:            &quot;.reg .u32          t0, t1, t2;\n\t&quot;
852:            &quot;.reg .u32          n;\n\t&quot;
853:            &quot;.reg .u64          s;\n\t&quot;
854:            &quot;.reg .pred         p, q;\n\t&quot;
855:            // t[2] = (uint32_t)(x &gt;&gt; (192-l));
856:            // t[1] = (uint32_t)(x &gt;&gt; (160-l));
857:            // t[0] = (uint32_t)(x &lt;&lt; (l-128));
858:            &quot;mov.b64            {r0, r1}, %0;\n\t&quot;
859:            &quot;sub.u32            n, %1, 128;\n\t&quot;
860:            &quot;shl.b32            t0, r0, n;\n\t&quot;
861:            &quot;sub.u32            n, 32, n;\n\t&quot;
862:            &quot;shr.b64            s, %0, n;\n\t&quot;
863:            &quot;mov.b64            {t1, t2}, s;\n\t&quot;
864:            // mod P
865:            &quot;add.u32            r1, t0, t1;\n\t&quot;
866:            &quot;sub.cc.u32         r0, 0, t1;\n\t&quot;
867:            &quot;subc.u32           r1, r1, 0;\n\t&quot;
868:            &quot;sub.cc.u32         r0, r0, t2;\n\t&quot;
869:            &quot;subc.u32           r1, r1, 0;\n\t&quot;
870:            &quot;mov.b64            %0, {r0, r1};\n\t&quot;
871:            // ret -= (uint32_t)(-(ret &gt; ((uint64_t)t[0] &lt;&lt; 32) &amp;&amp; t[1] == 0));
872:            &quot;setp.eq.u32        p|q, t1, 0;\n\t&quot;
873:            &quot;mov.b64            s, {0, t0};\n\t&quot;
874:            &quot;set.gt.and.u32.u64 t2, %0, s, p;\n\t&quot;
875:            &quot;sub.cc.u32         r0, r0, t2;\n\t&quot;
876:            &quot;subc.u32           r1, r1, 0;\n\t&quot;
877:            &quot;mov.b64            %0, {r0, r1};\n\t&quot;
878:            // ret += (uint32_t)(-(ret &lt; ((uint64_t)t[0] &lt;&lt; 32) &amp;&amp; t[1] != 0));
879:            &quot;set.lt.and.u32.u64 t2, %0, s, q;\n\t&quot;
880:            &quot;add.cc.u32         r0, r0, t2;\n\t&quot;
881:            &quot;addc.u32           r1, r1, 0;\n\t&quot;
882:            &quot;mov.b64            %0, {r0, r1};\n\t&quot;
883:            &quot;}&quot;
884:            : &quot;+l&quot;(x.val)
885:            : &quot;r&quot;(l));
886:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
887:        x = _module17_(x.val);
888:        x.val = 18446744069414584321UL - x.val;
889:        return x;
890:
891:
892:
893:}
894:
895:
896:
897:// TODO: technically, this operation is inplace, so `a` can be passed by pointer
898:WITHIN_KERNEL INLINE _module0_ _module19_(unsigned long a)
899:{
900:
901:    // uses the fact that 2 * P &gt; max(UInt64)
902:    // and that a::UInt64 - P == a + 2^32 - 1
903:
904:    _module0_ res = {a};
905:    res.val += (res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
906:    return res;
907:
908:}
909:
910:
911:
912:
913:/**
914:* Binary left shifting in FF(P): val_ = val_ * 2^l mod P.
915:* @param[in] l An integer in [0, 32)
916:*/
917:WITHIN_KERNEL INLINE _module0_ _module18_(_module0_ x, unsigned int l)
918:{
919:    /*
920:    Algorithm:
921:
922:    We can decompose the shift as
923:
924:        res = x * 2^l = x * M^k * 2^j,
925:
926:    where M=2^32, k is integer, and 0 &lt;= j &lt; 32.
927:    k=0 corresponds to 0 &lt;= l &lt; 32, k=1 to 32 &lt;= l &lt; 64 and so on.
928:
929:    After the multiplication by 2^j, the result contains 3 32-bit parts:
930:
931:        x * 2^j = t2 * M^2 + t1 * M + t0, where 0 &lt;= t0, t1, t2 &lt; M
932:
933:    Thus
934:
935:        res = t2 * M^(2+k) + t1 * M^(1+k) + t0 * M^k
936:
937:    Taking the modulus P = M^2 - M + 1, we get
938:
939:    k = 0, l in [0, 32)   : (t1 + t2) * M + t0 - t2  = (t1 * M + t0) + (t2 * M) - (t2)
940:    k = 1, l in [32, 64)  : (t0 + t1) * M - t1 - t2  = ((t0 + t1) * M) - (t1 + t2)
941:    k = 2, l in [64, 96)  : (t0 - t2) * M - t0 - t1  = (t0 * M) - (t2 * M + t0) - (t1)
942:    k = 3, l in [96, 128) : (-t1 - t2) * M - t0 + t2 = (t2) - (t1 * M + t0) - (t2 * M)
943:    k = 4, l in [128, 160): (-t0 - t1) * M + t1 + t2 = (t1 + t2) - ((t0 + t1) * M)
944:    k = 5, l in [160, 192): (-t0 + t2) * M + t0 + t1 = (t2 * M + t0) - (t0 * M) + (t1)
945:
946:    Here the arithmetic operations outside the parentheses have to be performed modulo P.
947:
948:    The processing for the things inside the parentheses is simpler:
949:
950:    - (x * M + y) = PACK(x, y) -- packing two 32-bit integers into a 64-bit one;
951:    - (x + y) = PACK(s &lt; y, s), where s = x + y
952:      (that is, check for overflow and add 1 in the high half)
953:    - ((x + y) * M) = PACK(s, s &lt; y ? 0xffffffff : 0)
954:      (that is, check for overflow and add (M-1) in the low half)
955:    */
956:
957:
958:
959:
960:        asm(&quot;{\n\t&quot;
961:            &quot;.reg .u32      r0, r1;\n\t&quot;
962:            &quot;.reg .u32      t0, t1, t2;\n\t&quot;
963:            &quot;.reg .u32      n;\n\t&quot;
964:            &quot;.reg .u64      s;\n\t&quot;
965:            // t[2] = (uint32_t)(x &gt;&gt; (160-l));
966:            // t[1] = (uint32_t)(x &gt;&gt; (128-l));
967:            // t[0] = (uint32_t)(x &lt;&lt; (l-96));
968:            &quot;mov.b64        {r0, r1}, %0;\n\t&quot;
969:            &quot;sub.u32        n, %1, 96;\n\t&quot;
970:            &quot;shl.b32        t0, r0, n;\n\t&quot;
971:            &quot;sub.u32        n, 32, n;\n\t&quot;
972:            &quot;shr.b64        s, %0, n;\n\t&quot;
973:            &quot;mov.b64        {t1, t2}, s;\n\t&quot;
974:            // mod P
975:            &quot;add.u32        r1, t1, t2;\n\t&quot;
976:            &quot;sub.cc.u32     r0, t0, t2;\n\t&quot;
977:            &quot;subc.u32       r1, r1, 0;\n\t&quot;
978:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
979:            // ret += (uint32_t)(-(ret &lt; ((uint64_t *)t)[0]));
980:            &quot;mov.b64        s, {t0, t1};\n\t&quot;
981:            &quot;set.lt.u32.u64 t2, %0, s;\n\t&quot;
982:            &quot;add.cc.u32     r0, r0, t2;\n\t&quot;
983:            &quot;addc.u32       r1, r1, 0;\n\t&quot;
984:            &quot;mov.b64        %0, {r0, r1};\n\t&quot;
985:            &quot;}&quot;
986:            : &quot;+l&quot;(x.val)
987:            : &quot;r&quot;(l));
988:        // ret += (uint32_t)(-(ret &gt;= FFP_MODULUS));
989:        x = _module19_(x.val);
990:        x.val = 18446744069414584321UL - x.val;
991:        return x;
992:
993:
994:
995:}
996:
997:
998:
999:/** Subtraction in FF(P): val_ = a + b mod P. */
1000:WITHIN_KERNEL INLINE _module0_ _module21_(_module0_ a, _module0_ b)
1001:{
1002:
1003:    /*
1004:    Algorithm:
1005:    We calculate `s = x - y`
1006:    Now there are three variants:
1007:    - no underflow (x &gt;= y): all good, `result = s`.
1008:    - underflow (detected if `s &gt; x`), so essentially `s = x - y + N`.
1009:      This means we need to calculate `s - N + P = s - (2^32 - 1)`
1010:    */
1011:
1012:    _module0_ res = {a.val - b.val};
1013:    unsigned int x = -(res.val &gt; a.val);
1014:    res.val -= x;
1015:    return res;
1016:
1017:}
1018:
1019:
1020:
1021:WITHIN_KERNEL INLINE _module0_ _module20_(_module0_ a, _module0_ b)
1022:{
1023:    /*
1024:    This function performs Montgomery multiplication with the fixed modulus (M=2**64-2**32+1)
1025:    and fixed word size (R=2**64), which helps simplify the algorithm.
1026:    The result is `a * b * R**(-1) mod M`.
1027:
1028:    Note that if you multiply two numbers `a` and `b` in Montgomery representation
1029:    (a&#x27; = a * R mod M, b&#x27; = b * R mod M), the result is the Montgomery representation of their
1030:    product (a&#x27; * b&#x27; * R**(-1) mod M = a * b * R mod M).
1031:    But if one of the numbers is in Montgomery representation and the other is not, you
1032:    get the normal product back (a * b&#x27; * R**(-1) mod M = a * b mod M).
1033:
1034:    This way if one of the factors is precalculated, you can convert it
1035:    into Montgomery representation, and use this function instead of the general
1036:    multiplication function (which is slower).
1037:    */
1038:
1039:
1040:    #ifdef CUDA
1041:    unsigned long hi = __umul64hi(a.val, b.val);
1042:    #else
1043:    unsigned long hi = mul_hi(a.val, b.val);
1044:    #endif
1045:    unsigned long lo = a.val * b.val;
1046:
1047:    unsigned long u = (lo &lt;&lt; 32) + lo;
1048:    unsigned long p2 = u - (u &gt;&gt; 32);
1049:    unsigned long uu = u &lt;&lt; 32;
1050:    if (uu &gt; u)
1051:    {
1052:        p2 -= 1;
1053:    }
1054:
1055:
1056:    _module0_ hi_ff = { hi };
1057:    _module0_ p2_ff = { p2 };
1058:
1059:    return _module21_(hi_ff, p2_ff);
1060:}
1061:
1062:
1063:
1064:
1065:
1066:#define _module5_CDATA_QUALIFIER GLOBAL_MEM_ARG
1067:
1068:
1069:WITHIN_KERNEL INLINE void _module5_swap(_module0_ *a, _module0_ *b)
1070:{
1071:    _module0_ t = *b;
1072:    *b = *a;
1073:    *a = t;
1074:}
1075:
1076:
1077:WITHIN_KERNEL INLINE void _module5_NTT2(_module0_* r0, _module0_* r1)
1078:{
1079:    _module0_ t = _module6_(*r0, *r1);
1080:    *r0 = _module7_(*r0, *r1);
1081:    *r1 = t;
1082:}
1083:
1084:
1085:WITHIN_KERNEL INLINE void _module5_NTT2_pair(_module0_* r)
1086:{
1087:    _module5_NTT2(&amp;r[0], &amp;r[1]);
1088:}
1089:
1090:
1091:WITHIN_KERNEL INLINE void _module5_NTTInv2(_module0_* r0, _module0_* r1)
1092:{
1093:    _module5_NTT2(r0, r1);
1094:}
1095:
1096:
1097:WITHIN_KERNEL INLINE void _module5_NTTInv2_pair(_module0_* r)
1098:{
1099:    _module5_NTT2(&amp;r[0], &amp;r[1]);
1100:}
1101:
1102:
1103:WITHIN_KERNEL INLINE void _module5_NTT8(_module0_* r)
1104:{
1105:    _module5_NTT2(&amp;r[0], &amp;r[4]);
1106:    _module5_NTT2(&amp;r[1], &amp;r[5]);
1107:    _module5_NTT2(&amp;r[2], &amp;r[6]);
1108:    _module5_NTT2(&amp;r[3], &amp;r[7]);
1109:    r[5] = _module8_(r[5], 24);
1110:    r[6] = _module10_(r[6], 48);
1111:    r[7] = _module12_(r[7], 72);
1112:    // instead of calling NTT4 ...
1113:    _module5_NTT2(&amp;r[0], &amp;r[2]);
1114:    _module5_NTT2(&amp;r[1], &amp;r[3]);
1115:    r[3] = _module10_(r[3], 48);
1116:    _module5_NTT2(&amp;r[4], &amp;r[6]);
1117:    _module5_NTT2(&amp;r[5], &amp;r[7]);
1118:    r[7] = _module10_(r[7], 48);
1119:    _module5_NTT2_pair(&amp;r[0]);
1120:    _module5_NTT2_pair(&amp;r[2]);
1121:    _module5_NTT2_pair(&amp;r[4]);
1122:    _module5_NTT2_pair(&amp;r[6]);
1123:    // ... we save 2 swaps (otherwise 4) here
1124:    _module5_swap(&amp;r[1], &amp;r[4]);
1125:    _module5_swap(&amp;r[3], &amp;r[6]);
1126:}
1127:
1128:
1129:WITHIN_KERNEL INLINE void _module5_NTTInv8(_module0_* r)
1130:{
1131:    _module5_NTTInv2(&amp;r[0], &amp;r[4]);
1132:    _module5_NTTInv2(&amp;r[1], &amp;r[5]);
1133:    _module5_NTTInv2(&amp;r[2], &amp;r[6]);
1134:    _module5_NTTInv2(&amp;r[3], &amp;r[7]);
1135:    r[5] = _module14_(r[5], 168);
1136:    r[6] = _module16_(r[6], 144);
1137:    r[7] = _module18_(r[7], 120);
1138:    // instead of calling NTT4 ...
1139:    _module5_NTTInv2(&amp;r[0], &amp;r[2]);
1140:    _module5_NTTInv2(&amp;r[1], &amp;r[3]);
1141:    r[3] = _module16_(r[3], 144);
1142:    _module5_NTTInv2(&amp;r[4], &amp;r[6]);
1143:    _module5_NTTInv2(&amp;r[5], &amp;r[7]);
1144:    r[7] = _module16_(r[7], 144);
1145:    _module5_NTTInv2_pair(&amp;r[0]);
1146:    _module5_NTTInv2_pair(&amp;r[2]);
1147:    _module5_NTTInv2_pair(&amp;r[4]);
1148:    _module5_NTTInv2_pair(&amp;r[6]);
1149:    // ... we save 2 swaps (otherwise 4) here
1150:    _module5_swap(&amp;r[1], &amp;r[4]);
1151:    _module5_swap(&amp;r[3], &amp;r[6]);
1152:}
1153:
1154:
1155:WITHIN_KERNEL INLINE void _module5_NTT8x2Lsh_1(_module0_* s)
1156:{
1157:    s[1] = _module8_(s[1], 12);
1158:    s[2] = _module8_(s[2], 24);
1159:    s[3] = _module10_(s[3], 36);
1160:    s[4] = _module10_(s[4], 48);
1161:    s[5] = _module10_(s[5], 60);
1162:    s[6] = _module12_(s[6], 72);
1163:    s[7] = _module12_(s[7], 84);
1164:}
1165:
1166:
1167:WITHIN_KERNEL INLINE void _module5_NTT8x2Lsh(_module0_* s, unsigned int col)
1168:{
1169:    if (1 == col)
1170:        _module5_NTT8x2Lsh_1(s);
1171:}
1172:
1173:
1174:WITHIN_KERNEL INLINE void _module5_NTTInv8x2Lsh_1(_module0_* s)
1175:{
1176:    s[1] = _module14_(s[1], 180);
1177:    s[2] = _module14_(s[2], 168);
1178:    s[3] = _module16_(s[3], 156);
1179:    s[4] = _module16_(s[4], 144);
1180:    s[5] = _module16_(s[5], 132);
1181:    s[6] = _module18_(s[6], 120);
1182:    s[7] = _module18_(s[7], 108);
1183:}
1184:
1185:
1186:WITHIN_KERNEL INLINE void _module5_NTTInv8x2Lsh(_module0_* s, unsigned int col)
1187:{
1188:    if (1 == col)
1189:        _module5_NTTInv8x2Lsh_1(s);
1190:}
1191:
1192:
1193:WITHIN_KERNEL INLINE void _module5_NTT8x8Lsh_1(_module0_* s)
1194:{
1195:    s[1] = _module8_(s[1], 3);
1196:    s[2] = _module8_(s[2], 6);
1197:    s[3] = _module8_(s[3], 9);
1198:    s[4] = _module8_(s[4], 12);
1199:    s[5] = _module8_(s[5], 15);
1200:    s[6] = _module8_(s[6], 18);
1201:    s[7] = _module8_(s[7], 21);
1202:}
1203:WITHIN_KERNEL INLINE void _module5_NTT8x8Lsh_2(_module0_* s)
1204:{
1205:    s[1] = _module8_(s[1], 6);
1206:    s[2] = _module8_(s[2], 12);
1207:    s[3] = _module8_(s[3], 18);
1208:    s[4] = _module8_(s[4], 24);
1209:    s[5] = _module8_(s[5], 30);
1210:    s[6] = _module10_(s[6], 36);
1211:    s[7] = _module10_(s[7], 42);
1212:}
1213:WITHIN_KERNEL INLINE void _module5_NTT8x8Lsh_3(_module0_* s)
1214:{
1215:    s[1] = _module8_(s[1], 9);
1216:    s[2] = _module8_(s[2], 18);
1217:    s[3] = _module8_(s[3], 27);
1218:    s[4] = _module10_(s[4], 36);
1219:    s[5] = _module10_(s[5], 45);
1220:    s[6] = _module10_(s[6], 54);
1221:    s[7] = _module10_(s[7], 63);
1222:}
1223:WITHIN_KERNEL INLINE void _module5_NTT8x8Lsh_4(_module0_* s)
1224:{
1225:    s[1] = _module8_(s[1], 12);
1226:    s[2] = _module8_(s[2], 24);
1227:    s[3] = _module10_(s[3], 36);
1228:    s[4] = _module10_(s[4], 48);
1229:    s[5] = _module10_(s[5], 60);
1230:    s[6] = _module12_(s[6], 72);
1231:    s[7] = _module12_(s[7], 84);
1232:}
1233:WITHIN_KERNEL INLINE void _module5_NTT8x8Lsh_5(_module0_* s)
1234:{
1235:    s[1] = _module8_(s[1], 15);
1236:    s[2] = _module8_(s[2], 30);
1237:    s[3] = _module10_(s[3], 45);
1238:    s[4] = _module10_(s[4], 60);
1239:    s[5] = _module12_(s[5], 75);
1240:    s[6] = _module12_(s[6], 90);
1241:    s[7] = _module18_(s[7], 105);
1242:}
1243:WITHIN_KERNEL INLINE void _module5_NTT8x8Lsh_6(_module0_* s)
1244:{
1245:    s[1] = _module8_(s[1], 18);
1246:    s[2] = _module10_(s[2], 36);
1247:    s[3] = _module10_(s[3], 54);
1248:    s[4] = _module12_(s[4], 72);
1249:    s[5] = _module12_(s[5], 90);
1250:    s[6] = _module18_(s[6], 108);
1251:    s[7] = _module18_(s[7], 126);
1252:}
1253:WITHIN_KERNEL INLINE void _module5_NTT8x8Lsh_7(_module0_* s)
1254:{
1255:    s[1] = _module8_(s[1], 21);
1256:    s[2] = _module10_(s[2], 42);
1257:    s[3] = _module10_(s[3], 63);
1258:    s[4] = _module12_(s[4], 84);
1259:    s[5] = _module18_(s[5], 105);
1260:    s[6] = _module18_(s[6], 126);
1261:    s[7] = _module16_(s[7], 147);
1262:}
1263:
1264:
1265:WITHIN_KERNEL INLINE void _module5_NTT8x8Lsh(_module0_* s, unsigned int col)
1266:{
1267:    if (1 == col)
1268:        _module5_NTT8x8Lsh_1(s);
1269:    else if (2 == col)
1270:        _module5_NTT8x8Lsh_2(s);
1271:    else if (3 == col)
1272:        _module5_NTT8x8Lsh_3(s);
1273:    else if (4 == col)
1274:        _module5_NTT8x8Lsh_4(s);
1275:    else if (5 == col)
1276:        _module5_NTT8x8Lsh_5(s);
1277:    else if (6 == col)
1278:        _module5_NTT8x8Lsh_6(s);
1279:    else if (7 == col)
1280:        _module5_NTT8x8Lsh_7(s);
1281:}
1282:
1283:
1284:WITHIN_KERNEL INLINE void _module5_NTTInv8x8Lsh_1(_module0_* s)
1285:{
1286:    s[1] = _module14_(s[1], 189);
1287:    s[2] = _module14_(s[2], 186);
1288:    s[3] = _module14_(s[3], 183);
1289:    s[4] = _module14_(s[4], 180);
1290:    s[5] = _module14_(s[5], 177);
1291:    s[6] = _module14_(s[6], 174);
1292:    s[7] = _module14_(s[7], 171);
1293:}
1294:WITHIN_KERNEL INLINE void _module5_NTTInv8x8Lsh_2(_module0_* s)
1295:{
1296:    s[1] = _module14_(s[1], 186);
1297:    s[2] = _module14_(s[2], 180);
1298:    s[3] = _module14_(s[3], 174);
1299:    s[4] = _module14_(s[4], 168);
1300:    s[5] = _module14_(s[5], 162);
1301:    s[6] = _module16_(s[6], 156);
1302:    s[7] = _module16_(s[7], 150);
1303:}
1304:WITHIN_KERNEL INLINE void _module5_NTTInv8x8Lsh_3(_module0_* s)
1305:{
1306:    s[1] = _module14_(s[1], 183);
1307:    s[2] = _module14_(s[2], 174);
1308:    s[3] = _module14_(s[3], 165);
1309:    s[4] = _module16_(s[4], 156);
1310:    s[5] = _module16_(s[5], 147);
1311:    s[6] = _module16_(s[6], 138);
1312:    s[7] = _module16_(s[7], 129);
1313:}
1314:WITHIN_KERNEL INLINE void _module5_NTTInv8x8Lsh_4(_module0_* s)
1315:{
1316:    s[1] = _module14_(s[1], 180);
1317:    s[2] = _module14_(s[2], 168);
1318:    s[3] = _module16_(s[3], 156);
1319:    s[4] = _module16_(s[4], 144);
1320:    s[5] = _module16_(s[5], 132);
1321:    s[6] = _module18_(s[6], 120);
1322:    s[7] = _module18_(s[7], 108);
1323:}
1324:WITHIN_KERNEL INLINE void _module5_NTTInv8x8Lsh_5(_module0_* s)
1325:{
1326:    s[1] = _module14_(s[1], 177);
1327:    s[2] = _module14_(s[2], 162);
1328:    s[3] = _module16_(s[3], 147);
1329:    s[4] = _module16_(s[4], 132);
1330:    s[5] = _module18_(s[5], 117);
1331:    s[6] = _module18_(s[6], 102);
1332:    s[7] = _module12_(s[7], 87);
1333:}
1334:WITHIN_KERNEL INLINE void _module5_NTTInv8x8Lsh_6(_module0_* s)
1335:{
1336:    s[1] = _module14_(s[1], 174);
1337:    s[2] = _module16_(s[2], 156);
1338:    s[3] = _module16_(s[3], 138);
1339:    s[4] = _module18_(s[4], 120);
1340:    s[5] = _module18_(s[5], 102);
1341:    s[6] = _module12_(s[6], 84);
1342:    s[7] = _module12_(s[7], 66);
1343:}
1344:WITHIN_KERNEL INLINE void _module5_NTTInv8x8Lsh_7(_module0_* s)
1345:{
1346:    s[1] = _module14_(s[1], 171);
1347:    s[2] = _module16_(s[2], 150);
1348:    s[3] = _module16_(s[3], 129);
1349:    s[4] = _module18_(s[4], 108);
1350:    s[5] = _module12_(s[5], 87);
1351:    s[6] = _module12_(s[6], 66);
1352:    s[7] = _module10_(s[7], 45);
1353:}
1354:
1355:
1356:WITHIN_KERNEL INLINE void _module5_NTTInv8x8Lsh(_module0_* s, unsigned int col)
1357:{
1358:    if (1 == col)
1359:        _module5_NTTInv8x8Lsh_1(s);
1360:    else if (2 == col)
1361:        _module5_NTTInv8x8Lsh_2(s);
1362:    else if (3 == col)
1363:        _module5_NTTInv8x8Lsh_3(s);
1364:    else if (4 == col)
1365:        _module5_NTTInv8x8Lsh_4(s);
1366:    else if (5 == col)
1367:        _module5_NTTInv8x8Lsh_5(s);
1368:    else if (6 == col)
1369:        _module5_NTTInv8x8Lsh_6(s);
1370:    else if (7 == col)
1371:        _module5_NTTInv8x8Lsh_7(s);
1372:}
1373:
1374:
1375:WITHIN_KERNEL INLINE void _module5_Index3DFrom1D(uint3 *t3d, unsigned int t1d, unsigned int dim_x, unsigned int dim_y, unsigned int dim_z)
1376:{
1377:    t3d-&gt;x = t1d % dim_x;
1378:    t1d /= dim_x;
1379:    t3d-&gt;y = t1d % dim_y;
1380:    t3d-&gt;z = t1d / dim_y;
1381:}
1382:
1383:
1384:WITHIN_KERNEL INLINE void _module5__forward(
1385:        _module0_* r,
1386:        LOCAL_MEM_ARG _module0_* s,
1387:        _module5_CDATA_QUALIFIER _module0_* twd,
1388:        const unsigned int t1d)
1389:{
1390:    uint3 t3d;
1391:    _module5_Index3DFrom1D(&amp;t3d, t1d, 8, 8, 2);
1392:
1393:    LOCAL_MEM_ARG _module0_* ptr;
1394:
1395:    _module5_NTT8(r);
1396:    _module5_NTT8x2Lsh(r, t3d.z);
1397:    ptr = &amp;s[(t3d.y &lt;&lt; 7) | (t3d.z &lt;&lt; 6) | (t3d.x &lt;&lt; 2)];
1398:    #pragma unroll
1399:    for (unsigned int i = 0; i &lt; 8; i ++)
1400:        ptr[(i &gt;&gt; 2 &lt;&lt; 5) | (i &amp; 0x3)] = r[i];
1401:    LOCAL_BARRIER;
1402:
1403:    ptr = &amp;s[(t3d.z &lt;&lt; 9) | (t3d.y &lt;&lt; 3) | t3d.x];
1404:    #pragma unroll
1405:    for (unsigned int i = 0; i &lt; 8; i ++)
1406:        r[i] = ptr[i &lt;&lt; 6];
1407:    _module5_NTT2_pair(r);
1408:    _module5_NTT2_pair(r + 2);
1409:    _module5_NTT2_pair(r + 4);
1410:    _module5_NTT2_pair(r + 6);
1411:    #pragma unroll
1412:    for (unsigned int i = 0; i &lt; 8; i ++)
1413:        ptr[i &lt;&lt; 6] = r[i];
1414:    LOCAL_BARRIER;
1415:
1416:    ptr = &amp;s[t1d];
1417:    #pragma unroll
1418:    for (unsigned int i = 0; i &lt; 8; i ++)
1419:        r[i] = _module20_(ptr[i &lt;&lt; 7], twd[i &lt;&lt; 7 | t1d]); // mult twiddle
1420:    _module5_NTT8(r);
1421:    #pragma unroll
1422:    for (unsigned int i = 0; i &lt; 8; i ++)
1423:        ptr[i &lt;&lt; 7] = r[i];
1424:    LOCAL_BARRIER;
1425:
1426:    ptr = &amp;s[(t1d &gt;&gt; 2 &lt;&lt; 5) | (t3d.x &amp; 0x3)];
1427:    #pragma unroll
1428:    for (unsigned int i = 0; i &lt; 8; i ++)
1429:        r[i] = ptr[i &lt;&lt; 2];
1430:    _module5_NTT8x8Lsh(r, t1d &gt;&gt; 4); // less divergence if put here!
1431:    _module5_NTT8(r);
1432:}
1433:
1434:
1435:WITHIN_KERNEL INLINE void _module5__inverse(
1436:        _module0_* r,
1437:        LOCAL_MEM_ARG _module0_* s,
1438:        _module5_CDATA_QUALIFIER _module0_* twd,
1439:        const unsigned int t1d)
1440:{
1441:    uint3 t3d;
1442:    _module5_Index3DFrom1D(&amp;t3d, t1d, 8, 8, 2);
1443:
1444:    LOCAL_MEM_ARG _module0_* ptr;
1445:
1446:    _module5_NTTInv8(r);
1447:    _module5_NTTInv8x2Lsh(r, t3d.z);
1448:    ptr = &amp;s[(t3d.y &lt;&lt; 7) | (t3d.z &lt;&lt; 6) | (t3d.x &lt;&lt; 2)];
1449:    #pragma unroll
1450:    for (unsigned int i = 0; i &lt; 8; i ++)
1451:        ptr[(i &gt;&gt; 2 &lt;&lt; 5) | (i &amp; 0x3)] = r[i];
1452:    LOCAL_BARRIER;
1453:
1454:    ptr = &amp;s[(t3d.z &lt;&lt; 9) | (t3d.y &lt;&lt; 3) | t3d.x];
1455:    #pragma unroll
1456:    for (unsigned int i = 0; i &lt; 8; i ++)
1457:        r[i] = ptr[i &lt;&lt; 6];
1458:    _module5_NTT2_pair(r);
1459:    _module5_NTT2_pair(r + 2);
1460:    _module5_NTT2_pair(r + 4);
1461:    _module5_NTT2_pair(r + 6);
1462:    #pragma unroll
1463:    for (unsigned int i = 0; i &lt; 8; i ++)
1464:        ptr[i &lt;&lt; 6] = r[i];
1465:    LOCAL_BARRIER;
1466:
1467:    ptr = &amp;s[t1d];
1468:    #pragma unroll
1469:    for (unsigned int i = 0; i &lt; 8; i ++)
1470:        r[i] = _module20_(ptr[i &lt;&lt; 7], twd[i &lt;&lt; 7 | t1d]); // mult twiddle
1471:    _module5_NTTInv8(r);
1472:    #pragma unroll
1473:    for (unsigned int i = 0; i &lt; 8; i ++)
1474:        ptr[i &lt;&lt; 7] = r[i];
1475:    LOCAL_BARRIER;
1476:
1477:    ptr = &amp;s[(t1d &gt;&gt; 2 &lt;&lt; 5) | (t3d.x &amp; 0x3)];
1478:    #pragma unroll
1479:        for (unsigned int i = 0; i &lt; 8; i ++)
1480:    r[i] = ptr[i &lt;&lt; 2];
1481:    _module5_NTTInv8x8Lsh(r, t1d &gt;&gt; 4); // less divergence if put here!
1482:    _module5_NTTInv8(r);
1483:}
1484:
1485:
1486:WITHIN_KERNEL INLINE void _module5_forward(
1487:        _module0_* r_out,
1488:        _module0_* r_in,
1489:        LOCAL_MEM_ARG _module0_* temp,
1490:        _module5_CDATA_QUALIFIER _module0_* cdata,
1491:        unsigned int thread_in_xform)
1492:{
1493:    // Preprocess
1494:    r_out[0] = _module20_(
1495:        r_in[0],
1496:        cdata[1024 + 0 + thread_in_xform]
1497:        );
1498:    r_out[1] = _module20_(
1499:        r_in[1],
1500:        cdata[1024 + 128 + thread_in_xform]
1501:        );
1502:    r_out[2] = _module20_(
1503:        r_in[2],
1504:        cdata[1024 + 256 + thread_in_xform]
1505:        );
1506:    r_out[3] = _module20_(
1507:        r_in[3],
1508:        cdata[1024 + 384 + thread_in_xform]
1509:        );
1510:    r_out[4] = _module20_(
1511:        r_in[4],
1512:        cdata[1024 + 512 + thread_in_xform]
1513:        );
1514:    r_out[5] = _module20_(
1515:        r_in[5],
1516:        cdata[1024 + 640 + thread_in_xform]
1517:        );
1518:    r_out[6] = _module20_(
1519:        r_in[6],
1520:        cdata[1024 + 768 + thread_in_xform]
1521:        );
1522:    r_out[7] = _module20_(
1523:        r_in[7],
1524:        cdata[1024 + 896 + thread_in_xform]
1525:        );
1526:
1527:    _module5__forward(r_out, temp, cdata, thread_in_xform);
1528:}
1529:
1530:
1531:WITHIN_KERNEL INLINE void _module5_inverse(
1532:        _module0_* r_out,
1533:        _module0_* r_in,
1534:        LOCAL_MEM_ARG _module0_* temp,
1535:        _module5_CDATA_QUALIFIER _module0_* cdata,
1536:        unsigned int thread_in_xform)
1537:{
1538:    _module5__inverse(r_in, temp, cdata, thread_in_xform);
1539:
1540:    // Postprocess
1541:    r_out[0] = _module20_(
1542:        r_in[0],
1543:        cdata[1024 + 0 + thread_in_xform]
1544:        );
1545:    r_out[1] = _module20_(
1546:        r_in[1],
1547:        cdata[1024 + 128 + thread_in_xform]
1548:        );
1549:    r_out[2] = _module20_(
1550:        r_in[2],
1551:        cdata[1024 + 256 + thread_in_xform]
1552:        );
1553:    r_out[3] = _module20_(
1554:        r_in[3],
1555:        cdata[1024 + 384 + thread_in_xform]
1556:        );
1557:    r_out[4] = _module20_(
1558:        r_in[4],
1559:        cdata[1024 + 512 + thread_in_xform]
1560:        );
1561:    r_out[5] = _module20_(
1562:        r_in[5],
1563:        cdata[1024 + 640 + thread_in_xform]
1564:        );
1565:    r_out[6] = _module20_(
1566:        r_in[6],
1567:        cdata[1024 + 768 + thread_in_xform]
1568:        );
1569:    r_out[7] = _module20_(
1570:        r_in[7],
1571:        cdata[1024 + 896 + thread_in_xform]
1572:        );
1573:}
1574:
1575:
1576:WITHIN_KERNEL INLINE _module0_ _module5_i32_to_elem(int x)
1577:{
1578:    _module0_ res = { (unsigned long)x - (unsigned int)(-(x &lt; 0)) };
1579:    return res;
1580:}
1581:
1582:
1583:WITHIN_KERNEL INLINE int _module5_ff_to_i32(_module0_ x)
1584:{
1585:    // Interpreting anything &gt; P/2 as a negative integer,
1586:    // then taking modulo 2^31
1587:    const unsigned long med = 18446744069414584321UL / 2;
1588:    return (int)(x.val) - (x.val &gt; med);
1589:}
1590:
1591:
1592:
1593:WITHIN_KERNEL INLINE void _module5_forward_i32(
1594:        _module0_* r_out,
1595:        int* r_in,
1596:        LOCAL_MEM_ARG _module0_* temp,
1597:        _module5_CDATA_QUALIFIER _module0_* cdata,
1598:        unsigned int thread_in_xform)
1599:{
1600:    r_out[0] = _module5_i32_to_elem(r_in[0]);
1601:    r_out[1] = _module5_i32_to_elem(r_in[1]);
1602:    r_out[2] = _module5_i32_to_elem(r_in[2]);
1603:    r_out[3] = _module5_i32_to_elem(r_in[3]);
1604:    r_out[4] = _module5_i32_to_elem(r_in[4]);
1605:    r_out[5] = _module5_i32_to_elem(r_in[5]);
1606:    r_out[6] = _module5_i32_to_elem(r_in[6]);
1607:    r_out[7] = _module5_i32_to_elem(r_in[7]);
1608:    _module5_forward(r_out, r_out, temp, cdata, thread_in_xform);
1609:}
1610:
1611:
1612:WITHIN_KERNEL INLINE void _module5_inverse_i32(
1613:        int* r_out,
1614:        _module0_* r_in,
1615:        LOCAL_MEM_ARG _module0_* temp,
1616:        _module5_CDATA_QUALIFIER _module0_* cdata,
1617:        unsigned int thread_in_xform)
1618:{
1619:    _module5_inverse(r_in, r_in, temp, cdata, thread_in_xform);
1620:    r_out[0] = _module5_ff_to_i32(r_in[0]);
1621:    r_out[1] = _module5_ff_to_i32(r_in[1]);
1622:    r_out[2] = _module5_ff_to_i32(r_in[2]);
1623:    r_out[3] = _module5_ff_to_i32(r_in[3]);
1624:    r_out[4] = _module5_ff_to_i32(r_in[4]);
1625:    r_out[5] = _module5_ff_to_i32(r_in[5]);
1626:    r_out[6] = _module5_ff_to_i32(r_in[6]);
1627:    r_out[7] = _module5_ff_to_i32(r_in[7]);
1628:}
1629:
1630:
1631:WITHIN_KERNEL INLINE void _module5_noop()
1632:{
1633:    LOCAL_BARRIER;
1634:    LOCAL_BARRIER;
1635:    LOCAL_BARRIER;
1636:}
1637:
1638:
1639:WITHIN_KERNEL INLINE void _module5_forward_i32_shared(
1640:        LOCAL_MEM_ARG _module0_* in_out,
1641:        LOCAL_MEM_ARG _module0_* temp,
1642:        _module5_CDATA_QUALIFIER _module0_* cdata,
1643:        unsigned int thread_in_xform)
1644:{
1645:    _module0_ r[8];
1646:    r[0] = in_out[0 + thread_in_xform];
1647:    r[1] = in_out[128 + thread_in_xform];
1648:    r[2] = in_out[256 + thread_in_xform];
1649:    r[3] = in_out[384 + thread_in_xform];
1650:    r[4] = in_out[512 + thread_in_xform];
1651:    r[5] = in_out[640 + thread_in_xform];
1652:    r[6] = in_out[768 + thread_in_xform];
1653:    r[7] = in_out[896 + thread_in_xform];
1654:    LOCAL_BARRIER;
1655:    _module5_forward(r, r, temp, cdata, thread_in_xform);
1656:    LOCAL_BARRIER;
1657:    in_out[0 + thread_in_xform] = r[0];
1658:    in_out[128 + thread_in_xform] = r[1];
1659:    in_out[256 + thread_in_xform] = r[2];
1660:    in_out[384 + thread_in_xform] = r[3];
1661:    in_out[512 + thread_in_xform] = r[4];
1662:    in_out[640 + thread_in_xform] = r[5];
1663:    in_out[768 + thread_in_xform] = r[6];
1664:    in_out[896 + thread_in_xform] = r[7];
1665:}
1666:
1667:
1668:WITHIN_KERNEL INLINE void _module5_inverse_i32_shared_add(
1669:        LOCAL_MEM_ARG int* out,
1670:        LOCAL_MEM_ARG _module0_* in,
1671:        LOCAL_MEM_ARG _module0_* temp,
1672:        _module5_CDATA_QUALIFIER _module0_* cdata,
1673:        unsigned int thread_in_xform)
1674:{
1675:    _module0_ r[8];
1676:    r[0] = in[0 + thread_in_xform];
1677:    r[1] = in[128 + thread_in_xform];
1678:    r[2] = in[256 + thread_in_xform];
1679:    r[3] = in[384 + thread_in_xform];
1680:    r[4] = in[512 + thread_in_xform];
1681:    r[5] = in[640 + thread_in_xform];
1682:    r[6] = in[768 + thread_in_xform];
1683:    r[7] = in[896 + thread_in_xform];
1684:    LOCAL_BARRIER;
1685:    _module5_inverse(r, r, temp, cdata, thread_in_xform);
1686:    LOCAL_BARRIER;
1687:    out[0 + thread_in_xform] += _module5_ff_to_i32(r[0]);
1688:    out[128 + thread_in_xform] += _module5_ff_to_i32(r[1]);
1689:    out[256 + thread_in_xform] += _module5_ff_to_i32(r[2]);
1690:    out[384 + thread_in_xform] += _module5_ff_to_i32(r[3]);
1691:    out[512 + thread_in_xform] += _module5_ff_to_i32(r[4]);
1692:    out[640 + thread_in_xform] += _module5_ff_to_i32(r[5]);
1693:    out[768 + thread_in_xform] += _module5_ff_to_i32(r[6]);
1694:    out[896 + thread_in_xform] += _module5_ff_to_i32(r[7]);
1695:}
1696:
1697:
1698:WITHIN_KERNEL INLINE void _module5_noop_shared()
1699:{
1700:    LOCAL_BARRIER;
1701:    _module5_noop();
1702:    LOCAL_BARRIER;
1703:}
1704:
1705:
1706:
1707:
1708:
1709:    // leaf input macro for &quot;gsw&quot;
1710:    #define _module22_(_idx0, _idx1, _idx2, _idx3, _idx4) (_leaf_gsw[(_idx0) * (8192) + (_idx1) * (4096) + (_idx2) * (2048) + (_idx3) * (1024) + (_idx4) * (1) + (0)])
1711:    
1712:
1713:
1714:
1715:// Addition in FF(P): val_ = a + b mod P.
1716:WITHIN_KERNEL INLINE _module0_ _module23_(_module0_ a, _module0_ b)
1717:{
1718:
1719:    /*
1720:    Algorithm:
1721:    We calculate `s = x + y`
1722:    Now there are three variants:
1723:    - `s &lt; P` and no integer overflow: all good, `result = s`.
1724:    - `s &gt; P` and no integer overflow: `result = s - P = s + (2^32 - 1)`
1725:    - integer overflow, so essentially `s = x + y - N`.
1726:      This means that we need to calculate `result = s + N - P = s + (2^32 - 1)`.
1727:    Note that the last two variants result in the same modifier being applied.
1728:    */
1729:    _module0_ res = {a.val + b.val};
1730:    res.val += ((res.val &lt; b.val) || res.val &gt;= 18446744069414584321UL) ? 0xffffffff : 0;
1731:    return res;
1732:
1733:}
1734:
1735:
1736:
1737:/** Subtraction in FF(P): val_ = a + b mod P. */
1738:WITHIN_KERNEL INLINE _module0_ _module25_(_module0_ a, _module0_ b)
1739:{
1740:
1741:    /*
1742:    Algorithm:
1743:    We calculate `s = x - y`
1744:    Now there are three variants:
1745:    - no underflow (x &gt;= y): all good, `result = s`.
1746:    - underflow (detected if `s &gt; x`), so essentially `s = x - y + N`.
1747:      This means we need to calculate `s - N + P = s - (2^32 - 1)`
1748:    */
1749:
1750:    _module0_ res = {a.val - b.val};
1751:    unsigned int x = -(res.val &gt; a.val);
1752:    res.val -= x;
1753:    return res;
1754:
1755:}
1756:
1757:
1758:
1759:WITHIN_KERNEL INLINE _module0_ _module24_(_module0_ a, _module0_ b)
1760:{
1761:    /*
1762:    This function performs Montgomery multiplication with the fixed modulus (M=2**64-2**32+1)
1763:    and fixed word size (R=2**64), which helps simplify the algorithm.
1764:    The result is `a * b * R**(-1) mod M`.
1765:
1766:    Note that if you multiply two numbers `a` and `b` in Montgomery representation
1767:    (a&#x27; = a * R mod M, b&#x27; = b * R mod M), the result is the Montgomery representation of their
1768:    product (a&#x27; * b&#x27; * R**(-1) mod M = a * b * R mod M).
1769:    But if one of the numbers is in Montgomery representation and the other is not, you
1770:    get the normal product back (a * b&#x27; * R**(-1) mod M = a * b mod M).
1771:
1772:    This way if one of the factors is precalculated, you can convert it
1773:    into Montgomery representation, and use this function instead of the general
1774:    multiplication function (which is slower).
1775:    */
1776:
1777:
1778:    #ifdef CUDA
1779:    unsigned long hi = __umul64hi(a.val, b.val);
1780:    #else
1781:    unsigned long hi = mul_hi(a.val, b.val);
1782:    #endif
1783:    unsigned long lo = a.val * b.val;
1784:
1785:    unsigned long u = (lo &lt;&lt; 32) + lo;
1786:    unsigned long p2 = u - (u &gt;&gt; 32);
1787:    unsigned long uu = u &lt;&lt; 32;
1788:    if (uu &gt; u)
1789:    {
1790:        p2 -= 1;
1791:    }
1792:
1793:
1794:    _module0_ hi_ff = { hi };
1795:    _module0_ p2_ff = { p2 };
1796:
1797:    return _module25_(hi_ff, p2_ff);
1798:}
1799:
1800:
1801:
1802:
1803:    // leaf output macro for &quot;_temp2&quot;
1804:    #define _module27_(_idx0, _val) _leaf__temp2[(_idx0) * (1) + (0)] = (_val)
1805:    
1806:
1807:
1808:
1809:
1810:    
1811:    INLINE WITHIN_KERNEL void _module26_func(
1812:        GLOBAL_MEM int *_leaf__temp2, VSIZE_T _c_idx0, int _val)
1813:    {
1814:        
1815:
1816:    
1817:        
1818:        VSIZE_T _idx0 = _c_idx0 / 1;
1819:    
1820:
1821:
1822:        _module27_(_idx0, _val);
1823:    }
1824:    
1825:    #define _module26_(_c_idx0, _val) _module26_func(        _leaf__temp2, _c_idx0, _val)
1826:    
1827:
1828:
1829:
1830:
1831:    // leaf output macro for &quot;_temp1&quot;
1832:    #define _module29_(_idx0, _idx1, _val) _leaf__temp1[(_idx0) * (1024) + (_idx1) * (1) + (0)] = (_val)
1833:    
1834:
1835:
1836:
1837:
1838:    
1839:    INLINE WITHIN_KERNEL void _module28_func(
1840:        GLOBAL_MEM int *_leaf__temp1, VSIZE_T _c_idx0, VSIZE_T _c_idx1, int _val)
1841:    {
1842:        
1843:
1844:    
1845:        
1846:        VSIZE_T _idx0 = _c_idx0 / 1;
1847:    
1848:        
1849:        VSIZE_T _idx1 = _c_idx1 / 1;
1850:    
1851:
1852:
1853:        _module29_(_idx0, _idx1, _val);
1854:    }
1855:    
1856:    #define _module28_(_c_idx0, _c_idx1, _val) _module28_func(        _leaf__temp1, _c_idx0, _c_idx1, _val)
1857:    
1858:
1859:
1860:
1861:
1862:
1863:
1864:
1865:KERNEL void blind_rotate(GLOBAL_MEM int *_leaf__temp1, GLOBAL_MEM int *_leaf__temp2, GLOBAL_MEM int *_leaf_accum_a, GLOBAL_MEM unsigned long *_leaf_gsw, GLOBAL_MEM int *_leaf_bara, GLOBAL_MEM unsigned long *_leaf__nested1__value1, GLOBAL_MEM unsigned long *_leaf__nested1__value2)
1866:
1867:{
1868:    VIRTUAL_SKIP_THREADS;
1869:
1870:    // We are trying to minimize the number of local memory buffers used.
1871:    // We need `decomp_length * (mask_size + 1)` buffers to store the transformed data,
1872:    // then all of them are used to fill `(mask_size + 1)` output buffers.
1873:    // `mask_size` of them should be put outside of the input buffers,
1874:    // but the last one can overwrite one of the input buffers.
1875:    //
1876:    // So, for example, for `mask_size=1` and `decomp_length=2`, we need
1877:    // `(1 + 1) * 2 = 4` input buffers and one additional output buffer.
1878:    LOCAL_MEM char sh_char[40960];
1879:    LOCAL_MEM int shared_accum[2048];
1880:
1881:    LOCAL_MEM_ARG _module0_* sh = (LOCAL_MEM_ARG _module0_*)sh_char;
1882:
1883:    const unsigned int batch_id = virtual_group_id(0);
1884:    const unsigned int tid = virtual_local_id(1);
1885:    const unsigned int transform_id = tid / 128;
1886:    const unsigned int mask_id = transform_id % 2;
1887:    const unsigned int decomp_id = transform_id / 2;
1888:    const unsigned int thread_in_transform = tid % 128;
1889:
1890:    // Load accum
1891:    if (tid &lt; 256)
1892:    {
1893:        #pragma unroll
1894:        for (unsigned int i = 0; i &lt; 8; i++)
1895:        {
1896:            shared_accum[mask_id * 1024 + i * 128 + thread_in_transform] =
1897:                _module1_(
1898:                    batch_id, mask_id, i * 128 + thread_in_transform);
1899:        }
1900:    }
1901:
1902:    LOCAL_BARRIER;
1903:
1904:    for (unsigned int bk_idx = 0; bk_idx &lt; 500; bk_idx++)
1905:    {
1906:
1907:    int ai = _module3_(batch_id, bk_idx);
1908:
1909:    {
1910:        
1911:
1912:        int temp0;
1913:
1914:        #pragma unroll
1915:        for (int i = tid; i &lt; 1024; i += 640)
1916:        {
1917:            int i0 = i + 0;
1918:            unsigned int cmp0 = (unsigned int)(i0 &lt; (ai &amp; 1023));
1919:            unsigned int neg0 = -(cmp0 ^ (ai &gt;&gt; 10));
1920:            unsigned int pos0 = -((1 - cmp0) ^ (ai &gt;&gt; 10));
1921:
1922:
1923:                temp0 = shared_accum[(0) | ((i0 - ai) &amp; 1023)];
1924:                temp0 = (temp0 &amp; pos0) + ((-temp0) &amp; neg0);
1925:                temp0 -= shared_accum[(0) | i0];
1926:                // decomp temp
1927:                temp0 += 2149580800;
1928:
1929:                    sh[0 + i] =
1930:                        _module5_i32_to_elem(
1931:                            ((temp0 &gt;&gt; 22)
1932:                                &amp; 1023) - 512
1933:                        );
1934:                    sh[2048 + i] =
1935:                        _module5_i32_to_elem(
1936:                            ((temp0 &gt;&gt; 12)
1937:                                &amp; 1023) - 512
1938:                        );
1939:
1940:                temp0 = shared_accum[(1024) | ((i0 - ai) &amp; 1023)];
1941:                temp0 = (temp0 &amp; pos0) + ((-temp0) &amp; neg0);
1942:                temp0 -= shared_accum[(1024) | i0];
1943:                // decomp temp
1944:                temp0 += 2149580800;
1945:
1946:                    sh[1024 + i] =
1947:                        _module5_i32_to_elem(
1948:                            ((temp0 &gt;&gt; 22)
1949:                                &amp; 1023) - 512
1950:                        );
1951:                    sh[3072 + i] =
1952:                        _module5_i32_to_elem(
1953:                            ((temp0 &gt;&gt; 12)
1954:                                &amp; 1023) - 512
1955:                        );
1956:        }
1957:    }
1958:
1959:    LOCAL_BARRIER;
1960:
1961:    if (tid &lt; 512)
1962:    {
1963:        // Forward transform
1964:        _module5_forward_i32_shared(
1965:            sh + (decomp_id * 2 + mask_id) * 1024,
1966:            (LOCAL_MEM_ARG _module0_*)(
1967:                sh + (decomp_id * 2 + mask_id) * 1024),
1968:            (_module5_CDATA_QUALIFIER _module0_*)_leaf__nested1__value1,
1969:            thread_in_transform);
1970:    }
1971:    else
1972:    {
1973:        _module5_noop_shared();
1974:    }
1975:
1976:    LOCAL_BARRIER;
1977:
1978:    {
1979:        _module0_ t, a, b;
1980:        {
1981:            int idx = 0 + tid;
1982:
1983:
1984:            t = _module0_zero;
1985:            a = sh[0 + idx];
1986:            b = _module0_pack(
1987:                    _module22_(bk_idx, 0, 0, 1, idx));
1988:            t = _module23_(t, _module24_(a, b));
1989:            a = sh[2048 + idx];
1990:            b = _module0_pack(
1991:                    _module22_(bk_idx, 0, 1, 1, idx));
1992:            t = _module23_(t, _module24_(a, b));
1993:            a = sh[1024 + idx];
1994:            b = _module0_pack(
1995:                    _module22_(bk_idx, 1, 0, 1, idx));
1996:            t = _module23_(t, _module24_(a, b));
1997:            a = sh[3072 + idx];
1998:            b = _module0_pack(
1999:                    _module22_(bk_idx, 1, 1, 1, idx));
2000:            t = _module23_(t, _module24_(a, b));
2001:
2002:            
2003:            sh[4096 + idx] = t;
2004:
2005:        }
2006:        {
2007:            int idx = 640 + tid;
2008:
2009:            if (idx &lt; 1024)
2010:            {
2011:
2012:            t = _module0_zero;
2013:            a = sh[0 + idx];
2014:            b = _module0_pack(
2015:                    _module22_(bk_idx, 0, 0, 1, idx));
2016:            t = _module23_(t, _module24_(a, b));
2017:            a = sh[2048 + idx];
2018:            b = _module0_pack(
2019:                    _module22_(bk_idx, 0, 1, 1, idx));
2020:            t = _module23_(t, _module24_(a, b));
2021:            a = sh[1024 + idx];
2022:            b = _module0_pack(
2023:                    _module22_(bk_idx, 1, 0, 1, idx));
2024:            t = _module23_(t, _module24_(a, b));
2025:            a = sh[3072 + idx];
2026:            b = _module0_pack(
2027:                    _module22_(bk_idx, 1, 1, 1, idx));
2028:            t = _module23_(t, _module24_(a, b));
2029:
2030:            
2031:            sh[4096 + idx] = t;
2032:
2033:            }
2034:        }
2035:    }
2036:    LOCAL_BARRIER;
2037:    {
2038:        _module0_ t, a, b;
2039:        {
2040:            int idx = 0 + tid;
2041:
2042:
2043:            t = _module0_zero;
2044:            a = sh[0 + idx];
2045:            b = _module0_pack(
2046:                    _module22_(bk_idx, 0, 0, 0, idx));
2047:            t = _module23_(t, _module24_(a, b));
2048:            a = sh[2048 + idx];
2049:            b = _module0_pack(
2050:                    _module22_(bk_idx, 0, 1, 0, idx));
2051:            t = _module23_(t, _module24_(a, b));
2052:            a = sh[1024 + idx];
2053:            b = _module0_pack(
2054:                    _module22_(bk_idx, 1, 0, 0, idx));
2055:            t = _module23_(t, _module24_(a, b));
2056:            a = sh[3072 + idx];
2057:            b = _module0_pack(
2058:                    _module22_(bk_idx, 1, 1, 0, idx));
2059:            t = _module23_(t, _module24_(a, b));
2060:
2061:            
2062:            sh[0 + idx] = t;
2063:
2064:        }
2065:        {
2066:            int idx = 640 + tid;
2067:
2068:            if (idx &lt; 1024)
2069:            {
2070:
2071:            t = _module0_zero;
2072:            a = sh[0 + idx];
2073:            b = _module0_pack(
2074:                    _module22_(bk_idx, 0, 0, 0, idx));
2075:            t = _module23_(t, _module24_(a, b));
2076:            a = sh[2048 + idx];
2077:            b = _module0_pack(
2078:                    _module22_(bk_idx, 0, 1, 0, idx));
2079:            t = _module23_(t, _module24_(a, b));
2080:            a = sh[1024 + idx];
2081:            b = _module0_pack(
2082:                    _module22_(bk_idx, 1, 0, 0, idx));
2083:            t = _module23_(t, _module24_(a, b));
2084:            a = sh[3072 + idx];
2085:            b = _module0_pack(
2086:                    _module22_(bk_idx, 1, 1, 0, idx));
2087:            t = _module23_(t, _module24_(a, b));
2088:
2089:            
2090:            sh[0 + idx] = t;
2091:
2092:            }
2093:        }
2094:    }
2095:    LOCAL_BARRIER;
2096:
2097:    // Inverse transform
2098:    if (tid &lt; 256)
2099:    {
2100:        // Following the temporary buffer usage scheme described at the beginning of the kernel.
2101:        int temp_id = mask_id == 0 ? 0 : 3 + mask_id;
2102:
2103:        _module5_inverse_i32_shared_add(
2104:            shared_accum + mask_id * 1024,
2105:            sh + temp_id * 1024,
2106:            (LOCAL_MEM_ARG _module0_*)(sh + temp_id * 1024),
2107:            (_module5_CDATA_QUALIFIER _module0_*)_leaf__nested1__value2,
2108:            thread_in_transform);
2109:    }
2110:    else
2111:    {
2112:        _module5_noop_shared();
2113:    }
2114:
2115:    LOCAL_BARRIER;
2116:    }
2117:
2118:    for (int i = tid; i &lt;= 1024; i += 640)
2119:    {
2120:        if (i == 1024)
2121:        {
2122:            _module26_(batch_id, shared_accum[1024]);
2123:        }
2124:        else
2125:        {
2126:            _module28_(
2127:                batch_id, i, i == 0 ? shared_accum[0] : -shared_accum[1024 - i]);
2128:        }
2129:    }
2130:}
2131:<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_ntt_lsh_method_performance[bs_kernel-cuda:0:0-ntt_lsh=cuda_asm]</td>
          <td class="col-duration">13.20</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f881594a8&gt;, single_kernel_bootstrap = True<br/>heavy_performance_load = False, ntt_lsh_method = &#x27;cuda_asm&#x27;<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(<br/>        &#x27;ntt_lsh_method&#x27;,<br/>        [&#x27;cuda_asm&#x27;, &#x27;c_from_asm&#x27;, &#x27;c&#x27;],<br/>        ids=[&#x27;ntt_lsh=cuda_asm&#x27;, &#x27;ntt_lsh=c_from_asm&#x27;, &#x27;ntt_lsh=c&#x27;])<br/>    def test_ntt_lsh_method_performance(<br/>            thread, single_kernel_bootstrap, heavy_performance_load, ntt_lsh_method):<br/>    <br/>        if thread.api.get_id() != cuda_id() and ntt_lsh_method == &#x27;cuda_asm&#x27;:<br/>            pytest.skip()<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>        secret_key, cloud_key = make_key_pair(thread, rng, transform_type=&#x27;NTT&#x27;)<br/>    <br/>        # TODO: instead of creating a whole key and then checking if the parameters are supported,<br/>        # we can just create a parameter object separately.<br/>        if (single_kernel_bootstrap<br/>                and not single_kernel_bootstrap_supported(secret_key.params, thread.device_params)):<br/>            pytest.skip()<br/>    <br/>        perf_params = PerformanceParameters(<br/>            secret_key.params,<br/>            single_kernel_bootstrap=single_kernel_bootstrap,<br/>            ntt_lsh_method=ntt_lsh_method).for_device(thread.device_params)<br/>    <br/>&gt;       results = check_performance(thread, (secret_key, cloud_key), perf_params, shape=size)<br/><br/>test/test_gates.py:509: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_gates.py:276: in check_performance<br/>    shape=shape1, performance_test=True, perf_params=perf_params)<br/>test/test_gates.py:65: in check_gate<br/>    nufhe_func(thread, cloud_key, answer, *ciphertexts, perf_params)<br/>nufhe/gates.py:121: in gate_nand<br/>    MU, temp_result, perf_params)<br/>nufhe/bootstrap.py:229: in bootstrap<br/>    no_keyswitch=no_keyswitch)<br/>nufhe/bootstrap.py:186: in blind_rotate_and_extract<br/>    BlindRotate_gpu(result, acc, bk, ks, bara, perf_params, no_keyswitch=no_keyswitch)<br/>nufhe/blind_rotate.py:278: in BlindRotate_gpu<br/>    ks.log2_base, ks.decomp_length, perf_params)<br/>nufhe/computation_cache.py:55: in get_computation<br/>    compiled_comp = comp.compile(thr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/blind_rotate.py:250: in _build_plan<br/>    plan.computation_call(blind_rotate, extracted_a, extracted_b, accum_a, gsw, bara)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:502: in computation_call<br/>    self._compiler_options, self._keep))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;nufhe.blind_rotate.BlindRotate object at 0x7f881ba9e8&gt;<br/>plan_factory = &lt;function Computation._get_plan.&lt;locals&gt;.&lt;lambda&gt; at 0x7f882c80d0&gt;<br/>device_params = &lt;reikna.cluda.cuda.DeviceParameters object at 0x7f88159400&gt;<br/>lwe_a = KernelArgument(_temp1), lwe_b = KernelArgument(_temp2)<br/>accum_a = KernelArgument(accum_a), gsw = KernelArgument(gsw), bara = KernelArgument(bara)<br/><br/>    def _build_plan(self, plan_factory, device_params, lwe_a, lwe_b, accum_a, gsw, bara):<br/>    <br/>        params = self._params<br/>        tlwe_params = params.tlwe_params<br/>        decomp_length = params.decomp_length<br/>        mask_size = tlwe_params.mask_size<br/>    <br/>        perf_params = self._perf_params<br/>        transform_type = self._params.tlwe_params.transform_type<br/>        transform = get_transform(transform_type)<br/>    <br/>        transform_module = transform.transform_module(perf_params, multi_iter=True)<br/>    <br/>        batch_shape = accum_a.shape[:-2]<br/>    <br/>        min_local_size = decomp_length * (mask_size + 1) * transform_module.threads_per_transform<br/>        local_size = device_params.max_work_group_size<br/>        while local_size &gt;= min_local_size:<br/>    <br/>            plan = plan_factory()<br/>    <br/>            if transform_module.use_constant_memory:<br/>                cdata_forward = plan.constant_array(transform_module.cdata_fw)<br/>                cdata_inverse = plan.constant_array(transform_module.cdata_inv)<br/>            else:<br/>                cdata_forward = plan.persistent_array(transform_module.cdata_fw)<br/>                cdata_inverse = plan.persistent_array(transform_module.cdata_inv)<br/>    <br/>            try:<br/>                plan.kernel_call(<br/>                    TEMPLATE.get_def(&quot;blind_rotate&quot;),<br/>                    [lwe_a, lwe_b, accum_a, gsw, bara, cdata_forward, cdata_inverse],<br/>                    kernel_name=&quot;blind_rotate&quot;,<br/>                    global_size=(<br/>                        helpers.product(batch_shape),<br/>                        local_size),<br/>                    local_size=(1, local_size),<br/>                    render_kwds=dict(<br/>                        local_size=local_size,<br/>                        slices=(len(batch_shape), 1, 1),<br/>                        slices2=(len(batch_shape), 1),<br/>                        slices3=(len(batch_shape),),<br/>                        transform=transform_module,<br/>                        mask_size=mask_size,<br/>                        decomp_length=decomp_length,<br/>                        output_size=self._in_out_params.size,<br/>                        input_size=tlwe_params.extracted_lweparams.size,<br/>                        bs_log2_base=self._params.bs_log2_base,<br/>                        mul_prepared=transform.transformed_mul_prepared(perf_params),<br/>                        add=transform.transformed_add(perf_params),<br/>                        tr_ctype=transform.transformed_internal_ctype(),<br/>                        min_blocks=helpers.min_blocks,<br/>                        )<br/>                    )<br/>            except OutOfResourcesError:<br/>                local_size -= transform_module.threads_per_transform<br/>                continue<br/>    <br/>            return plan<br/>    <br/>&gt;       raise ValueError(&quot;Could not find suitable local size for the kernel&quot;)<br/><span class="error">E       ValueError: Could not find suitable local size for the kernel</span><br/><br/>nufhe/blind_rotate.py:187: ValueError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_ntt_lsh_method_performance[bs_kernel-cuda:0:0-ntt_lsh=c_from_asm]</td>
          <td class="col-duration">5.84</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f881594a8&gt;, single_kernel_bootstrap = True<br/>heavy_performance_load = False, ntt_lsh_method = &#x27;c_from_asm&#x27;<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(<br/>        &#x27;ntt_lsh_method&#x27;,<br/>        [&#x27;cuda_asm&#x27;, &#x27;c_from_asm&#x27;, &#x27;c&#x27;],<br/>        ids=[&#x27;ntt_lsh=cuda_asm&#x27;, &#x27;ntt_lsh=c_from_asm&#x27;, &#x27;ntt_lsh=c&#x27;])<br/>    def test_ntt_lsh_method_performance(<br/>            thread, single_kernel_bootstrap, heavy_performance_load, ntt_lsh_method):<br/>    <br/>        if thread.api.get_id() != cuda_id() and ntt_lsh_method == &#x27;cuda_asm&#x27;:<br/>            pytest.skip()<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>        secret_key, cloud_key = make_key_pair(thread, rng, transform_type=&#x27;NTT&#x27;)<br/>    <br/>        # TODO: instead of creating a whole key and then checking if the parameters are supported,<br/>        # we can just create a parameter object separately.<br/>        if (single_kernel_bootstrap<br/>                and not single_kernel_bootstrap_supported(secret_key.params, thread.device_params)):<br/>            pytest.skip()<br/>    <br/>        perf_params = PerformanceParameters(<br/>            secret_key.params,<br/>            single_kernel_bootstrap=single_kernel_bootstrap,<br/>            ntt_lsh_method=ntt_lsh_method).for_device(thread.device_params)<br/>    <br/>&gt;       results = check_performance(thread, (secret_key, cloud_key), perf_params, shape=size)<br/><br/>test/test_gates.py:509: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>test/test_gates.py:276: in check_performance<br/>    shape=shape1, performance_test=True, perf_params=perf_params)<br/>test/test_gates.py:65: in check_gate<br/>    nufhe_func(thread, cloud_key, answer, *ciphertexts, perf_params)<br/>nufhe/gates.py:121: in gate_nand<br/>    MU, temp_result, perf_params)<br/>nufhe/bootstrap.py:229: in bootstrap<br/>    no_keyswitch=no_keyswitch)<br/>nufhe/bootstrap.py:186: in blind_rotate_and_extract<br/>    BlindRotate_gpu(result, acc, bk, ks, bara, perf_params, no_keyswitch=no_keyswitch)<br/>nufhe/blind_rotate.py:278: in BlindRotate_gpu<br/>    ks.log2_base, ks.decomp_length, perf_params)<br/>nufhe/computation_cache.py:55: in get_computation<br/>    compiled_comp = comp.compile(thr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/blind_rotate.py:250: in _build_plan<br/>    plan.computation_call(blind_rotate, extracted_a, extracted_b, accum_a, gsw, bara)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:502: in computation_call<br/>    self._compiler_options, self._keep))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;nufhe.blind_rotate.BlindRotate object at 0x7f88172e10&gt;<br/>plan_factory = &lt;function Computation._get_plan.&lt;locals&gt;.&lt;lambda&gt; at 0x7f882d8400&gt;<br/>device_params = &lt;reikna.cluda.cuda.DeviceParameters object at 0x7f88159400&gt;<br/>lwe_a = KernelArgument(_temp1), lwe_b = KernelArgument(_temp2)<br/>accum_a = KernelArgument(accum_a), gsw = KernelArgument(gsw), bara = KernelArgument(bara)<br/><br/>    def _build_plan(self, plan_factory, device_params, lwe_a, lwe_b, accum_a, gsw, bara):<br/>    <br/>        params = self._params<br/>        tlwe_params = params.tlwe_params<br/>        decomp_length = params.decomp_length<br/>        mask_size = tlwe_params.mask_size<br/>    <br/>        perf_params = self._perf_params<br/>        transform_type = self._params.tlwe_params.transform_type<br/>        transform = get_transform(transform_type)<br/>    <br/>        transform_module = transform.transform_module(perf_params, multi_iter=True)<br/>    <br/>        batch_shape = accum_a.shape[:-2]<br/>    <br/>        min_local_size = decomp_length * (mask_size + 1) * transform_module.threads_per_transform<br/>        local_size = device_params.max_work_group_size<br/>        while local_size &gt;= min_local_size:<br/>    <br/>            plan = plan_factory()<br/>    <br/>            if transform_module.use_constant_memory:<br/>                cdata_forward = plan.constant_array(transform_module.cdata_fw)<br/>                cdata_inverse = plan.constant_array(transform_module.cdata_inv)<br/>            else:<br/>                cdata_forward = plan.persistent_array(transform_module.cdata_fw)<br/>                cdata_inverse = plan.persistent_array(transform_module.cdata_inv)<br/>    <br/>            try:<br/>                plan.kernel_call(<br/>                    TEMPLATE.get_def(&quot;blind_rotate&quot;),<br/>                    [lwe_a, lwe_b, accum_a, gsw, bara, cdata_forward, cdata_inverse],<br/>                    kernel_name=&quot;blind_rotate&quot;,<br/>                    global_size=(<br/>                        helpers.product(batch_shape),<br/>                        local_size),<br/>                    local_size=(1, local_size),<br/>                    render_kwds=dict(<br/>                        local_size=local_size,<br/>                        slices=(len(batch_shape), 1, 1),<br/>                        slices2=(len(batch_shape), 1),<br/>                        slices3=(len(batch_shape),),<br/>                        transform=transform_module,<br/>                        mask_size=mask_size,<br/>                        decomp_length=decomp_length,<br/>                        output_size=self._in_out_params.size,<br/>                        input_size=tlwe_params.extracted_lweparams.size,<br/>                        bs_log2_base=self._params.bs_log2_base,<br/>                        mul_prepared=transform.transformed_mul_prepared(perf_params),<br/>                        add=transform.transformed_add(perf_params),<br/>                        tr_ctype=transform.transformed_internal_ctype(),<br/>                        min_blocks=helpers.min_blocks,<br/>                        )<br/>                    )<br/>            except OutOfResourcesError:<br/>                local_size -= transform_module.threads_per_transform<br/>                continue<br/>    <br/>            return plan<br/>    <br/>&gt;       raise ValueError(&quot;Could not find suitable local size for the kernel&quot;)<br/><span class="error">E       ValueError: Could not find suitable local size for the kernel</span><br/><br/>nufhe/blind_rotate.py:187: ValueError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_ntt_lsh_method_performance[bs_kernel-cuda:0:0-ntt_lsh=c]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f881594a8&gt;, single_kernel_bootstrap = True<br/>heavy_performance_load = False, ntt_lsh_method = &#x27;c&#x27;<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(<br/>        &#x27;ntt_lsh_method&#x27;,<br/>        [&#x27;cuda_asm&#x27;, &#x27;c_from_asm&#x27;, &#x27;c&#x27;],<br/>        ids=[&#x27;ntt_lsh=cuda_asm&#x27;, &#x27;ntt_lsh=c_from_asm&#x27;, &#x27;ntt_lsh=c&#x27;])<br/>    def test_ntt_lsh_method_performance(<br/>            thread, single_kernel_bootstrap, heavy_performance_load, ntt_lsh_method):<br/>    <br/>        if thread.api.get_id() != cuda_id() and ntt_lsh_method == &#x27;cuda_asm&#x27;:<br/>            pytest.skip()<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, transform_type=&#x27;NTT&#x27;)<br/><br/>test/test_gates.py:496: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f881594a8&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_gate_over_view[bs_kernel-cuda:0:0]</td>
          <td class="col-duration">5.84</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f82fd2278&gt;<br/>key_pair = (&lt;nufhe.api_low_level.NuFHESecretKey object at 0x7f88281978&gt;, &lt;nufhe.api_low_level.NuFHECloudKey object at 0x7f82f111d0&gt;)<br/>single_kernel_bootstrap = True<br/><br/>    def test_gate_over_view(thread, key_pair, single_kernel_bootstrap):<br/>    <br/>        secret_key, cloud_key = key_pair<br/>        params = cloud_key.params<br/>    <br/>        if (single_kernel_bootstrap<br/>                and not single_kernel_bootstrap_supported(params, thread.device_params)):<br/>            pytest.skip()<br/>    <br/>        perf_params = PerformanceParameters(params, single_kernel_bootstrap=single_kernel_bootstrap)<br/>        perf_params = perf_params.for_device(thread.device_params)<br/>    <br/>        nufhe_func = gate_nand<br/>        reference_func = nand_ref<br/>        num_arguments = 2<br/>    <br/>        rng = DeterministicRNG()<br/>    <br/>        shape = (5, 8)<br/>    <br/>        # FIXME: negative steps are supported as well, but the current stable PyCUDA<br/>        # has a bug where in that case it calculates strides incorrectly.<br/>        # It is fixed in the trunk, so we must add some negative steps here as soon as it is released.<br/>        slices1 = (slice(3, 5), slice(1, 7, 2))<br/>        slices2 = (slice(1, 3), slice(2, 8, 2))<br/>        result_slices = (slice(2, 4), slice(0, 6, 2))<br/>    <br/>        plaintexts = get_plaintexts(rng, num_arguments, shape=shape)<br/>        pt1 = plaintexts[0][slices1]<br/>        pt2 = plaintexts[1][slices2]<br/>    <br/>        ciphertexts = [encrypt(thread, rng, secret_key, plaintext) for plaintext in plaintexts]<br/>        ct1 = ciphertexts[0][slices1]<br/>        ct2 = ciphertexts[1][slices2]<br/>    <br/>        reference = reference_func(pt1, pt2)<br/>    <br/>        answer = empty_ciphertext(thread, params, shape)<br/>        answer_view = answer[result_slices]<br/>    <br/>&gt;       nufhe_func(thread, cloud_key, answer_view, ct1, ct2, perf_params=perf_params)<br/><br/>test/test_gates.py:554: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/gates.py:121: in gate_nand<br/>    MU, temp_result, perf_params)<br/>nufhe/bootstrap.py:229: in bootstrap<br/>    no_keyswitch=no_keyswitch)<br/>nufhe/bootstrap.py:186: in blind_rotate_and_extract<br/>    BlindRotate_gpu(result, acc, bk, ks, bara, perf_params, no_keyswitch=no_keyswitch)<br/>nufhe/blind_rotate.py:278: in BlindRotate_gpu<br/>    ks.log2_base, ks.decomp_length, perf_params)<br/>nufhe/computation_cache.py:55: in get_computation<br/>    compiled_comp = comp.compile(thr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:207: in compile<br/>    self._tr_tree, translator, thread, fast_math, compiler_options, keep).finalize()<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>nufhe/blind_rotate.py:250: in _build_plan<br/>    plan.computation_call(blind_rotate, extracted_a, extracted_b, accum_a, gsw, bara)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:502: in computation_call<br/>    self._compiler_options, self._keep))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/core/computation.py:192: in _get_plan<br/>    return self._build_plan(plan_factory, thread.device_params, *args)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;nufhe.blind_rotate.BlindRotate object at 0x7f882be128&gt;<br/>plan_factory = &lt;function Computation._get_plan.&lt;locals&gt;.&lt;lambda&gt; at 0x7f8816a488&gt;<br/>device_params = &lt;reikna.cluda.cuda.DeviceParameters object at 0x7f82fd2438&gt;<br/>lwe_a = KernelArgument(_temp1), lwe_b = KernelArgument(_temp2)<br/>accum_a = KernelArgument(accum_a), gsw = KernelArgument(gsw), bara = KernelArgument(bara)<br/><br/>    def _build_plan(self, plan_factory, device_params, lwe_a, lwe_b, accum_a, gsw, bara):<br/>    <br/>        params = self._params<br/>        tlwe_params = params.tlwe_params<br/>        decomp_length = params.decomp_length<br/>        mask_size = tlwe_params.mask_size<br/>    <br/>        perf_params = self._perf_params<br/>        transform_type = self._params.tlwe_params.transform_type<br/>        transform = get_transform(transform_type)<br/>    <br/>        transform_module = transform.transform_module(perf_params, multi_iter=True)<br/>    <br/>        batch_shape = accum_a.shape[:-2]<br/>    <br/>        min_local_size = decomp_length * (mask_size + 1) * transform_module.threads_per_transform<br/>        local_size = device_params.max_work_group_size<br/>        while local_size &gt;= min_local_size:<br/>    <br/>            plan = plan_factory()<br/>    <br/>            if transform_module.use_constant_memory:<br/>                cdata_forward = plan.constant_array(transform_module.cdata_fw)<br/>                cdata_inverse = plan.constant_array(transform_module.cdata_inv)<br/>            else:<br/>                cdata_forward = plan.persistent_array(transform_module.cdata_fw)<br/>                cdata_inverse = plan.persistent_array(transform_module.cdata_inv)<br/>    <br/>            try:<br/>                plan.kernel_call(<br/>                    TEMPLATE.get_def(&quot;blind_rotate&quot;),<br/>                    [lwe_a, lwe_b, accum_a, gsw, bara, cdata_forward, cdata_inverse],<br/>                    kernel_name=&quot;blind_rotate&quot;,<br/>                    global_size=(<br/>                        helpers.product(batch_shape),<br/>                        local_size),<br/>                    local_size=(1, local_size),<br/>                    render_kwds=dict(<br/>                        local_size=local_size,<br/>                        slices=(len(batch_shape), 1, 1),<br/>                        slices2=(len(batch_shape), 1),<br/>                        slices3=(len(batch_shape),),<br/>                        transform=transform_module,<br/>                        mask_size=mask_size,<br/>                        decomp_length=decomp_length,<br/>                        output_size=self._in_out_params.size,<br/>                        input_size=tlwe_params.extracted_lweparams.size,<br/>                        bs_log2_base=self._params.bs_log2_base,<br/>                        mul_prepared=transform.transformed_mul_prepared(perf_params),<br/>                        add=transform.transformed_add(perf_params),<br/>                        tr_ctype=transform.transformed_internal_ctype(),<br/>                        min_blocks=helpers.min_blocks,<br/>                        )<br/>                    )<br/>            except OutOfResourcesError:<br/>                local_size -= transform_module.threads_per_transform<br/>                continue<br/>    <br/>            return plan<br/>    <br/>&gt;       raise ValueError(&quot;Could not find suitable local size for the kernel&quot;)<br/><span class="error">E       ValueError: Could not find suitable local size for the kernel</span><br/><br/>nufhe/blind_rotate.py:187: ValueError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_transforms_per_block_performance[cuda:0:0-NTT-tpb=1]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1ac8&gt;, transform_type = &#x27;NTT&#x27;<br/>heavy_performance_load = False, transforms_per_block = 1<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(<br/>        &#x27;transforms_per_block&#x27;, [1, 2, 3, 4], ids=[&#x27;tpb=1&#x27;, &#x27;tpb=2&#x27;, &#x27;tpb=3&#x27;, &#x27;tpb=4&#x27;])<br/>    def test_transforms_per_block_performance(<br/>            thread, transform_type, heavy_performance_load, transforms_per_block):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        max_tpb = max_supported_transforms_per_block(thread.device_params, transform_type)<br/>        if transforms_per_block &gt; max_tpb:<br/>            pytest.skip()<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/><br/>test/test_gates.py:408: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1ac8&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_transforms_per_block_performance[cuda:0:0-NTT-tpb=2]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1ac8&gt;, transform_type = &#x27;NTT&#x27;<br/>heavy_performance_load = False, transforms_per_block = 2<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(<br/>        &#x27;transforms_per_block&#x27;, [1, 2, 3, 4], ids=[&#x27;tpb=1&#x27;, &#x27;tpb=2&#x27;, &#x27;tpb=3&#x27;, &#x27;tpb=4&#x27;])<br/>    def test_transforms_per_block_performance(<br/>            thread, transform_type, heavy_performance_load, transforms_per_block):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        max_tpb = max_supported_transforms_per_block(thread.device_params, transform_type)<br/>        if transforms_per_block &gt; max_tpb:<br/>            pytest.skip()<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/><br/>test/test_gates.py:408: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1ac8&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_transforms_per_block_performance[cuda:0:0-NTT-tpb=3]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1ac8&gt;, transform_type = &#x27;NTT&#x27;<br/>heavy_performance_load = False, transforms_per_block = 3<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(<br/>        &#x27;transforms_per_block&#x27;, [1, 2, 3, 4], ids=[&#x27;tpb=1&#x27;, &#x27;tpb=2&#x27;, &#x27;tpb=3&#x27;, &#x27;tpb=4&#x27;])<br/>    def test_transforms_per_block_performance(<br/>            thread, transform_type, heavy_performance_load, transforms_per_block):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        max_tpb = max_supported_transforms_per_block(thread.device_params, transform_type)<br/>        if transforms_per_block &gt; max_tpb:<br/>            pytest.skip()<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/><br/>test/test_gates.py:408: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1ac8&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_transforms_per_block_performance[cuda:0:0-NTT-tpb=4]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1ac8&gt;, transform_type = &#x27;NTT&#x27;<br/>heavy_performance_load = False, transforms_per_block = 4<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(<br/>        &#x27;transforms_per_block&#x27;, [1, 2, 3, 4], ids=[&#x27;tpb=1&#x27;, &#x27;tpb=2&#x27;, &#x27;tpb=3&#x27;, &#x27;tpb=4&#x27;])<br/>    def test_transforms_per_block_performance(<br/>            thread, transform_type, heavy_performance_load, transforms_per_block):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        max_tpb = max_supported_transforms_per_block(thread.device_params, transform_type)<br/>        if transforms_per_block &gt; max_tpb:<br/>            pytest.skip()<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/><br/>test/test_gates.py:408: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1ac8&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_transforms_per_block_performance[cuda:0:0-FFT-tpb=1]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1ac8&gt;, transform_type = &#x27;FFT&#x27;<br/>heavy_performance_load = False, transforms_per_block = 1<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(<br/>        &#x27;transforms_per_block&#x27;, [1, 2, 3, 4], ids=[&#x27;tpb=1&#x27;, &#x27;tpb=2&#x27;, &#x27;tpb=3&#x27;, &#x27;tpb=4&#x27;])<br/>    def test_transforms_per_block_performance(<br/>            thread, transform_type, heavy_performance_load, transforms_per_block):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        max_tpb = max_supported_transforms_per_block(thread.device_params, transform_type)<br/>        if transforms_per_block &gt; max_tpb:<br/>            pytest.skip()<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/><br/>test/test_gates.py:408: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1ac8&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_transforms_per_block_performance[cuda:0:0-FFT-tpb=2]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1ac8&gt;, transform_type = &#x27;FFT&#x27;<br/>heavy_performance_load = False, transforms_per_block = 2<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(<br/>        &#x27;transforms_per_block&#x27;, [1, 2, 3, 4], ids=[&#x27;tpb=1&#x27;, &#x27;tpb=2&#x27;, &#x27;tpb=3&#x27;, &#x27;tpb=4&#x27;])<br/>    def test_transforms_per_block_performance(<br/>            thread, transform_type, heavy_performance_load, transforms_per_block):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        max_tpb = max_supported_transforms_per_block(thread.device_params, transform_type)<br/>        if transforms_per_block &gt; max_tpb:<br/>            pytest.skip()<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/><br/>test/test_gates.py:408: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1ac8&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_transforms_per_block_performance[cuda:0:0-FFT-tpb=3]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1ac8&gt;, transform_type = &#x27;FFT&#x27;<br/>heavy_performance_load = False, transforms_per_block = 3<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(<br/>        &#x27;transforms_per_block&#x27;, [1, 2, 3, 4], ids=[&#x27;tpb=1&#x27;, &#x27;tpb=2&#x27;, &#x27;tpb=3&#x27;, &#x27;tpb=4&#x27;])<br/>    def test_transforms_per_block_performance(<br/>            thread, transform_type, heavy_performance_load, transforms_per_block):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        max_tpb = max_supported_transforms_per_block(thread.device_params, transform_type)<br/>        if transforms_per_block &gt; max_tpb:<br/>            pytest.skip()<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/><br/>test/test_gates.py:408: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1ac8&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test/test_gates.py::test_transforms_per_block_performance[cuda:0:0-FFT-tpb=4]</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">thread = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1ac8&gt;, transform_type = &#x27;FFT&#x27;<br/>heavy_performance_load = False, transforms_per_block = 4<br/><br/>    @pytest.mark.perf<br/>    @pytest.mark.parametrize(<br/>        &#x27;transforms_per_block&#x27;, [1, 2, 3, 4], ids=[&#x27;tpb=1&#x27;, &#x27;tpb=2&#x27;, &#x27;tpb=3&#x27;, &#x27;tpb=4&#x27;])<br/>    def test_transforms_per_block_performance(<br/>            thread, transform_type, heavy_performance_load, transforms_per_block):<br/>    <br/>        if not transform_supported(thread.device_params, transform_type):<br/>            pytest.skip()<br/>    <br/>        max_tpb = max_supported_transforms_per_block(thread.device_params, transform_type)<br/>        if transforms_per_block &gt; max_tpb:<br/>            pytest.skip()<br/>    <br/>        size = 4096 if heavy_performance_load else 64<br/>    <br/>        rng = DeterministicRNG()<br/>&gt;       secret_key, cloud_key = make_key_pair(thread, rng, transform_type=transform_type)<br/><br/>test/test_gates.py:408: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>nufhe/api_low_level.py:248: in make_key_pair<br/>    secret_key = NuFHESecretKey.from_rng(thr, nufhe_params, rng)<br/>nufhe/api_low_level.py:113: in from_rng<br/>    lwe_key = LweKey.from_rng(thr, params.in_out_params, rng)<br/>nufhe/lwe.py:79: in from_rng<br/>    return cls(params, rand_uniform_bool(thr, rng, (params.size,)))<br/>nufhe/random_numbers.py:143: in rand_uniform_bool<br/>    return thr.to_device(rng.uniform_bool(shape))<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:443: in to_device<br/>    arr_device = self.empty_like(arr)<br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/api.py:434: in empty_like<br/>    allocator=allocator)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;reikna.cluda.cuda.Thread object at 0x7f883b1ac8&gt;, shape = (500,)<br/>dtype = dtype(&#x27;int32&#x27;), strides = (4,), offset = 0, nbytes = 2000<br/>allocator = &lt;Boost.Python.function object at 0x20830b00&gt;, base = None, base_data = None<br/><br/>    def array(<br/>            self, shape, dtype, strides=None, offset=0, nbytes=None,<br/>            allocator=None, base=None, base_data=None):<br/>    <br/>        # In PyCUDA, the default allocator is not None, but a default alloc object<br/>        if allocator is None:<br/>            allocator = cuda.mem_alloc<br/>    <br/>        dtype = dtypes.normalize_type(dtype)<br/>        shape = wrap_in_tuple(shape)<br/>        if nbytes is None:<br/>            nbytes = int(min_buffer_size(shape, dtype.itemsize, strides=strides, offset=offset))<br/>    <br/>        if (offset != 0 or strides is not None) and base_data is None and base is None:<br/>&gt;           base_data = allocator(nbytes)<br/><span class="error">E           pycuda._driver.LogicError: cuMemAlloc failed: invalid device context</span><br/><br/>../../../envs/env_example/lib/python3.6/site-packages/reikna-0.7.5-py3.6.egg/reikna/cluda/cuda.py:184: LogicError<br/></div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_gates.py::test_transform_type[cuda:0:0-NTT]</td>
          <td class="col-duration">14.42</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_gates.py::test_transform_type[cuda:0:0-FFT]</td>
          <td class="col-duration">3.10</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_gates.py::test_single_kernel_bs_with_ks[bs_loop-cuda:0:0]</td>
          <td class="col-duration">6.19</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_gates.py::test_constant_mem_performance[bs_loop-cuda:0:0-NTT-global_mem]</td>
          <td class="col-duration">70.40</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log"> ------------------------------Captured stdout call------------------------------ <br/>
Overall speed: 51.4527 +/- 0.1840 ms/bit, scaled: 51.1685 +/- 0.6549 ms/bit, overhead: 18.1926 +/- 20.9575 ms
<br/></div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_gates.py::test_constant_mem_performance[bs_loop-cuda:0:0-NTT-constant_mem]</td>
          <td class="col-duration">65.16</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log"> ------------------------------Captured stdout call------------------------------ <br/>
Overall speed: 54.2805 +/- 0.1882 ms/bit, scaled: 53.7437 +/- 0.5012 ms/bit, overhead: 34.3571 +/- 16.0396 ms
<br/></div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_gates.py::test_ntt_base_method_performance[bs_loop-cuda:0:0-ntt_base=cuda_asm]</td>
          <td class="col-duration">70.44</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log"> ------------------------------Captured stdout call------------------------------ <br/>
Overall speed: 51.5462 +/- 0.2087 ms/bit, scaled: 51.1262 +/- 0.6104 ms/bit, overhead: 26.8793 +/- 19.5323 ms
<br/></div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_gates.py::test_ntt_mul_method_performance[bs_loop-cuda:0:0-ntt_mul=cuda_asm]</td>
          <td class="col-duration">70.68</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log"> ------------------------------Captured stdout call------------------------------ <br/>
Overall speed: 51.4753 +/- 0.1216 ms/bit, scaled: 51.1533 +/- 0.4990 ms/bit, overhead: 20.6102 +/- 15.9669 ms
<br/></div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_gates.py::test_ntt_mul_method_performance[bs_loop-cuda:0:0-ntt_mul=c_from_asm]</td>
          <td class="col-duration">61.87</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log"> ------------------------------Captured stdout call------------------------------ <br/>
Overall speed: 51.0488 +/- 0.1637 ms/bit, scaled: 49.6327 +/- 0.6560 ms/bit, overhead: 90.6262 +/- 20.9925 ms
<br/></div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_gates.py::test_ntt_mul_method_performance[bs_loop-cuda:0:0-ntt_mul=c]</td>
          <td class="col-duration">62.21</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log"> ------------------------------Captured stdout call------------------------------ <br/>
Overall speed: 51.5124 +/- 0.2074 ms/bit, scaled: 51.0979 +/- 0.5605 ms/bit, overhead: 26.5292 +/- 17.9372 ms
<br/></div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_gates.py::test_gate_over_view[bs_loop-cuda:0:0]</td>
          <td class="col-duration">5.13</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_gates.py::test_nand_gate[cuda:0:0]</td>
          <td class="col-duration">6.25</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_gates.py::test_and_gate[cuda:0:0]</td>
          <td class="col-duration">6.45</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_gates.py::test_xnor_gate[cuda:0:0]</td>
          <td class="col-duration">6.33</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_gates.py::test_copy_gate[cuda:0:0]</td>
          <td class="col-duration">0.78</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_gates.py::test_andny_gate[cuda:0:0]</td>
          <td class="col-duration">6.27</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_gates.py::test_orny_gate[cuda:0:0]</td>
          <td class="col-duration">6.16</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_gates.py::test_mux_gate[cuda:0:0]</td>
          <td class="col-duration">7.05</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">test/test_gates.py::test_uint_min[cuda:0:0]</td>
          <td class="col-duration">35.27</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody></table></body></html>